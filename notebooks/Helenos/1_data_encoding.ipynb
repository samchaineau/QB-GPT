{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "env = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env == \"local\":\n",
    "    os.chdir(\"/Users/samuel/Documents/GitHub/QB-GPT/\")\n",
    "else:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.chdir(\"/content/gdrive/MyDrive/NFL_Challenge/NFL-GPT/NFL data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_models',\n",
       " '.DS_Store',\n",
       " 'app',\n",
       " 'LICENSE',\n",
       " 'models',\n",
       " 'README.md',\n",
       " '.gitignore',\n",
       " '.gitattributes',\n",
       " 'indexv2',\n",
       " 'data_preprocessing',\n",
       " 'index',\n",
       " '.git',\n",
       " 'notebooks']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length is :  144074\n",
      "Test length is :  61746\n"
     ]
    }
   ],
   "source": [
    "training_data = tf.data.Dataset.load(\"data_models/Helenos/train_data\")\n",
    "testing_data = tf.data.Dataset.load(\"data_models/Helenos/test_data\")\n",
    "\n",
    "train_length = [i for i,_ in enumerate(training_data)][-1] + 1\n",
    "test_length = [i for i,_ in enumerate(testing_data)][-1] + 1\n",
    "\n",
    "print(\"Train length is : \", str(train_length))\n",
    "print(\"Test length is : \", str(test_length))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "training_data = training_data.shuffle(train_length).batch(batch_size)\n",
    "testing_data = testing_data.shuffle(test_length).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modeling.QBGPT.models import QBGPT, LargeQBGPT, XLargeQBGPT\n",
    "from models.modeling.QBGPT.losses_and_metrics import CustomSparseCategoricalAccuracy, CustomTopKAccuracy, CustomSparseCategoricalCrossentropy\n",
    "\n",
    "moves_to_pred = 11170\n",
    "input_size = 11172\n",
    "starts_size = 1954\n",
    "scrimmage_size = 100\n",
    "positions_id = 29\n",
    "\n",
    "temp_ids = 52\n",
    "off_def_size = 2\n",
    "token_type_size = 3\n",
    "play_type_size = 9\n",
    "\n",
    "model_large = LargeQBGPT(input_vocab_size = input_size,\n",
    "                         positional_vocab_size = temp_ids,\n",
    "                         position_vocab_size=positions_id,\n",
    "                         start_vocab_size=starts_size,\n",
    "                         scrimmage_vocab_size=scrimmage_size,\n",
    "                         offdef_vocab_size = off_def_size,\n",
    "                         type_vocab_size = token_type_size,\n",
    "                         playtype_vocab_size = play_type_size,\n",
    "                         embedding_dim = 128,\n",
    "                         hidden_dim = 128,\n",
    "                         num_heads = 3,\n",
    "                         diag_masks = True,\n",
    "                         to_pred_size = moves_to_pred)\n",
    "\n",
    "model_medium = QBGPT(input_vocab_size = input_size,\n",
    "                    positional_vocab_size = temp_ids,\n",
    "                    position_vocab_size=positions_id,\n",
    "                    start_vocab_size=starts_size,\n",
    "                    scrimmage_vocab_size=scrimmage_size,\n",
    "                    offdef_vocab_size = off_def_size,\n",
    "                    type_vocab_size = token_type_size,\n",
    "                    playtype_vocab_size = play_type_size,\n",
    "                    embedding_dim = 256,\n",
    "                    hidden_dim = 256,\n",
    "                    num_heads = 3,\n",
    "                    diag_masks = True,\n",
    "                    to_pred_size = moves_to_pred)\n",
    "\n",
    "model_small = QBGPT(input_vocab_size = input_size,\n",
    "                    positional_vocab_size = temp_ids,\n",
    "                    position_vocab_size=positions_id,\n",
    "                    start_vocab_size=starts_size,\n",
    "                    scrimmage_vocab_size=scrimmage_size,\n",
    "                    offdef_vocab_size = off_def_size,\n",
    "                    type_vocab_size = token_type_size,\n",
    "                    playtype_vocab_size = play_type_size,\n",
    "                    embedding_dim = 128,\n",
    "                    hidden_dim = 128,\n",
    "                    num_heads = 3,\n",
    "                    diag_masks = True,\n",
    "                    to_pred_size = moves_to_pred)\n",
    "\n",
    "model_tiny = QBGPT(input_vocab_size = input_size,\n",
    "                    positional_vocab_size = temp_ids,\n",
    "                    position_vocab_size=positions_id,\n",
    "                    start_vocab_size=starts_size,\n",
    "                    scrimmage_vocab_size=scrimmage_size,\n",
    "                    offdef_vocab_size = off_def_size,\n",
    "                    type_vocab_size = token_type_size,\n",
    "                    playtype_vocab_size = play_type_size,\n",
    "                    embedding_dim = 64,\n",
    "                    hidden_dim = 64,\n",
    "                    num_heads = 3,\n",
    "                    diag_masks = True,\n",
    "                    to_pred_size = moves_to_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x296ab7850>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tiny.load_weights(\"models/modeling/QBGPT/weights/model_tinyv2/QBGPT\")\n",
    "model_small.load_weights(\"models/modeling/QBGPT/weights/model_smallv2/QBGPT\")\n",
    "model_medium.load_weights(\"models/modeling/QBGPT/weights/model_mediumv2/QBGPT\")\n",
    "model_large.load_weights(\"models/modeling/QBGPT/weights/model_largev2/QBGPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modeling.StratFormer.models import StratEncoder\n",
    "\n",
    "tiny_encoder = StratEncoder(num_spec_token= 1,\n",
    "                            hidden_dim=64,\n",
    "                            team_vocab_size=32,\n",
    "                            player_vocab_size=7229,\n",
    "                            season_vocab_size= 7,\n",
    "                            down_vocab_size= 5,\n",
    "                            base_encoder=model_tiny.Encoder)\n",
    "\n",
    "small_encoder = StratEncoder(num_spec_token= 1,\n",
    "                            hidden_dim=128,\n",
    "                            team_vocab_size=32,\n",
    "                            player_vocab_size=7229,\n",
    "                            season_vocab_size= 7,\n",
    "                            down_vocab_size= 5,\n",
    "                            base_encoder=model_small.Encoder)\n",
    "\n",
    "medium_encoder = StratEncoder(num_spec_token= 1,\n",
    "                            hidden_dim=256,\n",
    "                            team_vocab_size=32,\n",
    "                            player_vocab_size=7229,\n",
    "                            season_vocab_size= 7,\n",
    "                            down_vocab_size= 5,\n",
    "                            base_encoder=model_medium.Encoder)\n",
    "\n",
    "large_encoder = StratEncoder(num_spec_token= 1,\n",
    "                            hidden_dim=256,\n",
    "                            team_vocab_size=32,\n",
    "                            player_vocab_size=7229,\n",
    "                            season_vocab_size= 7,\n",
    "                            down_vocab_size= 5,\n",
    "                            base_encoder=model_large.Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x296bec5d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tiny_encoder.load_weights(\"models/modeling/StratFormer/weights/stratformer_tiny/StratFormer/\")\n",
    "#small_encoder.load_weights(\"models/modeling/StratFormer/weights/stratformer_small/StratFormer/\")\n",
    "medium_encoder.load_weights(\"models/modeling/StratFormer/weights/stratformer_medium/StratFormer/\")\n",
    "#large_encoder.load_weights(\"models/modeling/StratFormer/weights/stratformer_large/StratFormer/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off Def encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_success(value):\n",
    "    return tf.cast(value !=0, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|                                                          | 0/4503 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████████████████████| 4503/4503 [08:18<00:00,  9.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "encodings = []\n",
    "\n",
    "for batch in tqdm(training_data, desc=\"Processing\", total=len(training_data), ncols=100):\n",
    "\n",
    "  off_encoding = tiny_encoder(batch[\"off\"])[:,0,:]\n",
    "  def_encoding = tiny_encoder(batch[\"def\"])[:,0,:]\n",
    "  \n",
    "  _ = gc.collect()\n",
    "  encodings.append({\"gameId\" : batch[\"off\"][\"gameId\"],\n",
    "                    \"playId\" : batch[\"off\"][\"playId\"],\n",
    "                    \"Off\" : off_encoding,\n",
    "                    \"Def\" : def_encoding,\n",
    "                    \"playtype\" : batch[\"off\"][\"PlayType\"],\n",
    "                    \"Success\" : convert_success(batch[\"off\"][\"yards_gained\"]),\n",
    "                    \"yards_gained\" : batch[\"off\"][\"yards_gained\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████████████████████| 1930/1930 [03:55<00:00,  8.20it/s]\n"
     ]
    }
   ],
   "source": [
    "test_encodings = []\n",
    "\n",
    "for batch in tqdm(testing_data, desc=\"Processing\", total=len(testing_data), ncols=100):\n",
    "\n",
    "  off_encoding = tiny_encoder(batch[\"off\"])[:,0,:]\n",
    "  def_encoding = tiny_encoder(batch[\"def\"])[:,0,:]\n",
    "  \n",
    "  _ = gc.collect()\n",
    "  test_encodings.append({\"gameId\" : batch[\"off\"][\"gameId\"],\n",
    "                         \"playId\" : batch[\"off\"][\"playId\"],\n",
    "                         \"Off\" : off_encoding,\n",
    "                         \"Def\" : def_encoding,\n",
    "                         \"playtype\" : batch[\"off\"][\"PlayType\"],\n",
    "                         \"Success\" : convert_success(batch[\"off\"][\"yards_gained\"]),\n",
    "                         \"yards_gained\" : batch[\"off\"][\"yards_gained\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unbatch_dict(d : dict):\n",
    "    conc_vector = tf.concat([d[\"Off\"], d[\"Def\"]], axis = 1).numpy()\n",
    "    sub_vector = np.array(d[\"Off\"]-d[\"Def\"])\n",
    "    \n",
    "    l_of_b = [{\"gameId\" : np.array(d[\"gameId\"])[i],\n",
    "               \"playId\" : np.array(d[\"playId\"])[i],\n",
    "               \"concatenated_vector\" : conc_vector[i],\n",
    "               \"sub_vector\" : sub_vector[i],\n",
    "               \"playtype\" :np.unique(np.array(d[\"playtype\"])[i])[0],\n",
    "               \"Success\" : np.array(d[\"Success\"])[i],\n",
    "               \"yards\" :  np.array(d[\"yards_gained\"])[i]} for i in range(d[\"Success\"].shape[0])]\n",
    "    return l_of_b\n",
    "\n",
    "def append_id(d, i):\n",
    "    d_copy = d.copy()\n",
    "    d_copy['id'] = i\n",
    "    return d_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def compile_seq(list_of_trajs):\n",
    "    merged_dict = {k : [] for k in list_of_trajs[0].keys()}\n",
    "\n",
    "    with tqdm(total=len(list_of_trajs)) as pbar:\n",
    "      for d in list_of_trajs:\n",
    "        for key, value in d.items():\n",
    "          merged_dict[key] += [value]\n",
    "        pbar.update(1)\n",
    "        \n",
    "    merged_dict = {k: np.array(v) for k,v in merged_dict.items()}\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144074\n",
      "61746\n"
     ]
    }
   ],
   "source": [
    "encodings_unb = [unbatch_dict(d) for d in encodings]\n",
    "encodings_unb = [d for l in encodings_unb for d in l]\n",
    "print(len(encodings_unb))\n",
    "\n",
    "test_encodings_unb = [unbatch_dict(d) for d in test_encodings]\n",
    "test_encodings_unb = [d for l in test_encodings_unb for d in l]\n",
    "print(len(test_encodings_unb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_unb = [append_id(encodings_unb[i], i) for i in range(len(encodings_unb))]\n",
    "test_encodings_unb = [append_id(test_encodings_unb[i], i) for i in range(len(test_encodings_unb))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144074/144074 [00:00<00:00, 1566106.84it/s]\n",
      "100%|██████████| 61746/61746 [00:00<00:00, 1651983.76it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = compile_seq(encodings_unb)\n",
    "test_dataset = compile_seq(test_encodings_unb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total = tf.data.Dataset.from_tensor_slices(train_dataset)\n",
    "tf.data.Dataset.save(train_total, \"data_models/Helenos/encoded_train_tiny\")\n",
    "\n",
    "test_total = tf.data.Dataset.from_tensor_slices(test_dataset)\n",
    "tf.data.Dataset.save(test_total, \"data_models/Helenos/encoded_test_tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nflgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
