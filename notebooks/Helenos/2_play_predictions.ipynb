{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23522,"status":"ok","timestamp":1695750089313,"user":{"displayName":"sam Chain","userId":"00779398991030525753"},"user_tz":-120},"id":"BcuLdi7seAKr","outputId":"a57ae5af-106b-43d0-9d5a-816d53a8a4ca"},"outputs":[],"source":["import os\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","import numpy as np\n","import polars as pl\n","import nfl_data_py as nfl\n","\n","env = \"local\""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["if env == \"local\":\n","    os.chdir(\"/Users/samuel/Documents/GitHub/QB-GPT/\")\n","else:\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    os.chdir(\"/content/gdrive/MyDrive/NFL_Challenge/NFL-GPT/NFL data\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":215,"status":"ok","timestamp":1695750225368,"user":{"displayName":"sam Chain","userId":"00779398991030525753"},"user_tz":-120},"id":"B2pOQ6qWeILn","outputId":"7e91fa10-a10c-4681-e348-1d5fed9420b1"},"outputs":[{"data":{"text/plain":["['data_models',\n"," '.DS_Store',\n"," 'app',\n"," 'LICENSE',\n"," 'models',\n"," 'README.md',\n"," '.gitignore',\n"," '.gitattributes',\n"," 'data_preprocessing',\n"," 'index',\n"," '.git',\n"," 'notebooks']"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["os.listdir()"]},{"cell_type":"code","execution_count":61,"metadata":{"executionInfo":{"elapsed":6338,"status":"ok","timestamp":1695750238260,"user":{"displayName":"sam Chain","userId":"00779398991030525753"},"user_tz":-120},"id":"n21dB8fmlAi_"},"outputs":[],"source":["training_data = tf.data.Dataset.load(\"data_models/Helenos_categ/train_play_prediction_pois\")\n","testing_data = tf.data.Dataset.load(\"data_models/Helenos_categ/test_play_prediction_pois\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1695750241105,"user":{"displayName":"sam Chain","userId":"00779398991030525753"},"user_tz":-120},"id":"8D31HaTelyvO","outputId":"1d3697f5-a143-43a1-9ee9-413564503b82"},"outputs":[],"source":["train_length = [i for i,_ in enumerate(training_data)][-1] + 1\n","test_length = [i for i,_ in enumerate(testing_data)][-1] + 1"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train length is :  293150\n","Test length is :  125636\n"]}],"source":["print(\"Train length is : \", str(train_length))\n","print(\"Test length is : \", str(test_length))"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["batch_size = 32\n","\n","training_data = training_data.shuffle(train_length).batch(batch_size)\n","testing_data = testing_data.shuffle(test_length).batch(batch_size)"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"EHNS1yT0p5sM"},"outputs":[{"data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x34ba39dd0>"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["from models.modeling.QBGPT.models import QBGPT, AttentionBlock, AttentionBlockHelenos\n","\n","moves_to_pred = 10876\n","input_size = 10878\n","starts_size = 1033\n","scrimmage_size = 100\n","positions_id = 29\n","temp_ids = 52\n","\n","off_def_size = 2\n","token_type_size = 3\n","play_type_size = 9\n","\n","\n","\n","model_tiny = QBGPT(input_vocab_size = input_size,\n","                    positional_vocab_size = temp_ids,\n","                    position_vocab_size=positions_id,\n","                    start_vocab_size=starts_size,\n","                    scrimmage_vocab_size=scrimmage_size,\n","                    offdef_vocab_size = off_def_size,\n","                    type_vocab_size = token_type_size,\n","                    playtype_vocab_size = play_type_size,\n","                    embedding_dim = 64,\n","                    hidden_dim = 64,\n","                    to_pred_size = moves_to_pred)\n","\n","\n","\n","model_tiny.load_weights(\"models/modeling/QBGPT/weights/model_tiny/QBGPT\")"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(32, 256), dtype=int64, numpy=\n","array([[0, 1, 1, ..., 2, 2, 2],\n","       [0, 1, 1, ..., 2, 2, 2],\n","       [1, 1, 1, ..., 2, 2, 2],\n","       ...,\n","       [1, 1, 1, ..., 2, 2, 2],\n","       [0, 1, 1, ..., 2, 2, 2],\n","       [1, 1, 1, ..., 2, 2, 2]])>"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["qbgpt_data = tf.data.Dataset.load(\"data_models/QBGPT/test_tokens_NFL_GPT\")\n","\n","to_build = list(qbgpt_data.batch(32))[0][0]\n","to_build[\"token_type_ids\"]"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(32, 256, 10876), dtype=float32, numpy=\n","array([[[-31.184824, -31.321651, -33.827995, ..., -32.139767,\n","         -35.21571 , -32.04264 ],\n","        [-26.570581, -26.655142, -28.346746, ..., -27.595945,\n","         -30.311228, -27.650642],\n","        [-25.891638, -26.007206, -28.418224, ..., -26.820076,\n","         -29.752003, -26.752388],\n","        ...,\n","        [-18.545486, -18.92145 , -17.710766, ..., -18.594082,\n","         -19.279867, -18.179737],\n","        [-18.545486, -18.92145 , -17.710766, ..., -18.594082,\n","         -19.279867, -18.179737],\n","        [-18.545486, -18.92145 , -17.710766, ..., -18.594082,\n","         -19.279867, -18.179737]],\n","\n","       [[-32.39939 , -32.851208, -35.903957, ..., -34.911182,\n","         -36.811886, -33.84422 ],\n","        [-32.60915 , -33.241703, -36.130302, ..., -35.601456,\n","         -36.518887, -34.383034],\n","        [-30.747688, -31.308493, -34.758984, ..., -33.396282,\n","         -34.767628, -32.17215 ],\n","        ...,\n","        [-20.650797, -21.537125, -21.102806, ..., -20.775711,\n","         -22.334778, -20.350077],\n","        [-20.650797, -21.537125, -21.102806, ..., -20.775711,\n","         -22.334778, -20.350077],\n","        [-20.650797, -21.537125, -21.102806, ..., -20.775711,\n","         -22.334778, -20.350077]],\n","\n","       [[-24.738272, -25.056183, -25.688929, ..., -25.718847,\n","         -24.18402 , -25.617039],\n","        [-23.352306, -23.863628, -24.874683, ..., -24.605017,\n","         -22.953398, -24.342762],\n","        [-23.217756, -23.732666, -24.647781, ..., -24.334864,\n","         -22.760057, -24.131817],\n","        ...,\n","        [-20.743385, -21.61172 , -21.071976, ..., -20.876465,\n","         -22.411484, -20.466309],\n","        [-20.743385, -21.61172 , -21.071976, ..., -20.876465,\n","         -22.411484, -20.466309],\n","        [-20.743385, -21.61172 , -21.071976, ..., -20.876465,\n","         -22.411484, -20.466309]],\n","\n","       ...,\n","\n","       [[-22.288963, -22.553507, -21.731644, ..., -21.28702 ,\n","         -22.72992 , -22.86511 ],\n","        [-19.59786 , -19.986908, -21.77935 , ..., -19.739437,\n","         -19.430841, -20.53044 ],\n","        [-19.325815, -19.727587, -21.385668, ..., -19.442696,\n","         -19.209517, -20.247183],\n","        ...,\n","        [-23.954767, -24.8646  , -24.248957, ..., -23.968468,\n","         -25.266176, -23.423216],\n","        [-23.954767, -24.8646  , -24.248957, ..., -23.968468,\n","         -25.266176, -23.423216],\n","        [-23.954767, -24.8646  , -24.248957, ..., -23.968468,\n","         -25.266176, -23.423216]],\n","\n","       [[-33.707256, -34.72658 , -37.806576, ..., -36.540115,\n","         -37.98335 , -35.375923],\n","        [-35.303707, -36.591625, -38.334896, ..., -37.93991 ,\n","         -38.795708, -37.20785 ],\n","        [-34.17473 , -35.25723 , -38.691677, ..., -36.627113,\n","         -38.428364, -35.72672 ],\n","        ...,\n","        [-28.706999, -29.843624, -32.46796 , ..., -30.4116  ,\n","         -32.035927, -28.98823 ],\n","        [-28.706999, -29.843624, -32.46796 , ..., -30.4116  ,\n","         -32.035927, -28.98823 ],\n","        [-28.706999, -29.843624, -32.46796 , ..., -30.4116  ,\n","         -32.035927, -28.98823 ]],\n","\n","       [[-29.194162, -29.465807, -37.011227, ..., -32.306004,\n","         -30.874794, -29.089014],\n","        [-26.696032, -26.929087, -31.258884, ..., -28.357437,\n","         -27.609562, -26.592936],\n","        [-25.86347 , -26.080954, -30.147081, ..., -27.543941,\n","         -26.629616, -25.86658 ],\n","        ...,\n","        [-20.650797, -21.537125, -21.102806, ..., -20.775711,\n","         -22.334778, -20.350077],\n","        [-20.650797, -21.537125, -21.102806, ..., -20.775711,\n","         -22.334778, -20.350077],\n","        [-20.650797, -21.537125, -21.102806, ..., -20.775711,\n","         -22.334778, -20.350077]]], dtype=float32)>"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["model_tiny(to_build)"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["# Define the original embedding layer with an index size of 28 and embedding dimension.\n","original_index_size = 29\n","embedding_dim = 64  # Change this to your desired embedding dimension.\n","\n","# Create a new embedding layer with an index size of 29.\n","new_index_size = 30\n","new_embedding_layer = tf.keras.layers.Embedding(input_dim=new_index_size, output_dim=embedding_dim)\n","\n","# Assume you have the original embedding weights as a NumPy array.\n","# Replace this with your actual weights.\n","original_embedding_weights = np.random.rand(original_index_size, embedding_dim)\n","\n","# Copy weights from the original embedding layer to the new one, preserving 28 weights.\n","new_embedding_weights = np.zeros((new_index_size, embedding_dim))\n","new_embedding_weights[:original_index_size, :] = original_embedding_weights\n","\n","# Set the weights of the new index (29) to the 21st weight.\n","new_embedding_weights[new_index_size - 1, :] = original_embedding_weights[4, :]\n","\n","# Set the weights of the new embedding layer to the modified weights.\n","new_embedding_layer.build((None,))\n","new_embedding_layer.set_weights([new_embedding_weights])"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["model_tiny.Encoder.Embedding.PositionEmbedding.Embedding = new_embedding_layer"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["tfd = tfp.distributions\n","tfpl = tfp.layers"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["from typing import List, Optional, Union\n","import numpy as np\n","\n","def shape_list(tensor: Union[tf.Tensor, np.ndarray]) -> List[int]:\n","    \"\"\"\n","    Deal with dynamic shape in tensorflow cleanly.\n","\n","    Args:\n","        tensor (`tf.Tensor` or `np.ndarray`): The tensor we want the shape of.\n","\n","    Returns:\n","        `List[int]`: The shape of the tensor as a list.\n","    \"\"\"\n","    if isinstance(tensor, np.ndarray):\n","        return list(tensor.shape)\n","\n","    dynamic = tf.shape(tensor)\n","\n","    if tensor.shape == tf.TensorShape(None):\n","        return dynamic\n","\n","    static = tensor.shape.as_list()\n","\n","    return [dynamic[i] if s is None else s for i, s in enumerate(static)]"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["class AttentionBlockHelenos(tf.keras.Model):\n","  def __init__(self,\n","               num_heads : int,\n","               hidden_dim : int,\n","               output_dim : int):\n","        super(AttentionBlockHelenos, self).__init__()\n","\n","        self.num_attention_heads = num_heads\n","        self.attention_head_size = hidden_dim\n","        self.total_dim = num_heads * hidden_dim\n","        self.output_dim = output_dim\n","        \n","        self.NormIn = tf.keras.layers.LayerNormalization(name = \"Norm_in\")\n","        self.Query = tf.keras.layers.Dense(self.total_dim, name = \"Query\")\n","        self.Key = tf.keras.layers.Dense(self.total_dim, name = \"Key\")\n","        self.Value = tf.keras.layers.Dense(self.total_dim, name = \"Value\")\n","        self.DenseAtt = tf.keras.layers.Dense(output_dim, name = \"Dense\", activation = \"relu\")\n","        \n","        self.Add = tf.keras.layers.Add(name = \"Add\")\n","        self.Drop = tf.keras.layers.Dropout(rate = 0.1)\n","        self.DenseOut = tf.keras.layers.Dense(output_dim, name = \"Dense\", activation = \"relu\")\n","        self.NormOut = tf.keras.layers.LayerNormalization(name = \"Norm_out\")\n","\n","  def transpose_for_scores(self, tensor: tf.Tensor, batch_size: int) -> tf.Tensor:\n","        # Reshape from [batch_size, seq_length, all_head_size] to [batch_size, seq_length, num_attention_heads, attention_head_size]\n","        tensor = tf.reshape(tensor=tensor, shape=(batch_size, -1, self.num_attention_heads, self.attention_head_size))\n","\n","        # Transpose the tensor from [batch_size, seq_length, num_attention_heads, attention_head_size] to [batch_size, num_attention_heads, seq_length, attention_head_size]\n","        return tf.transpose(tensor, perm=[0, 2, 1, 3])\n","\n","  def compute_scaled_attn_scores(self, query, key):\n","    attention_scores = tf.matmul(query, key, transpose_b=True)  # Transpose the second sequence\n","\n","    # If you want scaled dot-product attention, divide by the square root of the embedding dimension\n","    embedding_dim = query.shape[-1]\n","    scaled_attention_scores = attention_scores / tf.math.sqrt(tf.cast(embedding_dim, dtype=tf.float32))\n","\n","    return scaled_attention_scores\n","\n","  def call(self,\n","           q : tf.Tensor,\n","           k : tf.Tensor,\n","           v : tf.Tensor):\n","\n","    batch_size = shape_list(q)[0]\n","    \n","    norm_hidden_states_q = self.NormIn(q)\n","    norm_hidden_states_k = self.NormIn(k)\n","    norm_hidden_states_v = self.NormIn(v)\n","    \n","    query = self.Query(norm_hidden_states_q)\n","    queries = self.transpose_for_scores(query, batch_size)\n","\n","    key = self.Key(norm_hidden_states_k)\n","    keys = self.transpose_for_scores(key, batch_size)\n","    \n","    value = self.Key(norm_hidden_states_v)\n","    values = self.transpose_for_scores(value, batch_size)\n","    \n","    attention_weights = self.compute_scaled_attn_scores(queries, keys)\n","    \n","    attention_scores = tf.matmul(attention_weights, values)\n","    attention_scores = tf.transpose(attention_scores, perm=[0, 2, 1, 3])\n","    attention_scores = tf.reshape(tensor=attention_scores, shape=(batch_size, -1, self.total_dim))\n","    attention_scores = self.DenseAtt(attention_scores)\n","    \n","    output = self.Add([attention_scores, norm_hidden_states_v])\n","    \n","    norm_output = self.NormOut(output)\n","    densed_output = self.DenseOut(norm_output)\n","    \n","    output = self.Add([densed_output, output])\n","    \n","    output = self.Drop(output)\n","    return output"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["class SimpleAttentionBlock(tf.keras.Model):\n","  def __init__(self,\n","               hidden_dim : int,\n","               output_dim : int):\n","        super(SimpleAttentionBlock, self).__init__()\n","\n","        self.num_attention_heads = 1\n","        self.attention_head_size = hidden_dim\n","        self.total_dim = 1 * hidden_dim\n","        self.output_dim = output_dim\n","        \n","        self.NormIn = tf.keras.layers.LayerNormalization(name = \"Norm_in\")\n","        self.Query = tf.keras.layers.Dense(self.total_dim, name = \"Query\")\n","        self.Key = tf.keras.layers.Dense(self.total_dim, name = \"Key\")\n","        self.Value = tf.keras.layers.Dense(self.total_dim, name = \"Value\")\n","        self.DenseAtt = tf.keras.layers.Dense(output_dim, name = \"Dense\", activation = \"relu\")\n","        \n","        self.Add = tf.keras.layers.Add(name = \"Add\")\n","        self.Drop = tf.keras.layers.Dropout(rate = 0.1)\n","        self.DenseOut = tf.keras.layers.Dense(output_dim, name = \"Dense\", activation = \"relu\")\n","        self.NormOut = tf.keras.layers.LayerNormalization(name = \"Norm_out\")\n","\n","  def compute_scaled_attn_scores(self, query, key):\n","    attention_scores = tf.matmul(query, key, transpose_b=True)  # Transpose the second sequence\n","\n","    # If you want scaled dot-product attention, divide by the square root of the embedding dimension\n","    embedding_dim = query.shape[-1]\n","    scaled_attention_scores = attention_scores / tf.math.sqrt(tf.cast(embedding_dim, dtype=tf.float32))\n","\n","    return scaled_attention_scores\n","\n","  def call(self,\n","           q : tf.Tensor,\n","           k : tf.Tensor,\n","           v : tf.Tensor):\n","\n","    batch_size = shape_list(q)[0]\n","    \n","    norm_hidden_states_q = self.NormIn(q)\n","    norm_hidden_states_k = self.NormIn(k)\n","    norm_hidden_states_v = self.NormIn(v)\n","    \n","    queries = self.Query(norm_hidden_states_q)\n","\n","    keys = self.Key(norm_hidden_states_k)\n","    \n","    values = self.Key(norm_hidden_states_v)\n","    \n","    attention_weights = self.compute_scaled_attn_scores(queries, keys)\n","    \n","    attention_scores = tf.matmul(attention_weights, values)\n","    attention_scores = self.DenseAtt(attention_scores)\n","    \n","    output = self.Add([attention_scores, norm_hidden_states_v])\n","    \n","    norm_output = self.NormOut(output)\n","    densed_output = self.DenseOut(norm_output)\n","    \n","    output = self.Add([densed_output, output])\n","    \n","    output = self.Drop(output)\n","    return output"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["class DownEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(DownEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"down_ID\"])\n","    return embed\n","\n","class SeasonEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(SeasonEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"season_ID\"])\n","    return embed\n","\n","class TeamEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(TeamEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"team_ID\"])\n","    return embed\n","\n","class PlayerEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(PlayerEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"player_ids\"])\n","    return embed\n","\n","\n","class MetaEmbedding(tf.keras.Model):\n","  def __init__(self, \n","               team_vocab_size : int, \n","               player_vocab_size : int, \n","               season_vocab_size : int, \n","               down_vocab_size : int, \n","               embedding_dim : int):\n","        super(MetaEmbedding, self).__init__()\n","        \n","        self.embedding_dim = embedding_dim\n","\n","        self.TeamEmbedding = TeamEncoder(vocab_size= team_vocab_size,\n","                                         embedding_dim=embedding_dim)\n","        self.PlayerEmbedding = PlayerEncoder(vocab_size= player_vocab_size,\n","                                             embedding_dim=embedding_dim)\n","        self.SeasonEmbedding = SeasonEncoder(vocab_size= season_vocab_size,\n","                                             embedding_dim=embedding_dim)\n","        self.DownEmbedding = DownEncoder(vocab_size= down_vocab_size,\n","                                         embedding_dim=embedding_dim)\n","        \n","        self.Attention = SimpleAttentionBlock(hidden_dim=embedding_dim,\n","                                              output_dim=embedding_dim)\n","        \n","        self.Conc = tf.keras.layers.Concatenate(axis = 2)\n","        \n","        self.Dense = tf.keras.layers.Dense(embedding_dim)\n","\n","  def call(self, x):\n","    team_embed = self.TeamEmbedding(x)\n","    team_embed = tf.expand_dims(team_embed, axis = 2)\n","    \n","    player_embed = self.PlayerEmbedding(x)\n","    player_embed = tf.expand_dims(player_embed, axis = 2)\n","    \n","    season_embed = self.SeasonEmbedding(x)\n","    season_embed = tf.expand_dims(season_embed, axis = 2)\n","    \n","    down_embed = self.DownEmbedding(x)\n","    down_embed = tf.expand_dims(down_embed, axis = 2)\n","    \n","    added = self.Conc([team_embed, player_embed, season_embed, down_embed])\n","    \n","    logits = self.Attention(added, added, added)\n","    logits = tf.reshape(tensor=logits, shape=(batch_size, 11, -1, self.embedding_dim * 4))\n","    logits = tf.squeeze(logits, axis = 2)\n","    \n","    encoded = self.Dense(logits)\n","    \n","    return encoded\n","\n","class Helenos(tf.keras.Model):\n","    def __init__(self, \n","                 team_vocab_size : int, \n","                 player_vocab_size : int,\n","                season_vocab_size : int,\n","                down_vocab_size : int, \n","                embedding_dim : int,\n","                encoder : tf.keras.Model):\n","        super(Helenos, self).__init__()\n","        \n","        self.Encoder = encoder\n","        \n","        self.MetaEmbedding = MetaEmbedding(team_vocab_size=team_vocab_size,\n","                                           player_vocab_size = player_vocab_size,\n","                                           season_vocab_size=season_vocab_size,\n","                                           down_vocab_size=down_vocab_size,\n","                                           embedding_dim=embedding_dim)\n","        \n","        self.Add = tf.keras.layers.Add()\n","        \n","        self.Concat2 = tf.keras.layers.Concatenate(axis = -2)\n","        \n","        self.Attention = AttentionBlockHelenos(num_heads=3,\n","                                               hidden_dim=64,\n","                                               output_dim=embedding_dim)\n","        \n","        self.Concat3 = tf.keras.layers.Concatenate(axis = -1)\n","        \n","        self.Average = tf.keras.layers.GlobalAveragePooling1D()\n","        \n","        self.Params = tf.keras.layers.Dense(1)\n","        \n","        self.Dist = tfpl.DistributionLambda(lambda t:\n","                    tfd.Poisson(rate = tf.math.softplus(t[...,0])))\n","        \n","        \n","    \n","    def call(self, x):\n","        encoded_qb_off = self.Encoder(x[\"off\"])\n","        encoded_meta_off = self.MetaEmbedding(x[\"off\"])\n","        \n","        encoded_qb_def = self.Encoder(x[\"def\"])\n","        encoded_meta_def = self.MetaEmbedding(x[\"def\"])\n","        \n","        logits_off = self.Attention(q = encoded_qb_off, k = encoded_meta_off, v = encoded_meta_off)\n","        logits_def = self.Attention(q = encoded_qb_def, k = encoded_meta_def, v = encoded_meta_def)\n","        \n","        logits = self.Concat2([logits_off, logits_def])\n","        \n","        logits = self.Average(logits)\n","        params = self.Params(logits)\n","        \n","        dist = self.Dist(params)\n","        \n","        return dist"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["model = Helenos(team_vocab_size=32,\n","                player_vocab_size=7_226,\n","                season_vocab_size= 7,\n","                down_vocab_size= 5,\n","                embedding_dim= 64,\n","                encoder = model_tiny.Encoder)\n","\n","model.Encoder.trainable = False"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(32,), dtype=float64, numpy=\n","array([110., 111., 107., 105., 104., 102., 101., 105.,  98., 100., 120.,\n","        99.,  99., 122., 104.,  96., 109., 111.,  99.,  98., 155.,  97.,\n","        99.,  99., 101., 102., 105.,  99., 106., 103., 102., 109.])>"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["list(testing_data)[0][1]"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[{"data":{"text/plain":["<tfp.distributions._TensorCoercible 'tensor_coercible' batch_shape=[32] event_shape=[] dtype=float32>"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["essai = list(testing_data)[0][0]\n","model(essai)"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["negloglik = lambda y, rv_y: -rv_y.log_prob(y)"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["def scheduler(epoch, lr):\n","  if epoch < 1:\n","      return 1e-4\n","  else:\n","      return 5e-5\n","\n","\n","schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n"]},{"name":"stderr","output_type":"stream","text":["2023-10-09 23:21:19.118462: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:79: Filling up shuffle buffer (this may take a while): 182427 of 293150\n"]},{"name":"stdout","output_type":"stream","text":["  15/9161 [..............................] - ETA: 2:00 - loss: 87.3030   "]},{"name":"stderr","output_type":"stream","text":["2023-10-09 23:21:25.263220: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.\n"]},{"name":"stdout","output_type":"stream","text":["9159/9161 [============================>.] - ETA: 0s - loss: 3.7808"]},{"ename":"InvalidArgumentError","evalue":"Graph execution error:\n\nDetected at node gradient_tape/lambda/tensor_coercible_CONSTRUCTED_AT_helenos_6_distribution_lambda_6/log_prob/multiply_no_nan/BroadcastGradientArgs defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n\n  File \"/var/folders/xc/clt_qlgj3cncjzx754k5yw_c0000gn/T/ipykernel_34688/4211755303.py\", line 4, in <module>\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1130, in train_step\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 543, in minimize\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n\nIncompatible shapes: [32] vs. [30]\n\t [[{{node gradient_tape/lambda/tensor_coercible_CONSTRUCTED_AT_helenos_6_distribution_lambda_6/log_prob/multiply_no_nan/BroadcastGradientArgs}}]] [Op:__inference_train_function_16210089]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[1;32m/Users/samuel/Documents/GitHub/QB-GPT/notebooks/Helenos/2_play_predictions.ipynb Cell 23\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samuel/Documents/GitHub/QB-GPT/notebooks/Helenos/2_play_predictions.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samuel/Documents/GitHub/QB-GPT/notebooks/Helenos/2_play_predictions.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                     loss\u001b[39m=\u001b[39m negloglik)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/samuel/Documents/GitHub/QB-GPT/notebooks/Helenos/2_play_predictions.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m history_small \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(training_data, epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n","File \u001b[0;32m~/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m~/anaconda3/envs/nflgpt/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node gradient_tape/lambda/tensor_coercible_CONSTRUCTED_AT_helenos_6_distribution_lambda_6/log_prob/multiply_no_nan/BroadcastGradientArgs defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n\n  File \"/var/folders/xc/clt_qlgj3cncjzx754k5yw_c0000gn/T/ipykernel_34688/4211755303.py\", line 4, in <module>\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1130, in train_step\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 543, in minimize\n\n  File \"/Users/samuel/anaconda3/envs/nflgpt/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n\nIncompatible shapes: [32] vs. [30]\n\t [[{{node gradient_tape/lambda/tensor_coercible_CONSTRUCTED_AT_helenos_6_distribution_lambda_6/log_prob/multiply_no_nan/BroadcastGradientArgs}}]] [Op:__inference_train_function_16210089]"]}],"source":["model.compile(optimizer=tf.keras.optimizers.Adam(),\n","                    loss= negloglik)\n","\n","history_small = model.fit(training_data, epochs=2)"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["X_Y_test = list(testing_data)[0]\n","X_test = X_Y_test[0]\n","Y_test = X_Y_test[1].numpy()\n","pred_dist = model(X_test)"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["loc_hat = np.mean(pred_dist.parameters[\"loc\"])\n","scale_hat = np.mean(pred_dist.parameters[\"scale\"])\n","rate_hat = np.mean(pred_dist.parameters[\"rate\"])"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[{"data":{"text/plain":["{'loc': <tf.Tensor: shape=(32,), dtype=float32, numpy=\n"," array([-1.9706415 , -0.27730155, -1.8973212 , -0.12310744, -2.0525064 ,\n","        -1.9909205 , -2.0068512 , -0.04371835,  0.03401811, -1.853869  ,\n","        -1.9749445 , -0.12070109, -1.942014  , -1.8648635 , -0.34762996,\n","        -2.1113677 , -2.5591092 , -0.11513726, -1.9697163 , -2.9186897 ,\n","        -2.2004464 , -3.0807996 , -0.28684127, -0.02177463, -0.10883667,\n","        -1.9541225 , -2.0828385 , -0.11423492, -2.6962228 , -0.13184486,\n","        -2.6722188 , -0.00380048], dtype=float32)>,\n"," 'scale': <tf.Tensor: shape=(32,), dtype=float32, numpy=\n"," array([3.2900844, 1.967712 , 3.1387942, 2.1550481, 3.250607 , 3.2937913,\n","        3.3023915, 2.1097374, 2.146712 , 3.146023 , 3.28955  , 2.1755946,\n","        3.2438848, 3.1593826, 1.9089994, 3.4005303, 3.6838508, 2.1110296,\n","        3.2780778, 3.9324925, 3.4529335, 4.07618  , 1.8737396, 2.0216503,\n","        2.2148094, 3.301769 , 3.3243136, 2.1905463, 3.7907004, 2.0425477,\n","        3.8286166, 2.1850023], dtype=float32)>,\n"," 'rate': <tf.Tensor: shape=(32,), dtype=float32, numpy=\n"," array([0.11349906, 0.16211218, 0.11346258, 0.20212282, 0.10902141,\n","        0.11040459, 0.10903695, 0.20223945, 0.20099632, 0.12110565,\n","        0.11322328, 0.20450771, 0.10765915, 0.1204741 , 0.16240834,\n","        0.10580668, 0.11652442, 0.20278722, 0.11096116, 0.11372487,\n","        0.1139101 , 0.11001013, 0.17058302, 0.19911794, 0.20559281,\n","        0.11259024, 0.10991221, 0.20747331, 0.10823727, 0.20179255,\n","        0.12183753, 0.20305738], dtype=float32)>,\n"," 'validate_args': False,\n"," 'allow_nan_stats': True,\n"," 'name': 'ExponentiallyModifiedGaussian'}"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["pred_dist.parameters"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Class</th>\n","      <th>Percentage</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>34.375</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>9.375</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>6.250</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>6.250</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>12</td>\n","      <td>3.125</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>28</td>\n","      <td>3.125</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>3.125</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>17</td>\n","      <td>3.125</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>10</td>\n","      <td>3.125</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>-3</td>\n","      <td>3.125</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>9</td>\n","      <td>3.125</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>21</td>\n","      <td>3.125</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>52</td>\n","      <td>3.125</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>32</td>\n","      <td>3.125</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>8</td>\n","      <td>3.125</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1</td>\n","      <td>3.125</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>55</td>\n","      <td>3.125</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>25</td>\n","      <td>3.125</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Class  Percentage\n","0       0      34.375\n","1       3       9.375\n","2       2       6.250\n","3       4       6.250\n","4      12       3.125\n","5      28       3.125\n","6       7       3.125\n","7      17       3.125\n","8      10       3.125\n","9      -3       3.125\n","10      9       3.125\n","11     21       3.125\n","12     52       3.125\n","13     32       3.125\n","14      8       3.125\n","15      1       3.125\n","16     55       3.125\n","17     25       3.125"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"line":{"color":"red"},"mode":"lines","name":"Fitted Exponnorm PDF","type":"scatter","x":[-20,-19.95995995995996,-19.91991991991992,-19.87987987987988,-19.83983983983984,-19.7997997997998,-19.75975975975976,-19.71971971971972,-19.67967967967968,-19.63963963963964,-19.5995995995996,-19.55955955955956,-19.51951951951952,-19.47947947947948,-19.43943943943944,-19.3993993993994,-19.35935935935936,-19.31931931931932,-19.27927927927928,-19.23923923923924,-19.1991991991992,-19.15915915915916,-19.11911911911912,-19.07907907907908,-19.03903903903904,-18.998998998999,-18.95895895895896,-18.91891891891892,-18.87887887887888,-18.83883883883884,-18.7987987987988,-18.75875875875876,-18.71871871871872,-18.67867867867868,-18.63863863863864,-18.5985985985986,-18.55855855855856,-18.51851851851852,-18.47847847847848,-18.43843843843844,-18.3983983983984,-18.35835835835836,-18.31831831831832,-18.27827827827828,-18.23823823823824,-18.1981981981982,-18.15815815815816,-18.11811811811812,-18.07807807807808,-18.03803803803804,-17.997997997998,-17.95795795795796,-17.91791791791792,-17.87787787787788,-17.83783783783784,-17.7977977977978,-17.75775775775776,-17.71771771771772,-17.67767767767768,-17.63763763763764,-17.5975975975976,-17.55755755755756,-17.51751751751752,-17.47747747747748,-17.43743743743744,-17.3973973973974,-17.35735735735736,-17.31731731731732,-17.27727727727728,-17.237237237237238,-17.197197197197198,-17.157157157157158,-17.117117117117118,-17.077077077077078,-17.037037037037038,-16.996996996996998,-16.956956956956958,-16.916916916916918,-16.876876876876878,-16.836836836836838,-16.796796796796798,-16.756756756756758,-16.716716716716718,-16.676676676676678,-16.636636636636638,-16.596596596596598,-16.556556556556558,-16.516516516516518,-16.476476476476478,-16.436436436436438,-16.396396396396398,-16.356356356356358,-16.316316316316318,-16.276276276276278,-16.236236236236238,-16.196196196196198,-16.156156156156158,-16.116116116116117,-16.076076076076077,-16.036036036036037,-15.995995995995996,-15.955955955955956,-15.915915915915916,-15.875875875875876,-15.835835835835836,-15.795795795795796,-15.755755755755755,-15.715715715715715,-15.675675675675675,-15.635635635635635,-15.595595595595595,-15.555555555555555,-15.515515515515515,-15.475475475475475,-15.435435435435435,-15.395395395395395,-15.355355355355355,-15.315315315315315,-15.275275275275275,-15.235235235235235,-15.195195195195195,-15.155155155155155,-15.115115115115115,-15.075075075075075,-15.035035035035035,-14.994994994994995,-14.954954954954955,-14.914914914914915,-14.874874874874875,-14.834834834834835,-14.794794794794795,-14.754754754754755,-14.714714714714715,-14.674674674674675,-14.634634634634635,-14.594594594594595,-14.554554554554555,-14.514514514514515,-14.474474474474475,-14.434434434434435,-14.394394394394395,-14.354354354354355,-14.314314314314315,-14.274274274274275,-14.234234234234235,-14.194194194194194,-14.154154154154154,-14.114114114114114,-14.074074074074074,-14.034034034034034,-13.993993993993994,-13.953953953953954,-13.913913913913914,-13.873873873873874,-13.833833833833834,-13.793793793793794,-13.753753753753754,-13.713713713713714,-13.673673673673674,-13.633633633633634,-13.593593593593594,-13.553553553553552,-13.513513513513512,-13.473473473473472,-13.433433433433432,-13.393393393393392,-13.353353353353352,-13.313313313313312,-13.273273273273272,-13.233233233233232,-13.193193193193192,-13.153153153153152,-13.113113113113112,-13.073073073073072,-13.033033033033032,-12.992992992992992,-12.952952952952952,-12.912912912912912,-12.872872872872872,-12.832832832832832,-12.792792792792792,-12.752752752752752,-12.712712712712712,-12.672672672672672,-12.632632632632632,-12.592592592592592,-12.552552552552552,-12.512512512512512,-12.472472472472472,-12.432432432432432,-12.392392392392392,-12.352352352352352,-12.312312312312311,-12.272272272272271,-12.232232232232231,-12.192192192192191,-12.152152152152151,-12.112112112112111,-12.072072072072071,-12.032032032032031,-11.991991991991991,-11.951951951951951,-11.911911911911911,-11.871871871871871,-11.831831831831831,-11.791791791791791,-11.751751751751751,-11.711711711711711,-11.671671671671671,-11.631631631631631,-11.591591591591591,-11.551551551551551,-11.511511511511511,-11.471471471471471,-11.431431431431431,-11.391391391391391,-11.35135135135135,-11.31131131131131,-11.27127127127127,-11.23123123123123,-11.19119119119119,-11.15115115115115,-11.11111111111111,-11.07107107107107,-11.03103103103103,-10.99099099099099,-10.95095095095095,-10.91091091091091,-10.87087087087087,-10.83083083083083,-10.79079079079079,-10.75075075075075,-10.71071071071071,-10.67067067067067,-10.63063063063063,-10.59059059059059,-10.55055055055055,-10.51051051051051,-10.47047047047047,-10.43043043043043,-10.39039039039039,-10.35035035035035,-10.31031031031031,-10.27027027027027,-10.23023023023023,-10.19019019019019,-10.15015015015015,-10.11011011011011,-10.07007007007007,-10.03003003003003,-9.98998998998999,-9.94994994994995,-9.90990990990991,-9.86986986986987,-9.82982982982983,-9.78978978978979,-9.74974974974975,-9.70970970970971,-9.66966966966967,-9.62962962962963,-9.58958958958959,-9.54954954954955,-9.50950950950951,-9.46946946946947,-9.42942942942943,-9.38938938938939,-9.34934934934935,-9.30930930930931,-9.26926926926927,-9.22922922922923,-9.18918918918919,-9.14914914914915,-9.10910910910911,-9.06906906906907,-9.02902902902903,-8.98898898898899,-8.94894894894895,-8.90890890890891,-8.86886886886887,-8.82882882882883,-8.78878878878879,-8.74874874874875,-8.70870870870871,-8.66866866866867,-8.62862862862863,-8.588588588588589,-8.548548548548549,-8.508508508508509,-8.468468468468469,-8.428428428428429,-8.388388388388389,-8.348348348348349,-8.308308308308309,-8.268268268268269,-8.228228228228229,-8.188188188188189,-8.148148148148149,-8.108108108108109,-8.068068068068069,-8.028028028028029,-7.987987987987989,-7.947947947947949,-7.907907907907909,-7.867867867867869,-7.827827827827829,-7.787787787787789,-7.7477477477477485,-7.7077077077077085,-7.6676676676676685,-7.6276276276276285,-7.5875875875875884,-7.547547547547548,-7.507507507507508,-7.467467467467468,-7.427427427427428,-7.387387387387388,-7.347347347347348,-7.307307307307308,-7.267267267267268,-7.227227227227228,-7.187187187187188,-7.147147147147146,-7.107107107107106,-7.067067067067066,-7.027027027027026,-6.986986986986986,-6.946946946946946,-6.906906906906906,-6.866866866866866,-6.826826826826826,-6.786786786786786,-6.746746746746746,-6.706706706706706,-6.666666666666666,-6.626626626626626,-6.586586586586586,-6.546546546546546,-6.506506506506506,-6.466466466466466,-6.426426426426426,-6.386386386386386,-6.346346346346346,-6.306306306306306,-6.266266266266266,-6.226226226226226,-6.186186186186186,-6.146146146146146,-6.106106106106106,-6.066066066066066,-6.026026026026026,-5.985985985985986,-5.945945945945946,-5.905905905905906,-5.865865865865866,-5.8258258258258255,-5.7857857857857855,-5.7457457457457455,-5.7057057057057055,-5.665665665665665,-5.625625625625625,-5.585585585585585,-5.545545545545545,-5.505505505505505,-5.465465465465465,-5.425425425425425,-5.385385385385385,-5.345345345345345,-5.305305305305305,-5.265265265265265,-5.225225225225225,-5.185185185185185,-5.145145145145145,-5.105105105105105,-5.065065065065065,-5.025025025025025,-4.984984984984985,-4.944944944944945,-4.904904904904905,-4.864864864864865,-4.824824824824825,-4.784784784784785,-4.744744744744745,-4.704704704704705,-4.664664664664665,-4.624624624624625,-4.584584584584585,-4.544544544544545,-4.504504504504505,-4.464464464464465,-4.424424424424425,-4.384384384384385,-4.344344344344345,-4.3043043043043046,-4.2642642642642645,-4.2242242242242245,-4.1841841841841845,-4.1441441441441444,-4.104104104104104,-4.064064064064064,-4.024024024024024,-3.9839839839839826,-3.9439439439439425,-3.9039039039039025,-3.8638638638638625,-3.8238238238238225,-3.7837837837837824,-3.7437437437437424,-3.7037037037037024,-3.6636636636636624,-3.6236236236236223,-3.5835835835835823,-3.5435435435435423,-3.5035035035035023,-3.4634634634634622,-3.423423423423422,-3.383383383383382,-3.343343343343342,-3.303303303303302,-3.263263263263262,-3.223223223223222,-3.183183183183182,-3.143143143143142,-3.103103103103102,-3.063063063063062,-3.023023023023022,-2.982982982982982,-2.942942942942942,-2.902902902902902,-2.862862862862862,-2.822822822822822,-2.782782782782782,-2.7427427427427418,-2.7027027027027017,-2.6626626626626617,-2.6226226226226217,-2.5825825825825817,-2.5425425425425416,-2.5025025025025016,-2.4624624624624616,-2.4224224224224216,-2.3823823823823815,-2.3423423423423415,-2.3023023023023015,-2.2622622622622615,-2.2222222222222214,-2.1821821821821814,-2.1421421421421414,-2.1021021021021014,-2.0620620620620613,-2.0220220220220213,-1.9819819819819813,-1.9419419419419413,-1.9019019019019012,-1.8618618618618612,-1.8218218218218212,-1.7817817817817811,-1.7417417417417411,-1.701701701701701,-1.661661661661661,-1.621621621621621,-1.581581581581581,-1.541541541541541,-1.501501501501501,-1.461461461461461,-1.421421421421421,-1.381381381381381,-1.3413413413413409,-1.3013013013013008,-1.2612612612612608,-1.2212212212212208,-1.1811811811811808,-1.1411411411411407,-1.1011011011011007,-1.0610610610610607,-1.0210210210210207,-0.9809809809809806,-0.9409409409409406,-0.9009009009009006,-0.8608608608608606,-0.8208208208208205,-0.7807807807807805,-0.7407407407407405,-0.7007007007007005,-0.6606606606606604,-0.6206206206206204,-0.5805805805805804,-0.5405405405405403,-0.5005005005005003,-0.4604604604604603,-0.42042042042042027,-0.38038038038038025,-0.3403403403403402,-0.3003003003003002,-0.26026026026026017,-0.22022022022022014,-0.18018018018018012,-0.1401401401401401,-0.10010010010010006,-0.06006006006006004,-0.020020020020020013,0.020020020020020013,0.06006006006006004,0.10010010010010006,0.1401401401401401,0.18018018018018012,0.22022022022022014,0.26026026026026017,0.3003003003003002,0.3403403403403402,0.38038038038038025,0.42042042042042027,0.4604604604604603,0.5005005005005003,0.5405405405405403,0.5805805805805804,0.6206206206206204,0.6606606606606604,0.7007007007007005,0.7407407407407405,0.7807807807807805,0.8208208208208205,0.8608608608608606,0.9009009009009006,0.9409409409409406,0.9809809809809806,1.0210210210210207,1.0610610610610607,1.1011011011011007,1.1411411411411407,1.1811811811811808,1.2212212212212208,1.2612612612612608,1.3013013013013008,1.3413413413413409,1.381381381381381,1.421421421421421,1.461461461461461,1.501501501501501,1.541541541541541,1.581581581581581,1.621621621621621,1.661661661661661,1.701701701701701,1.7417417417417411,1.7817817817817811,1.8218218218218212,1.8618618618618612,1.9019019019019012,1.9419419419419413,1.9819819819819813,2.0220220220220213,2.0620620620620613,2.1021021021021014,2.1421421421421414,2.1821821821821814,2.2222222222222214,2.2622622622622615,2.3023023023023015,2.3423423423423415,2.3823823823823815,2.4224224224224216,2.4624624624624616,2.5025025025025016,2.5425425425425416,2.5825825825825817,2.6226226226226217,2.6626626626626617,2.7027027027027017,2.7427427427427418,2.782782782782782,2.822822822822822,2.862862862862862,2.902902902902902,2.942942942942942,2.982982982982982,3.023023023023022,3.063063063063062,3.103103103103102,3.143143143143142,3.183183183183182,3.223223223223222,3.263263263263262,3.303303303303302,3.343343343343342,3.383383383383382,3.423423423423422,3.4634634634634622,3.5035035035035023,3.5435435435435423,3.5835835835835823,3.6236236236236223,3.6636636636636624,3.7037037037037024,3.7437437437437424,3.7837837837837824,3.8238238238238225,3.8638638638638625,3.9039039039039025,3.9439439439439425,3.9839839839839826,4.024024024024023,4.064064064064063,4.104104104104103,4.144144144144143,4.184184184184183,4.224224224224223,4.264264264264263,4.304304304304303,4.344344344344343,4.384384384384383,4.424424424424423,4.464464464464463,4.504504504504503,4.544544544544543,4.584584584584583,4.624624624624623,4.664664664664663,4.704704704704703,4.744744744744743,4.784784784784783,4.824824824824823,4.864864864864863,4.904904904904903,4.944944944944943,4.984984984984983,5.025025025025023,5.065065065065063,5.105105105105103,5.145145145145143,5.185185185185183,5.225225225225223,5.265265265265263,5.305305305305303,5.3453453453453434,5.3853853853853835,5.4254254254254235,5.4654654654654635,5.5055055055055035,5.545545545545544,5.585585585585584,5.625625625625624,5.665665665665667,5.705705705705707,5.745745745745747,5.785785785785787,5.825825825825827,5.865865865865867,5.905905905905907,5.945945945945947,5.985985985985987,6.026026026026027,6.0660660660660675,6.1061061061061075,6.1461461461461475,6.1861861861861875,6.226226226226228,6.266266266266268,6.306306306306308,6.346346346346348,6.386386386386388,6.426426426426428,6.466466466466468,6.506506506506508,6.546546546546548,6.586586586586588,6.626626626626628,6.666666666666668,6.706706706706708,6.746746746746748,6.786786786786788,6.826826826826828,6.866866866866868,6.906906906906908,6.946946946946948,6.986986986986988,7.027027027027028,7.067067067067068,7.107107107107108,7.147147147147148,7.187187187187188,7.227227227227228,7.267267267267268,7.307307307307308,7.347347347347348,7.387387387387388,7.427427427427428,7.467467467467468,7.507507507507508,7.547547547547548,7.5875875875875884,7.6276276276276285,7.6676676676676685,7.7077077077077085,7.7477477477477485,7.787787787787789,7.827827827827829,7.867867867867869,7.907907907907909,7.947947947947949,7.987987987987989,8.028028028028029,8.068068068068069,8.108108108108109,8.148148148148149,8.188188188188189,8.228228228228229,8.268268268268269,8.308308308308309,8.348348348348349,8.388388388388389,8.428428428428429,8.468468468468469,8.508508508508509,8.548548548548549,8.588588588588589,8.62862862862863,8.66866866866867,8.70870870870871,8.74874874874875,8.78878878878879,8.82882882882883,8.86886886886887,8.90890890890891,8.94894894894895,8.98898898898899,9.02902902902903,9.06906906906907,9.10910910910911,9.14914914914915,9.18918918918919,9.22922922922923,9.26926926926927,9.30930930930931,9.34934934934935,9.38938938938939,9.42942942942943,9.46946946946947,9.50950950950951,9.54954954954955,9.58958958958959,9.62962962962963,9.66966966966967,9.70970970970971,9.74974974974975,9.78978978978979,9.82982982982983,9.86986986986987,9.90990990990991,9.94994994994995,9.98998998998999,10.03003003003003,10.07007007007007,10.11011011011011,10.15015015015015,10.19019019019019,10.23023023023023,10.27027027027027,10.31031031031031,10.35035035035035,10.39039039039039,10.43043043043043,10.47047047047047,10.51051051051051,10.55055055055055,10.59059059059059,10.63063063063063,10.67067067067067,10.71071071071071,10.75075075075075,10.79079079079079,10.83083083083083,10.87087087087087,10.91091091091091,10.95095095095095,10.99099099099099,11.03103103103103,11.07107107107107,11.11111111111111,11.15115115115115,11.19119119119119,11.23123123123123,11.27127127127127,11.31131131131131,11.35135135135135,11.391391391391391,11.431431431431431,11.471471471471471,11.511511511511511,11.551551551551551,11.591591591591591,11.631631631631631,11.671671671671671,11.711711711711711,11.751751751751751,11.791791791791791,11.831831831831831,11.871871871871871,11.911911911911911,11.951951951951951,11.991991991991991,12.032032032032035,12.072072072072075,12.112112112112115,12.152152152152155,12.192192192192195,12.232232232232235,12.272272272272275,12.312312312312315,12.352352352352355,12.392392392392395,12.432432432432435,12.472472472472475,12.512512512512515,12.552552552552555,12.592592592592595,12.632632632632635,12.672672672672675,12.712712712712715,12.752752752752755,12.792792792792795,12.832832832832835,12.872872872872875,12.912912912912915,12.952952952952955,12.992992992992995,13.033033033033036,13.073073073073076,13.113113113113116,13.153153153153156,13.193193193193196,13.233233233233236,13.273273273273276,13.313313313313316,13.353353353353356,13.393393393393396,13.433433433433436,13.473473473473476,13.513513513513516,13.553553553553556,13.593593593593596,13.633633633633636,13.673673673673676,13.713713713713716,13.753753753753756,13.793793793793796,13.833833833833836,13.873873873873876,13.913913913913916,13.953953953953956,13.993993993993996,14.034034034034036,14.074074074074076,14.114114114114116,14.154154154154156,14.194194194194196,14.234234234234236,14.274274274274276,14.314314314314316,14.354354354354356,14.394394394394396,14.434434434434436,14.474474474474476,14.514514514514516,14.554554554554556,14.594594594594597,14.634634634634637,14.674674674674677,14.714714714714717,14.754754754754757,14.794794794794797,14.834834834834837,14.874874874874877,14.914914914914917,14.954954954954957,14.994994994994997,15.035035035035037,15.075075075075077,15.115115115115117,15.155155155155157,15.195195195195197,15.235235235235237,15.275275275275277,15.315315315315317,15.355355355355357,15.395395395395397,15.435435435435437,15.475475475475477,15.515515515515517,15.555555555555557,15.595595595595597,15.635635635635637,15.675675675675677,15.715715715715717,15.755755755755757,15.795795795795797,15.835835835835837,15.875875875875877,15.915915915915917,15.955955955955957,15.995995995995997,16.036036036036037,16.076076076076077,16.116116116116117,16.156156156156158,16.196196196196198,16.236236236236238,16.276276276276278,16.316316316316318,16.356356356356358,16.396396396396398,16.436436436436438,16.476476476476478,16.516516516516518,16.556556556556558,16.596596596596598,16.636636636636638,16.676676676676678,16.716716716716718,16.756756756756758,16.796796796796798,16.836836836836838,16.876876876876878,16.916916916916918,16.956956956956958,16.996996996996998,17.037037037037038,17.077077077077078,17.117117117117118,17.157157157157158,17.197197197197198,17.237237237237238,17.27727727727728,17.31731731731732,17.35735735735736,17.3973973973974,17.43743743743744,17.47747747747748,17.51751751751752,17.55755755755756,17.5975975975976,17.63763763763764,17.67767767767768,17.71771771771772,17.75775775775776,17.7977977977978,17.83783783783784,17.87787787787788,17.91791791791792,17.95795795795796,17.997997997998,18.03803803803804,18.07807807807808,18.11811811811812,18.15815815815816,18.1981981981982,18.23823823823824,18.27827827827828,18.31831831831832,18.35835835835836,18.3983983983984,18.43843843843844,18.47847847847848,18.51851851851852,18.55855855855856,18.5985985985986,18.63863863863864,18.67867867867868,18.71871871871872,18.75875875875876,18.7987987987988,18.83883883883884,18.87887887887888,18.91891891891892,18.95895895895896,18.998998998999,19.03903903903904,19.07907907907908,19.11911911911912,19.15915915915916,19.1991991991992,19.23923923923924,19.27927927927928,19.31931931931932,19.35935935935936,19.3993993993994,19.43943943943944,19.47947947947948,19.51951951951952,19.55955955955956,19.5995995995996,19.63963963963964,19.67967967967968,19.71971971971972,19.75975975975976,19.7997997997998,19.83983983983984,19.87987987987988,19.91991991991992,19.95995995995996,20],"y":[4.387241946958801e-8,4.674626267473745e-8,4.980154031836557e-8,5.30492473801269e-8,5.650101510782568e-8,6.01691459395116e-8,6.40666502265289e-8,6.820728484361389e-8,7.260559377583873e-8,7.727695077610896e-8,8.22376041908591e-8,8.750472405587734e-8,9.309645156837709e-8,9.903195104602923e-8,1.053314644882184e-7,1.1201636885963575e-7,1.1910923622134042e-7,1.2663389683943182e-7,1.3461550540703173e-7,1.4308061052057395e-7,1.5205722755727354e-7,1.6157491510649062e-7,1.7166485511376955e-7,1.8235993690278048e-7,1.9369484524673877e-7,2.057061526677752e-7,2.1843241614959563e-7,2.3191427845591694e-7,2.461945742547978e-7,2.6131844125623744e-7,2.7733343657876245e-7,2.942896585684191e-7,3.12239874302341e-7,3.3123965301739097e-7,3.513475057134466e-7,3.7262503119006614e-7,3.9513706878446943e-7,4.189518580887783e-7,4.4414120593412893e-7,4.707806609398382e-7,4.989496959358464e-7,5.287318985781899e-7,5.602151704875125e-7,5.934919352528113e-7,6.286593556538592e-7,6.658195604677742e-7,7.050798812377233e-7,7.465530993941931e-7,7.903577041322326e-7,8.36618161461212e-7,8.854651948574077e-7,9.370360779633068e-7,9.914749397918303e-7,0.0000010489330829083018,0.000001109569315077374,0.0000011735502948776968,0.0000012410508918022246,0.000001312254561377995,0.0000013873537358548585,0.0000014665502310300255,0.0000015500556697901873,0.0000016380919229719275,0.000001730891568156728,0.0000018286983670357506,0.0000019317677619961532,0.000002040367392600059,0.000002154777632644675,0.0000022752921485113114,0.000002402218479529526,0.0000025358786411026486,0.000002676609751359002,0.0000028247646821139823,0.0000029807127349470696,0.000003144840343219203,0.00000331755180087388,0.0000034992700188888188,0.0000036904373102623734,0.000003891516204442657,0.0000041029902921253984,0.000004325365101369344,0.000004559169005998538,0.000004804954167281041,0.000005063297509896246,0.000005334801733223251,0.000005620096359002866,0.00000591983881644885,0.000006234715565903882,0.000006565443262155529,0.000006912769958551599,0.00000727747635307104,0.000007660377077528074,0.000008062322031109888,0.000008484197759463184,0.000008926928880571287,0.000009391479558673253,0.000009878855027506765,0.000010390103164163104,0.00001092631611487023,0.00001148863197403079,0.000012078236517861838,0.000012696364993998008,0.000013344303968436589,0.000014023393231214271,0.000014735027762223829,0.000015480659758586695,0.000016261800725017523,0.0000170800236286158,0.00001793696511954362,0.00001883432781904418,0.000019773882676274332,0.0000207574713954237,0.000021787008934604114,0.000022864486077988366,0.000023991972082688322,0.000025171617401859933,0.000026405656485518696,0.00002769641066055113,0.00002904629109140447,0.000030457801822917822,0.000031933542906777776,0.000033476213613030404,0.00003508861572811531,0.00003677365694082925,0.000038534354317649873,0.000040373837868792476,0.00004229535420638976,0.00004430227029612471,0.000046398077303645614,0.00004858639453704514,0.00005087097348667377,0.000053255701963495975,0.000055744608337180326,0.0000583418658750624,0.00006105179718308016,0.00006387887874972526,0.0000668277455940253,0.00006990319601846841,0.00007311019646778893,0.00007645388649441003,0.00007993958383131726,0.00008357278957302617,0.00008735919346528836,0.00009130467930403765,0.00009541533044405163,0.00009969743541767759,0.00010415749366391342,0.00010880222136800079,0.00011363855741164694,0.00011867366943380134,0.00012391496000192125,0.00012937007289340208,0.00013504689948688558,0.0001409535852628871,0.00014709853641320787,0.0001534904265583022,0.0001601382035717797,0.00016705109651097855,0.00017423862265244909,0.0001817105946310173,0.00018947712768094794,0.00019754864697754288,0.0002059358950773763,0.00021464993945516733,0.00022370218013511667,0.00023310435741435854,0.00024286855967597994,0.0002530072312889137,0.00026353318059167685,0.0002744595879569221,0.000285800013933366,0.0002975684074615845,0.00030977911415987994,0.00032244688467622247,0.0003355868831019481,0.0003492146954429124,0.00036334633814319266,0.0003779982666565771,0.00039318738406047247,0.000408931049706924,0.0004252470879049512,0.0004421537966282549,0.0004596699562421037,0.0004778148382428708,0.0004966082140034847,0.0005160703635176654,0.0005362220841358502,0.0005570846992849016,0.0005786800671640127,0.0006010305894085166,0.0006241592197131035,0.0006480894724058573,0.0006728454309639683,0.0006984517564619021,0.0007249336959422798,0.0007523170906998113,0.0007806283844678302,0.0008098946314971694,0.0008401435045164382,0.0008714033025627959,0.000903702958671763,0.0009370720474144898,0.0009715407922705504,0.0010071400728240666,0.0010439014317706486,0.00108185708172252,0.0011210399117986207,0.0011614834939865078,0.0012032220892625382,0.001246290653456345,0.0012907248428459,0.0013365610194683805,0.0013838362561326921,0.001432588341118659,0.0014828557825481902,0.0015346778124125668,0.0015880943902414548,0.0016431462063969627,0.0016998746849779054,0.0017583219863174478,0.0018185310090590163,0.0018805453917932276,0.0019444095142401663,0.002010168497960206,0.0020778682065766547,0.0021475552454938026,0.0022192769610934965,0.0022930814393930174,0.0023690175041483517,0.002447134714384598,0.0025274833613377137,0.002610114464790066,0.0026950797687834206,0.002782431736691607,0.002872223545637581,0.0029645090802365576,0.0030593429256497917,0.003156780359932286,0.00325687734565769,0.0033596905208052395,0.003465277188891871,0.003573695308334274,0.0036850034810257497,0.0037992609401116704,0.003916527536950666,0.004036863727244198,0.004160330556323745,0.004286989643578009,0.004416903166010494,0.004550133840911486,0.004686744907634235,0.004826800108461009,0.004970363668550303,0.005117500274951568,0.005268275054678834,0.005422753551832626,0.005581001703761544,0.005743085816254432,0.005909072537756086,0.006079028832598319,0.006253021953241712,0.006431119411519707,0.006613388948884168,0.006799898505644531,0.006990716189200411,0.007185910241263063,0.0073855490040669524,0.007589700885568303,0.007798434323633837,0.008011817749219443,0.008229919548542233,0.008452808024248895,0.008680551355587013,0.008913217557581782,0.009150874439228868,0.00939358956070772,0.00964143018962897,0.009894463256320601,0.010152755308170764,0.010416372463034009,0.010685380361720401,0.010959844119578208,0.011239828277190354,0.011525396750200105,0.01181661277828566,0.012113538873304722,0.01241623676663025,0.01272476735570116,0.013039190649810291,0.013359565715158742,0.01368595061920079,0.014018402374307633,0.01435697688078515,0.014701728869267752,0.015052711842527685,0.01540997801673332,0.015773578262187865,0.016143562043586197,0.01651997735983184,0.01690287068344791,0.017292286899622283,0.0176882692449366,0.018090859245811334,0.018500096656717868,0.018916019398202782,0.019338663494769457,0.019768063012662828,0.02020424999761291,0.020647254412580814,0.02109710407555921,0.02155382459748752,0.022017439320321932,0.0224879692553265,0.022965433021635787,0.023449846785143235,0.023941224197777455,0.02443957633722237,0.024944911647138204,0.025457235877942085,0.02597655202821958,0.026502860286807903,0.027036157975633378,0.027576439493350837,0.02812369625985245,0.028677916661712193,0.029239085998622426,0.029807186430892068,0.03038219692806438,0.0309640932187286,0.03155284774157564,0.0321484295977743,0.032750804504723535,0.03335993475125203,0.03397577915431994,0.03459829301729821,0.03522742808987689,0.035863132529673894,0.03650535086559764,0.03715402396304173,0.037809088990948994,0.03847047939082964,0.039138124847773535,0.03981195126353013,0.04049188073170486,0.04117783151513008,0.041869718025472805,0.042567450805122614,0.04327093651142367,0.04398007790329609,0.04469477383030055,0.04541491922419394,0.046140405093019865,0.04687111851779019,0.04760694265178582,0.04834775672253865,0.04909343603651116,0.04984385198653602,0.05059887206203531,0.0513583598620605,0.05212217511118272,0.05289017367826527,0.05366220759814575,0.05443812509625041,0.05521777061616056,0.05600098485016804,0.05678760477280808,0.05757746367741662,0.05837039121569679,0.0591662134403246,0.05996475285059169,0.06076582844108031,0.061569255753398236,0.06237484693093851,0.06318241077668688,0.06399175281405382,0.064802675350726,0.06561497754552237,0.06642845547823949,0.06724290222246689,0.0680581079213434,0.06887385986623765,0.069689942578322,0.07050613789299787,0.07132222504715287,0.0721379807691985,0.07295317937185392,0.07376759284763015,0.07458099096696451,0.07539314137896101,0.07620380971467053,0.0770127596928708,0.07781975322827511,0.07862455054210803,0.07942691027498666,0.08022658960203778,0.08102334435017858,0.08181692911748352,0.08260709739457162,0.08339360168791995,0.08417619364503964,0.0849546241814038,0.08572864360906775,0.08649800176688183,0.08726244815218945,0.08802173205394628,0.08877560268713532,0.08952380932840512,0.09026610145280366,0.09100222887153374,0.0917319418706077,0.09245499135029467,0.09317112896527133,0.09388010726534511,0.09458167983664978,0.09527560144321055,0.09596162816874872,0.09663951755862284,0.09730902876179089,0.09796992267267698,0.0986219620728209,0.09926491177220881,0.09989853875014673,0.10052261229557627,0.10113690414670716,0.1017411886298556,0.10233524279735134,0.10291884656442458,0.10349178284493543,0.10405383768582789,0.10460480040020496,0.10514446369889996,0.10567262382042901,0.10618908065921392,0.1066936378919666,0.10718610310210493,0.10766628790211871,0.10813400805374894,0.10858908358589073,0.1090313389101029,0.10946060293363737,0.10987670916985785,0.1102794958459771,0.11066880600800572,0.1110444876227972,0.11140639367713388,0.11175438227373026,0.11208831672407667,0.11240806563804916,0.11271350301018991,0.1130045083025767,0.11328096652421576,0.11354276830688745,0.1137898099773497,0.11402199362586644,0.1142392271709722,0.1144414244204221,0.11462850512826962,0.11480039504803019,0.11495702598184937,0.11509833582567144,0.11522426861033791,0.11533477453858634,0.11542981001791047,0.11550933768926964,0.11557332645158976,0.11562175148206366,0.1156545942522161,0.11567184253971632,0.11567349043594272,0.11565953834927369,0.11562999300412334,0.1155848674357022,0.11552418098052462,0.11544795926266392,0.1153562341757757,0.11524904386089704,0.115126432680054,0.11498845118569455,0.11483515608598394,0.11466661020598433,0.11448288244477205,0.1142840477285183,0.11407018695958603,0.11384138696169323,0.11359774042118388,0.11333934582447527,0.11306630739173457,0.11277873500684489,0.11247674414372655,0.1121604557890984,0.11182999636171835,0.11148549762821586,0.11112709661557316,0.11075493552033389,0.11036916161464026,0.10996992714916859,0.10955738925306509,0.10913170983096235,0.10869305545718848,0.108241597267239,0.10777751084663861,0.10730097611727732,0.10681217722132269,0.10631130240282442,0.10579854388710747,0.10527409775806276,0.10473816383345134,0.10419094553832765,0.10363264977669401,0.10306348680149746,0.102483670083093,0.10189341617627415,0.10129294458599143,0.1006824776318848,0.10006224031172854,0.09943246016392286,0.09879336712913228,0.09814519341120648,0.09748817333748433,0.09682254321860403,0.09614854120794174,0.09546640716078322,0.09477638249335167,0.09407871004180426,0.093373633921318,0.09266139938535664,0.09194225268526188,0.09121644093025373,0.0904842119479622,0.08974581414559264,0.08900149637183984,0.08825150777964401,0.08749609768990424,0.08673551545624308,0.08597001033092107,0.0851998313320099,0.08442522711190505,0.0836464458272857,0.08286373501060006,0.08207734144317913,0.08128751103005112,0.08049448867655666,0.07969851816683149,0.07889984204424547,0.07809870149387359,0.07729533622706898,0.07648998436821279,0.07568288234370832,0.07487426477328486,0.07406436436367349,0.07325341180471817,0.07244163566797027,0.07162926230783608,0.07081651576530917,0.07000361767435151,0.06919078717096142,0.06837824080496595,0.06756619245458918,0.06675485324381947,0.06594443146261406,0.06513513248997356,0.0643271587199043,0.0635207094903021,0.0627159810147729,0.061913166317406336,0.06111245517051866,0.06031403403538088,0.05951808600593217,0.058724790755491,0.057934324486465245,0.0571468598830589,0.056362566066975546,0.055581608556112966,0.05480414922623663,0.05403034627562741,0.053260354192684746,0.05249432372647005,0.051732401860173576,0.05097473178748506,0.05022145289183591,0.04947270072849873,0.04872860700950917,0.04798929959137793,0.047254902465567954,0.0465255357516944,0.0458013156934147,0.045082354656970136,0.04436876113233711,0.04366063973694623,0.04295809122192499,0.04226121248082061,0.04157009656075156,0.04088483267594283,0.04020550622359384,0.03953219880202494,0.03886498823105458,0.03820394857454857,0.03754915016508885,0.036900659630704644,0.0362585399236105,0.035622850350890134,0.03499364660707194,0.034370980808531085,0.033754901529663345,0.03314545384076672,0.03254267934757042,0.03194661623235054,0.03135729929656901,0.03077476000497394,0.0301990265311003,0.02963012380410669,0.029068073556885496,0.02851289437538497,0.02796460174908099,0.027423208122533068,0.026888722947966185,0.02636115273881328,0.025840501124159226,0.025326768904023925,0.02481995410542241,0.02432005203914336,0.023827055357187033,0.0233409541107994,0.022861735809048985,0.022389385477884726,0.021923885719619337,0.021465216772782413,0.02101335657228589,0.020568280809849127,0.020129962994629255,0.019698374514003743,0.01927348469445349,0.018855260862496483,0.018443668405621835,0.018038670833174716,0.017640229837146842,0.017248305352824435,0.01686285561924861,0.016483837239445468,0.016111205240382127,0.01574491313260679,0.015384912969533749,0.01503115540633352,0.014683589758390249,0.014342164059290098,0.01400682511830554,0.013677518577341879,0.013354188967311786,0.013036779763909156,0.01272523344275006,0.012419491533852947,0.01211949467543082,0.011825182666969036,0.011536494521564236,0.011253368517499866,0.01097574224903815,0.010703552676405085,0.010436736174950783,0.010175228583465723,0.009918965251635888,0.009667881086621028,0.009421910598741434,0.00918098794625942,0.008945046979242949,0.008714021282501422,0.008487844217581995,0.008266448963818665,0.00804976855842628,0.007837735935632075,0.007630283964839986,0.007427345487822768,0.007228853354938115,0.007034740460366953,0.006844939776371543,0.006659384386573511,0.006478007518251334,0.006300742573659384,0.006127523160369761,0.005958283120640295,0.005792956559812559,0.005631477873744041,0.005473781775280088,0.005319803319771599,0.005169477929645116,0.005022741418032902,0.004879530011470999,0.004739780371674068,0.004603429616396341,0.004470415339388393,0.004340675629460559,0.004214149088663692,0.004090774849598736,0.003970492591867174,0.0038532425576747417,0.003738965566600998,0.003627603029548211,0.003519096961882815,0.0034133899957836176,0.0033104253918106942,0.0032101470497096665,0.003112499518465899,0.003017428005623825,0.0029248783858865,0.0028347972090107328,0.0027471317070134945,0.002661829800705268,0.002578840105566189,0.0024981119369809996,0.0024195953148487617,0.0023432409675835956,0.002269000335522519,0.002196825573756623,0.0021266695544018,0.0020584858683252625,0.001992228826343961,0.0019278534599112137,0.0018653155213074638,0.001804571483351321,0.0017455785386468246,0.0016882945983827042,0.0016326782906994762,0.0015786889586399226,0.001526286657698451,0.001475432152984691,0.0014260869160164717,0.001378213121157196,0.001331773641712419,0.0012867320457003117,0.0012430525913103838,0.0012007002220647256,0.0011596405616958057,0.001119839908754568,0.0010812652309624333,0.001043884159320524,0.0010076649819891754,0.0009725766379506123,0.000938588710467366,0.0009056714203487654,0.000873795619037568,0.0008429327815285685,0.0008130549991307215,0.0007841349720840381,0.0007561460020422959,0.0007290619844322781,0.0007028574007000028,0.0006775073104541351,0.0006529873435164969,0.0006292736918892774,0.0006063431016483567,0.0005841728647717708,0.000562740810912156,0.000542025299121691,0.0005220052095378091,0.0005026599350376571,0.00048396937286902044,0.00046591391626516794,0.0004484744460508055,0.0004316323222460491,0.0004153693756750945,0.0003996678995859678,0.0003845106412875245,0.0003698807938095902,0.0003557619875918985,0.0003421382822072138,0.0003289941581238385,0.0003163145085124141,0.0003040846311017181,0.0002922902200879304,0.0002809173581016204,0.00026995250823647066,0.0002593825061435669,0.00024919455219483516,0.0002393762037190387,0.00022991536731352654,0.00022080029123472362,0.00021201955787017535,0.00020356207629476384,0.00019541707491353526,0.00018757409419339113,0.00018002297948573328,0.00017275387394198447,0.00016575721152373442,0.0001590237101091099,0.0001525443646968183,0.0001463104407091498,0.000140313467395101,0.0001345452313346269,0.00012899777004489582,0.0001236633656893056,0.0001185345388898766,0.00011360404264352966,0.00010886485634263177,0.0001043101799000925,0.00009993342797917294,0.00009572822432807809,0.00009168839621929982,0.00008780796899358524,0.00008408116070831204,0.00008050237688997368,0.00007706620539038493,0.00007376741134615409,0.00007060093224087778,0.00006756187306946056,0.00006464550160388065,0.00006184724375967184,0.00005916267906232184,0.000056587536212741776,0.00005411768875089781,0.000051749150816655045,0.0000494780730068367,0.00004730073832745131,0.000045213558240008936,0.000043213068800807744,0.00004129592689203403,0.00003945890654349136,0.000037698895343747375,0.000036012890939451216,0.00003439799762156196,0.00003285142299720005,0.00003137047474581535,0.000029952557458349715,0.0000285951695580575,0.000027295900301633028,0.000026052426859283736,0.000024862511472380745,0.000023723998687308703,0.0000226348126641329,0.000021592954558699234,0.000020596499976777977,0.00001964359649886338,0.00001873246127424385,0.000017861378682955013,0.000017028698064237118,0.000016232831510119897,0.00001547225172276472,0.000014745489934201946,0.000014051133887109243,0.000013387825875285964,0.000012754260842488486,0.000012149184538304388,0.000011571391729753004,0.000011019724467314007,0.000010493070404099864,0.000009990361166900122,0.000009510570777842322,0.000009052714125428778,0.000008615845483724591,0.000008199057078488256,0.000007801477699054218,0.000007422271354792523,0.000007060635974989362,0.000006715802151010152,0.000006387031919624355,0.000006073617586390522,0.0000057748805880184635,0.0000054901703926438794,0.000005218863436970679,0.000004960362099254656,0.0000047140937071218716,0.000004479509579233897,0.000004256084099832428,0.000004043313825213873,0.000003840716621204933,0.0000036478308307290847,0.0000034642144705731,0.0000032894444564823392,0.0000031231158557323254,0.0000029648411663431665,0.000002814249622122772,0.0000026709865227428267,0.000002534712588070767,0.0000024051033359991956,0.000002281848483032283,0.0000021646513669071077,0.000002053228390545681,0.000001947308486650809,0.0000018466326022767402,0.0000017509532027226895,0.00000166003379411411,0.000001573648464053496,0.0000014915814397390743,0.0000014136266629656987,0.0000013395873814384443,0.0000012692757558450696,0.000001202512482148859,0.0000011391264285785624,0.0000010789542868071283,0.000001021840236825271,9.676356250305516e-7,9.161986550664371e-7,8.67394090959701e-7,8.210929721179049e-7,7.771723397620001e-7,7.355149743818823e-7,6.960091438153878e-7,6.585483615636476e-7,6.230311549676035e-7,5.893608428824316e-7,5.574453224980486e-7,5.271968649650788e-7,4.985319194966724e-7,4.7137092562723884e-7,4.4563813331952355e-7,4.212614306216737e-7,3.981721785857942e-7,3.7630505316914116e-7,3.5559789384843766e-7,3.359915586870184e-7,3.174297856032471e-7,2.998590595974197e-7,2.832284857026829e-7,2.6748966743365274e-7,2.5259659051439807e-7,2.3850551167509495e-7,2.2517485231416364e-7,2.1256509682992992e-7,2.0063869543293075e-7,1.8935997125675898e-7,1.786950315920076e-7,1.6861168307426728e-7,1.590793506633581e-7,1.500690002570051e-7,1.415530647880051e-7,1.3350537365956752e-7,1.2590108537901318e-7,1.1871662325528247e-7,1.1192961403083167e-7,1.0551882932346743e-7,9.946412975843171e-8,9.374641167570478e-8,8.834755630195328e-8,8.325038128087716e-8,7.843859445988505e-8,7.389674983506716e-8,6.961020556030788e-8,6.556508393015808e-8,6.174823324969953e-8,5.8147191508131236e-8,5.475015177617984e-8,5.154592925069404e-8,4.8523929872900307e-8,4.567412044983394e-8,4.2987000211357835e-8,4.045357373798496e-8,3.806532519742179e-8,3.5814193830344444e-8,3.369255062841807e-8,3.1693176149979863e-8,2.980923942111774e-8,2.8034277872100325e-8,2.6362178261257954e-8,2.4787158540471696e-8,2.3303750618401878e-8,2.1906783979489378e-8,2.0591370118591995e-8,1.9352887752865285e-8,1.8186968774186227e-8,1.7089484907032345e-8,1.6056535038280986e-8,1.5084433186881498e-8,1.4169697082784191e-8,1.3309037325876448e-8,1.2499347096993451e-8,1.173769239432979e-8,1.1021302769785434e-8,1.0347562540937859e-8,9.714002455441368e-9,9.118291785715707e-9,8.558230832805174e-9,8.031743819263635e-9,7.536872151852532e-9,7.0717680357336524e-9,6.634688422691735e-9,6.223989276740219e-9,5.838120141245542e-9,5.475618992452981e-9,5.135107365013039e-9,4.815285735790714e-9,4.5149291528936755e-9,4.2328830974801445e-9,3.96805956650421e-9,3.7194333651258384e-9,3.4860385980580703e-9,3.2669653496434948e-9,3.0613565429482253e-9,2.8684049686359843e-9,2.687350474836586e-9,2.5174773096549673e-9,2.3581116083779048e-9,2.2086190178291054e-9,2.0684024506966275e-9,1.936899963014286e-9,1.8135827483188515e-9,1.697953242328663e-9,1.5895433322989159e-9,1.4879126655028901e-9,1.3926470515689853e-9,1.3033569536703805e-9,1.219676063818723e-9,1.1412599577549852e-9,1.0677848251614476e-9]},{"name":"Bar Plot","type":"bar","x":[0,3,2,4,12,28,7,17,10,-3,9,21,52,32,8,1,55,25],"y":[0.34375,0.09375,0.0625,0.0625,0.03125,0.03125,0.03125,0.03125,0.03125,0.03125,0.03125,0.03125,0.03125,0.03125,0.03125,0.03125,0.03125,0.03125]}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Combined Line and Bar Plot"},"xaxis":{"title":{"text":"X-Axis"}},"yaxis":{"title":{"text":"Y-Axis"}}}}},"metadata":{},"output_type":"display_data"}],"source":["import numpy as np\n","import plotly.graph_objs as go\n","from scipy.stats import exponnorm\n","from scipy.optimize import curve_fit\n","\n","def exponnorm_pdf(x, loc, scale, K):\n","    return exponnorm.pdf(x, K, loc=loc, scale=scale)\n","\n","value_counts = pd.Series(Y_test.astype(\"int32\")).value_counts(normalize=True)\n","\n","# Create a DataFrame for plotting\n","df = pd.DataFrame({'Class': value_counts.index, 'Percentage': value_counts.values})\n","\n","# Create a figure\n","fig = go.Figure()\n","x = np.linspace(-20, 20, 1000)\n","\n","# Add the line plot\n","fig.add_trace(go.Scatter(x=x, y=exponnorm_pdf(x, loc = loc_hat, scale = scale_hat, K = rate_hat), \n","                         mode='lines', name='Fitted Exponnorm PDF', line=dict(color='red')))\n","\n","# Add the bar plot\n","fig.add_trace(go.Bar(x=df['Class'], y=df['Percentage'], name='Bar Plot'))\n","\n","# Update layout and labels\n","fig.update_layout(title='Combined Line and Bar Plot', xaxis=dict(title='X-Axis'), yaxis=dict(title='Y-Axis'))\n","\n","# Show the combined plot\n","fig.show()"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"line":{"color":"red"},"mode":"lines","name":"Fitted Exponnorm PDF","type":"scatter","x":[-20,-19.95995995995996,-19.91991991991992,-19.87987987987988,-19.83983983983984,-19.7997997997998,-19.75975975975976,-19.71971971971972,-19.67967967967968,-19.63963963963964,-19.5995995995996,-19.55955955955956,-19.51951951951952,-19.47947947947948,-19.43943943943944,-19.3993993993994,-19.35935935935936,-19.31931931931932,-19.27927927927928,-19.23923923923924,-19.1991991991992,-19.15915915915916,-19.11911911911912,-19.07907907907908,-19.03903903903904,-18.998998998999,-18.95895895895896,-18.91891891891892,-18.87887887887888,-18.83883883883884,-18.7987987987988,-18.75875875875876,-18.71871871871872,-18.67867867867868,-18.63863863863864,-18.5985985985986,-18.55855855855856,-18.51851851851852,-18.47847847847848,-18.43843843843844,-18.3983983983984,-18.35835835835836,-18.31831831831832,-18.27827827827828,-18.23823823823824,-18.1981981981982,-18.15815815815816,-18.11811811811812,-18.07807807807808,-18.03803803803804,-17.997997997998,-17.95795795795796,-17.91791791791792,-17.87787787787788,-17.83783783783784,-17.7977977977978,-17.75775775775776,-17.71771771771772,-17.67767767767768,-17.63763763763764,-17.5975975975976,-17.55755755755756,-17.51751751751752,-17.47747747747748,-17.43743743743744,-17.3973973973974,-17.35735735735736,-17.31731731731732,-17.27727727727728,-17.237237237237238,-17.197197197197198,-17.157157157157158,-17.117117117117118,-17.077077077077078,-17.037037037037038,-16.996996996996998,-16.956956956956958,-16.916916916916918,-16.876876876876878,-16.836836836836838,-16.796796796796798,-16.756756756756758,-16.716716716716718,-16.676676676676678,-16.636636636636638,-16.596596596596598,-16.556556556556558,-16.516516516516518,-16.476476476476478,-16.436436436436438,-16.396396396396398,-16.356356356356358,-16.316316316316318,-16.276276276276278,-16.236236236236238,-16.196196196196198,-16.156156156156158,-16.116116116116117,-16.076076076076077,-16.036036036036037,-15.995995995995996,-15.955955955955956,-15.915915915915916,-15.875875875875876,-15.835835835835836,-15.795795795795796,-15.755755755755755,-15.715715715715715,-15.675675675675675,-15.635635635635635,-15.595595595595595,-15.555555555555555,-15.515515515515515,-15.475475475475475,-15.435435435435435,-15.395395395395395,-15.355355355355355,-15.315315315315315,-15.275275275275275,-15.235235235235235,-15.195195195195195,-15.155155155155155,-15.115115115115115,-15.075075075075075,-15.035035035035035,-14.994994994994995,-14.954954954954955,-14.914914914914915,-14.874874874874875,-14.834834834834835,-14.794794794794795,-14.754754754754755,-14.714714714714715,-14.674674674674675,-14.634634634634635,-14.594594594594595,-14.554554554554555,-14.514514514514515,-14.474474474474475,-14.434434434434435,-14.394394394394395,-14.354354354354355,-14.314314314314315,-14.274274274274275,-14.234234234234235,-14.194194194194194,-14.154154154154154,-14.114114114114114,-14.074074074074074,-14.034034034034034,-13.993993993993994,-13.953953953953954,-13.913913913913914,-13.873873873873874,-13.833833833833834,-13.793793793793794,-13.753753753753754,-13.713713713713714,-13.673673673673674,-13.633633633633634,-13.593593593593594,-13.553553553553552,-13.513513513513512,-13.473473473473472,-13.433433433433432,-13.393393393393392,-13.353353353353352,-13.313313313313312,-13.273273273273272,-13.233233233233232,-13.193193193193192,-13.153153153153152,-13.113113113113112,-13.073073073073072,-13.033033033033032,-12.992992992992992,-12.952952952952952,-12.912912912912912,-12.872872872872872,-12.832832832832832,-12.792792792792792,-12.752752752752752,-12.712712712712712,-12.672672672672672,-12.632632632632632,-12.592592592592592,-12.552552552552552,-12.512512512512512,-12.472472472472472,-12.432432432432432,-12.392392392392392,-12.352352352352352,-12.312312312312311,-12.272272272272271,-12.232232232232231,-12.192192192192191,-12.152152152152151,-12.112112112112111,-12.072072072072071,-12.032032032032031,-11.991991991991991,-11.951951951951951,-11.911911911911911,-11.871871871871871,-11.831831831831831,-11.791791791791791,-11.751751751751751,-11.711711711711711,-11.671671671671671,-11.631631631631631,-11.591591591591591,-11.551551551551551,-11.511511511511511,-11.471471471471471,-11.431431431431431,-11.391391391391391,-11.35135135135135,-11.31131131131131,-11.27127127127127,-11.23123123123123,-11.19119119119119,-11.15115115115115,-11.11111111111111,-11.07107107107107,-11.03103103103103,-10.99099099099099,-10.95095095095095,-10.91091091091091,-10.87087087087087,-10.83083083083083,-10.79079079079079,-10.75075075075075,-10.71071071071071,-10.67067067067067,-10.63063063063063,-10.59059059059059,-10.55055055055055,-10.51051051051051,-10.47047047047047,-10.43043043043043,-10.39039039039039,-10.35035035035035,-10.31031031031031,-10.27027027027027,-10.23023023023023,-10.19019019019019,-10.15015015015015,-10.11011011011011,-10.07007007007007,-10.03003003003003,-9.98998998998999,-9.94994994994995,-9.90990990990991,-9.86986986986987,-9.82982982982983,-9.78978978978979,-9.74974974974975,-9.70970970970971,-9.66966966966967,-9.62962962962963,-9.58958958958959,-9.54954954954955,-9.50950950950951,-9.46946946946947,-9.42942942942943,-9.38938938938939,-9.34934934934935,-9.30930930930931,-9.26926926926927,-9.22922922922923,-9.18918918918919,-9.14914914914915,-9.10910910910911,-9.06906906906907,-9.02902902902903,-8.98898898898899,-8.94894894894895,-8.90890890890891,-8.86886886886887,-8.82882882882883,-8.78878878878879,-8.74874874874875,-8.70870870870871,-8.66866866866867,-8.62862862862863,-8.588588588588589,-8.548548548548549,-8.508508508508509,-8.468468468468469,-8.428428428428429,-8.388388388388389,-8.348348348348349,-8.308308308308309,-8.268268268268269,-8.228228228228229,-8.188188188188189,-8.148148148148149,-8.108108108108109,-8.068068068068069,-8.028028028028029,-7.987987987987989,-7.947947947947949,-7.907907907907909,-7.867867867867869,-7.827827827827829,-7.787787787787789,-7.7477477477477485,-7.7077077077077085,-7.6676676676676685,-7.6276276276276285,-7.5875875875875884,-7.547547547547548,-7.507507507507508,-7.467467467467468,-7.427427427427428,-7.387387387387388,-7.347347347347348,-7.307307307307308,-7.267267267267268,-7.227227227227228,-7.187187187187188,-7.147147147147146,-7.107107107107106,-7.067067067067066,-7.027027027027026,-6.986986986986986,-6.946946946946946,-6.906906906906906,-6.866866866866866,-6.826826826826826,-6.786786786786786,-6.746746746746746,-6.706706706706706,-6.666666666666666,-6.626626626626626,-6.586586586586586,-6.546546546546546,-6.506506506506506,-6.466466466466466,-6.426426426426426,-6.386386386386386,-6.346346346346346,-6.306306306306306,-6.266266266266266,-6.226226226226226,-6.186186186186186,-6.146146146146146,-6.106106106106106,-6.066066066066066,-6.026026026026026,-5.985985985985986,-5.945945945945946,-5.905905905905906,-5.865865865865866,-5.8258258258258255,-5.7857857857857855,-5.7457457457457455,-5.7057057057057055,-5.665665665665665,-5.625625625625625,-5.585585585585585,-5.545545545545545,-5.505505505505505,-5.465465465465465,-5.425425425425425,-5.385385385385385,-5.345345345345345,-5.305305305305305,-5.265265265265265,-5.225225225225225,-5.185185185185185,-5.145145145145145,-5.105105105105105,-5.065065065065065,-5.025025025025025,-4.984984984984985,-4.944944944944945,-4.904904904904905,-4.864864864864865,-4.824824824824825,-4.784784784784785,-4.744744744744745,-4.704704704704705,-4.664664664664665,-4.624624624624625,-4.584584584584585,-4.544544544544545,-4.504504504504505,-4.464464464464465,-4.424424424424425,-4.384384384384385,-4.344344344344345,-4.3043043043043046,-4.2642642642642645,-4.2242242242242245,-4.1841841841841845,-4.1441441441441444,-4.104104104104104,-4.064064064064064,-4.024024024024024,-3.9839839839839826,-3.9439439439439425,-3.9039039039039025,-3.8638638638638625,-3.8238238238238225,-3.7837837837837824,-3.7437437437437424,-3.7037037037037024,-3.6636636636636624,-3.6236236236236223,-3.5835835835835823,-3.5435435435435423,-3.5035035035035023,-3.4634634634634622,-3.423423423423422,-3.383383383383382,-3.343343343343342,-3.303303303303302,-3.263263263263262,-3.223223223223222,-3.183183183183182,-3.143143143143142,-3.103103103103102,-3.063063063063062,-3.023023023023022,-2.982982982982982,-2.942942942942942,-2.902902902902902,-2.862862862862862,-2.822822822822822,-2.782782782782782,-2.7427427427427418,-2.7027027027027017,-2.6626626626626617,-2.6226226226226217,-2.5825825825825817,-2.5425425425425416,-2.5025025025025016,-2.4624624624624616,-2.4224224224224216,-2.3823823823823815,-2.3423423423423415,-2.3023023023023015,-2.2622622622622615,-2.2222222222222214,-2.1821821821821814,-2.1421421421421414,-2.1021021021021014,-2.0620620620620613,-2.0220220220220213,-1.9819819819819813,-1.9419419419419413,-1.9019019019019012,-1.8618618618618612,-1.8218218218218212,-1.7817817817817811,-1.7417417417417411,-1.701701701701701,-1.661661661661661,-1.621621621621621,-1.581581581581581,-1.541541541541541,-1.501501501501501,-1.461461461461461,-1.421421421421421,-1.381381381381381,-1.3413413413413409,-1.3013013013013008,-1.2612612612612608,-1.2212212212212208,-1.1811811811811808,-1.1411411411411407,-1.1011011011011007,-1.0610610610610607,-1.0210210210210207,-0.9809809809809806,-0.9409409409409406,-0.9009009009009006,-0.8608608608608606,-0.8208208208208205,-0.7807807807807805,-0.7407407407407405,-0.7007007007007005,-0.6606606606606604,-0.6206206206206204,-0.5805805805805804,-0.5405405405405403,-0.5005005005005003,-0.4604604604604603,-0.42042042042042027,-0.38038038038038025,-0.3403403403403402,-0.3003003003003002,-0.26026026026026017,-0.22022022022022014,-0.18018018018018012,-0.1401401401401401,-0.10010010010010006,-0.06006006006006004,-0.020020020020020013,0.020020020020020013,0.06006006006006004,0.10010010010010006,0.1401401401401401,0.18018018018018012,0.22022022022022014,0.26026026026026017,0.3003003003003002,0.3403403403403402,0.38038038038038025,0.42042042042042027,0.4604604604604603,0.5005005005005003,0.5405405405405403,0.5805805805805804,0.6206206206206204,0.6606606606606604,0.7007007007007005,0.7407407407407405,0.7807807807807805,0.8208208208208205,0.8608608608608606,0.9009009009009006,0.9409409409409406,0.9809809809809806,1.0210210210210207,1.0610610610610607,1.1011011011011007,1.1411411411411407,1.1811811811811808,1.2212212212212208,1.2612612612612608,1.3013013013013008,1.3413413413413409,1.381381381381381,1.421421421421421,1.461461461461461,1.501501501501501,1.541541541541541,1.581581581581581,1.621621621621621,1.661661661661661,1.701701701701701,1.7417417417417411,1.7817817817817811,1.8218218218218212,1.8618618618618612,1.9019019019019012,1.9419419419419413,1.9819819819819813,2.0220220220220213,2.0620620620620613,2.1021021021021014,2.1421421421421414,2.1821821821821814,2.2222222222222214,2.2622622622622615,2.3023023023023015,2.3423423423423415,2.3823823823823815,2.4224224224224216,2.4624624624624616,2.5025025025025016,2.5425425425425416,2.5825825825825817,2.6226226226226217,2.6626626626626617,2.7027027027027017,2.7427427427427418,2.782782782782782,2.822822822822822,2.862862862862862,2.902902902902902,2.942942942942942,2.982982982982982,3.023023023023022,3.063063063063062,3.103103103103102,3.143143143143142,3.183183183183182,3.223223223223222,3.263263263263262,3.303303303303302,3.343343343343342,3.383383383383382,3.423423423423422,3.4634634634634622,3.5035035035035023,3.5435435435435423,3.5835835835835823,3.6236236236236223,3.6636636636636624,3.7037037037037024,3.7437437437437424,3.7837837837837824,3.8238238238238225,3.8638638638638625,3.9039039039039025,3.9439439439439425,3.9839839839839826,4.024024024024023,4.064064064064063,4.104104104104103,4.144144144144143,4.184184184184183,4.224224224224223,4.264264264264263,4.304304304304303,4.344344344344343,4.384384384384383,4.424424424424423,4.464464464464463,4.504504504504503,4.544544544544543,4.584584584584583,4.624624624624623,4.664664664664663,4.704704704704703,4.744744744744743,4.784784784784783,4.824824824824823,4.864864864864863,4.904904904904903,4.944944944944943,4.984984984984983,5.025025025025023,5.065065065065063,5.105105105105103,5.145145145145143,5.185185185185183,5.225225225225223,5.265265265265263,5.305305305305303,5.3453453453453434,5.3853853853853835,5.4254254254254235,5.4654654654654635,5.5055055055055035,5.545545545545544,5.585585585585584,5.625625625625624,5.665665665665667,5.705705705705707,5.745745745745747,5.785785785785787,5.825825825825827,5.865865865865867,5.905905905905907,5.945945945945947,5.985985985985987,6.026026026026027,6.0660660660660675,6.1061061061061075,6.1461461461461475,6.1861861861861875,6.226226226226228,6.266266266266268,6.306306306306308,6.346346346346348,6.386386386386388,6.426426426426428,6.466466466466468,6.506506506506508,6.546546546546548,6.586586586586588,6.626626626626628,6.666666666666668,6.706706706706708,6.746746746746748,6.786786786786788,6.826826826826828,6.866866866866868,6.906906906906908,6.946946946946948,6.986986986986988,7.027027027027028,7.067067067067068,7.107107107107108,7.147147147147148,7.187187187187188,7.227227227227228,7.267267267267268,7.307307307307308,7.347347347347348,7.387387387387388,7.427427427427428,7.467467467467468,7.507507507507508,7.547547547547548,7.5875875875875884,7.6276276276276285,7.6676676676676685,7.7077077077077085,7.7477477477477485,7.787787787787789,7.827827827827829,7.867867867867869,7.907907907907909,7.947947947947949,7.987987987987989,8.028028028028029,8.068068068068069,8.108108108108109,8.148148148148149,8.188188188188189,8.228228228228229,8.268268268268269,8.308308308308309,8.348348348348349,8.388388388388389,8.428428428428429,8.468468468468469,8.508508508508509,8.548548548548549,8.588588588588589,8.62862862862863,8.66866866866867,8.70870870870871,8.74874874874875,8.78878878878879,8.82882882882883,8.86886886886887,8.90890890890891,8.94894894894895,8.98898898898899,9.02902902902903,9.06906906906907,9.10910910910911,9.14914914914915,9.18918918918919,9.22922922922923,9.26926926926927,9.30930930930931,9.34934934934935,9.38938938938939,9.42942942942943,9.46946946946947,9.50950950950951,9.54954954954955,9.58958958958959,9.62962962962963,9.66966966966967,9.70970970970971,9.74974974974975,9.78978978978979,9.82982982982983,9.86986986986987,9.90990990990991,9.94994994994995,9.98998998998999,10.03003003003003,10.07007007007007,10.11011011011011,10.15015015015015,10.19019019019019,10.23023023023023,10.27027027027027,10.31031031031031,10.35035035035035,10.39039039039039,10.43043043043043,10.47047047047047,10.51051051051051,10.55055055055055,10.59059059059059,10.63063063063063,10.67067067067067,10.71071071071071,10.75075075075075,10.79079079079079,10.83083083083083,10.87087087087087,10.91091091091091,10.95095095095095,10.99099099099099,11.03103103103103,11.07107107107107,11.11111111111111,11.15115115115115,11.19119119119119,11.23123123123123,11.27127127127127,11.31131131131131,11.35135135135135,11.391391391391391,11.431431431431431,11.471471471471471,11.511511511511511,11.551551551551551,11.591591591591591,11.631631631631631,11.671671671671671,11.711711711711711,11.751751751751751,11.791791791791791,11.831831831831831,11.871871871871871,11.911911911911911,11.951951951951951,11.991991991991991,12.032032032032035,12.072072072072075,12.112112112112115,12.152152152152155,12.192192192192195,12.232232232232235,12.272272272272275,12.312312312312315,12.352352352352355,12.392392392392395,12.432432432432435,12.472472472472475,12.512512512512515,12.552552552552555,12.592592592592595,12.632632632632635,12.672672672672675,12.712712712712715,12.752752752752755,12.792792792792795,12.832832832832835,12.872872872872875,12.912912912912915,12.952952952952955,12.992992992992995,13.033033033033036,13.073073073073076,13.113113113113116,13.153153153153156,13.193193193193196,13.233233233233236,13.273273273273276,13.313313313313316,13.353353353353356,13.393393393393396,13.433433433433436,13.473473473473476,13.513513513513516,13.553553553553556,13.593593593593596,13.633633633633636,13.673673673673676,13.713713713713716,13.753753753753756,13.793793793793796,13.833833833833836,13.873873873873876,13.913913913913916,13.953953953953956,13.993993993993996,14.034034034034036,14.074074074074076,14.114114114114116,14.154154154154156,14.194194194194196,14.234234234234236,14.274274274274276,14.314314314314316,14.354354354354356,14.394394394394396,14.434434434434436,14.474474474474476,14.514514514514516,14.554554554554556,14.594594594594597,14.634634634634637,14.674674674674677,14.714714714714717,14.754754754754757,14.794794794794797,14.834834834834837,14.874874874874877,14.914914914914917,14.954954954954957,14.994994994994997,15.035035035035037,15.075075075075077,15.115115115115117,15.155155155155157,15.195195195195197,15.235235235235237,15.275275275275277,15.315315315315317,15.355355355355357,15.395395395395397,15.435435435435437,15.475475475475477,15.515515515515517,15.555555555555557,15.595595595595597,15.635635635635637,15.675675675675677,15.715715715715717,15.755755755755757,15.795795795795797,15.835835835835837,15.875875875875877,15.915915915915917,15.955955955955957,15.995995995995997,16.036036036036037,16.076076076076077,16.116116116116117,16.156156156156158,16.196196196196198,16.236236236236238,16.276276276276278,16.316316316316318,16.356356356356358,16.396396396396398,16.436436436436438,16.476476476476478,16.516516516516518,16.556556556556558,16.596596596596598,16.636636636636638,16.676676676676678,16.716716716716718,16.756756756756758,16.796796796796798,16.836836836836838,16.876876876876878,16.916916916916918,16.956956956956958,16.996996996996998,17.037037037037038,17.077077077077078,17.117117117117118,17.157157157157158,17.197197197197198,17.237237237237238,17.27727727727728,17.31731731731732,17.35735735735736,17.3973973973974,17.43743743743744,17.47747747747748,17.51751751751752,17.55755755755756,17.5975975975976,17.63763763763764,17.67767767767768,17.71771771771772,17.75775775775776,17.7977977977978,17.83783783783784,17.87787787787788,17.91791791791792,17.95795795795796,17.997997997998,18.03803803803804,18.07807807807808,18.11811811811812,18.15815815815816,18.1981981981982,18.23823823823824,18.27827827827828,18.31831831831832,18.35835835835836,18.3983983983984,18.43843843843844,18.47847847847848,18.51851851851852,18.55855855855856,18.5985985985986,18.63863863863864,18.67867867867868,18.71871871871872,18.75875875875876,18.7987987987988,18.83883883883884,18.87887887887888,18.91891891891892,18.95895895895896,18.998998998999,19.03903903903904,19.07907907907908,19.11911911911912,19.15915915915916,19.1991991991992,19.23923923923924,19.27927927927928,19.31931931931932,19.35935935935936,19.3993993993994,19.43943943943944,19.47947947947948,19.51951951951952,19.55955955955956,19.5995995995996,19.63963963963964,19.67967967967968,19.71971971971972,19.75975975975976,19.7997997997998,19.83983983983984,19.87987987987988,19.91991991991992,19.95995995995996,20],"y":[4.387241946958801e-8,4.674626267473745e-8,4.980154031836557e-8,5.30492473801269e-8,5.650101510782568e-8,6.01691459395116e-8,6.40666502265289e-8,6.820728484361389e-8,7.260559377583873e-8,7.727695077610896e-8,8.22376041908591e-8,8.750472405587734e-8,9.309645156837709e-8,9.903195104602923e-8,1.053314644882184e-7,1.1201636885963575e-7,1.1910923622134042e-7,1.2663389683943182e-7,1.3461550540703173e-7,1.4308061052057395e-7,1.5205722755727354e-7,1.6157491510649062e-7,1.7166485511376955e-7,1.8235993690278048e-7,1.9369484524673877e-7,2.057061526677752e-7,2.1843241614959563e-7,2.3191427845591694e-7,2.461945742547978e-7,2.6131844125623744e-7,2.7733343657876245e-7,2.942896585684191e-7,3.12239874302341e-7,3.3123965301739097e-7,3.513475057134466e-7,3.7262503119006614e-7,3.9513706878446943e-7,4.189518580887783e-7,4.4414120593412893e-7,4.707806609398382e-7,4.989496959358464e-7,5.287318985781899e-7,5.602151704875125e-7,5.934919352528113e-7,6.286593556538592e-7,6.658195604677742e-7,7.050798812377233e-7,7.465530993941931e-7,7.903577041322326e-7,8.36618161461212e-7,8.854651948574077e-7,9.370360779633068e-7,9.914749397918303e-7,0.0000010489330829083018,0.000001109569315077374,0.0000011735502948776968,0.0000012410508918022246,0.000001312254561377995,0.0000013873537358548585,0.0000014665502310300255,0.0000015500556697901873,0.0000016380919229719275,0.000001730891568156728,0.0000018286983670357506,0.0000019317677619961532,0.000002040367392600059,0.000002154777632644675,0.0000022752921485113114,0.000002402218479529526,0.0000025358786411026486,0.000002676609751359002,0.0000028247646821139823,0.0000029807127349470696,0.000003144840343219203,0.00000331755180087388,0.0000034992700188888188,0.0000036904373102623734,0.000003891516204442657,0.0000041029902921253984,0.000004325365101369344,0.000004559169005998538,0.000004804954167281041,0.000005063297509896246,0.000005334801733223251,0.000005620096359002866,0.00000591983881644885,0.000006234715565903882,0.000006565443262155529,0.000006912769958551599,0.00000727747635307104,0.000007660377077528074,0.000008062322031109888,0.000008484197759463184,0.000008926928880571287,0.000009391479558673253,0.000009878855027506765,0.000010390103164163104,0.00001092631611487023,0.00001148863197403079,0.000012078236517861838,0.000012696364993998008,0.000013344303968436589,0.000014023393231214271,0.000014735027762223829,0.000015480659758586695,0.000016261800725017523,0.0000170800236286158,0.00001793696511954362,0.00001883432781904418,0.000019773882676274332,0.0000207574713954237,0.000021787008934604114,0.000022864486077988366,0.000023991972082688322,0.000025171617401859933,0.000026405656485518696,0.00002769641066055113,0.00002904629109140447,0.000030457801822917822,0.000031933542906777776,0.000033476213613030404,0.00003508861572811531,0.00003677365694082925,0.000038534354317649873,0.000040373837868792476,0.00004229535420638976,0.00004430227029612471,0.000046398077303645614,0.00004858639453704514,0.00005087097348667377,0.000053255701963495975,0.000055744608337180326,0.0000583418658750624,0.00006105179718308016,0.00006387887874972526,0.0000668277455940253,0.00006990319601846841,0.00007311019646778893,0.00007645388649441003,0.00007993958383131726,0.00008357278957302617,0.00008735919346528836,0.00009130467930403765,0.00009541533044405163,0.00009969743541767759,0.00010415749366391342,0.00010880222136800079,0.00011363855741164694,0.00011867366943380134,0.00012391496000192125,0.00012937007289340208,0.00013504689948688558,0.0001409535852628871,0.00014709853641320787,0.0001534904265583022,0.0001601382035717797,0.00016705109651097855,0.00017423862265244909,0.0001817105946310173,0.00018947712768094794,0.00019754864697754288,0.0002059358950773763,0.00021464993945516733,0.00022370218013511667,0.00023310435741435854,0.00024286855967597994,0.0002530072312889137,0.00026353318059167685,0.0002744595879569221,0.000285800013933366,0.0002975684074615845,0.00030977911415987994,0.00032244688467622247,0.0003355868831019481,0.0003492146954429124,0.00036334633814319266,0.0003779982666565771,0.00039318738406047247,0.000408931049706924,0.0004252470879049512,0.0004421537966282549,0.0004596699562421037,0.0004778148382428708,0.0004966082140034847,0.0005160703635176654,0.0005362220841358502,0.0005570846992849016,0.0005786800671640127,0.0006010305894085166,0.0006241592197131035,0.0006480894724058573,0.0006728454309639683,0.0006984517564619021,0.0007249336959422798,0.0007523170906998113,0.0007806283844678302,0.0008098946314971694,0.0008401435045164382,0.0008714033025627959,0.000903702958671763,0.0009370720474144898,0.0009715407922705504,0.0010071400728240666,0.0010439014317706486,0.00108185708172252,0.0011210399117986207,0.0011614834939865078,0.0012032220892625382,0.001246290653456345,0.0012907248428459,0.0013365610194683805,0.0013838362561326921,0.001432588341118659,0.0014828557825481902,0.0015346778124125668,0.0015880943902414548,0.0016431462063969627,0.0016998746849779054,0.0017583219863174478,0.0018185310090590163,0.0018805453917932276,0.0019444095142401663,0.002010168497960206,0.0020778682065766547,0.0021475552454938026,0.0022192769610934965,0.0022930814393930174,0.0023690175041483517,0.002447134714384598,0.0025274833613377137,0.002610114464790066,0.0026950797687834206,0.002782431736691607,0.002872223545637581,0.0029645090802365576,0.0030593429256497917,0.003156780359932286,0.00325687734565769,0.0033596905208052395,0.003465277188891871,0.003573695308334274,0.0036850034810257497,0.0037992609401116704,0.003916527536950666,0.004036863727244198,0.004160330556323745,0.004286989643578009,0.004416903166010494,0.004550133840911486,0.004686744907634235,0.004826800108461009,0.004970363668550303,0.005117500274951568,0.005268275054678834,0.005422753551832626,0.005581001703761544,0.005743085816254432,0.005909072537756086,0.006079028832598319,0.006253021953241712,0.006431119411519707,0.006613388948884168,0.006799898505644531,0.006990716189200411,0.007185910241263063,0.0073855490040669524,0.007589700885568303,0.007798434323633837,0.008011817749219443,0.008229919548542233,0.008452808024248895,0.008680551355587013,0.008913217557581782,0.009150874439228868,0.00939358956070772,0.00964143018962897,0.009894463256320601,0.010152755308170764,0.010416372463034009,0.010685380361720401,0.010959844119578208,0.011239828277190354,0.011525396750200105,0.01181661277828566,0.012113538873304722,0.01241623676663025,0.01272476735570116,0.013039190649810291,0.013359565715158742,0.01368595061920079,0.014018402374307633,0.01435697688078515,0.014701728869267752,0.015052711842527685,0.01540997801673332,0.015773578262187865,0.016143562043586197,0.01651997735983184,0.01690287068344791,0.017292286899622283,0.0176882692449366,0.018090859245811334,0.018500096656717868,0.018916019398202782,0.019338663494769457,0.019768063012662828,0.02020424999761291,0.020647254412580814,0.02109710407555921,0.02155382459748752,0.022017439320321932,0.0224879692553265,0.022965433021635787,0.023449846785143235,0.023941224197777455,0.02443957633722237,0.024944911647138204,0.025457235877942085,0.02597655202821958,0.026502860286807903,0.027036157975633378,0.027576439493350837,0.02812369625985245,0.028677916661712193,0.029239085998622426,0.029807186430892068,0.03038219692806438,0.0309640932187286,0.03155284774157564,0.0321484295977743,0.032750804504723535,0.03335993475125203,0.03397577915431994,0.03459829301729821,0.03522742808987689,0.035863132529673894,0.03650535086559764,0.03715402396304173,0.037809088990948994,0.03847047939082964,0.039138124847773535,0.03981195126353013,0.04049188073170486,0.04117783151513008,0.041869718025472805,0.042567450805122614,0.04327093651142367,0.04398007790329609,0.04469477383030055,0.04541491922419394,0.046140405093019865,0.04687111851779019,0.04760694265178582,0.04834775672253865,0.04909343603651116,0.04984385198653602,0.05059887206203531,0.0513583598620605,0.05212217511118272,0.05289017367826527,0.05366220759814575,0.05443812509625041,0.05521777061616056,0.05600098485016804,0.05678760477280808,0.05757746367741662,0.05837039121569679,0.0591662134403246,0.05996475285059169,0.06076582844108031,0.061569255753398236,0.06237484693093851,0.06318241077668688,0.06399175281405382,0.064802675350726,0.06561497754552237,0.06642845547823949,0.06724290222246689,0.0680581079213434,0.06887385986623765,0.069689942578322,0.07050613789299787,0.07132222504715287,0.0721379807691985,0.07295317937185392,0.07376759284763015,0.07458099096696451,0.07539314137896101,0.07620380971467053,0.0770127596928708,0.07781975322827511,0.07862455054210803,0.07942691027498666,0.08022658960203778,0.08102334435017858,0.08181692911748352,0.08260709739457162,0.08339360168791995,0.08417619364503964,0.0849546241814038,0.08572864360906775,0.08649800176688183,0.08726244815218945,0.08802173205394628,0.08877560268713532,0.08952380932840512,0.09026610145280366,0.09100222887153374,0.0917319418706077,0.09245499135029467,0.09317112896527133,0.09388010726534511,0.09458167983664978,0.09527560144321055,0.09596162816874872,0.09663951755862284,0.09730902876179089,0.09796992267267698,0.0986219620728209,0.09926491177220881,0.09989853875014673,0.10052261229557627,0.10113690414670716,0.1017411886298556,0.10233524279735134,0.10291884656442458,0.10349178284493543,0.10405383768582789,0.10460480040020496,0.10514446369889996,0.10567262382042901,0.10618908065921392,0.1066936378919666,0.10718610310210493,0.10766628790211871,0.10813400805374894,0.10858908358589073,0.1090313389101029,0.10946060293363737,0.10987670916985785,0.1102794958459771,0.11066880600800572,0.1110444876227972,0.11140639367713388,0.11175438227373026,0.11208831672407667,0.11240806563804916,0.11271350301018991,0.1130045083025767,0.11328096652421576,0.11354276830688745,0.1137898099773497,0.11402199362586644,0.1142392271709722,0.1144414244204221,0.11462850512826962,0.11480039504803019,0.11495702598184937,0.11509833582567144,0.11522426861033791,0.11533477453858634,0.11542981001791047,0.11550933768926964,0.11557332645158976,0.11562175148206366,0.1156545942522161,0.11567184253971632,0.11567349043594272,0.11565953834927369,0.11562999300412334,0.1155848674357022,0.11552418098052462,0.11544795926266392,0.1153562341757757,0.11524904386089704,0.115126432680054,0.11498845118569455,0.11483515608598394,0.11466661020598433,0.11448288244477205,0.1142840477285183,0.11407018695958603,0.11384138696169323,0.11359774042118388,0.11333934582447527,0.11306630739173457,0.11277873500684489,0.11247674414372655,0.1121604557890984,0.11182999636171835,0.11148549762821586,0.11112709661557316,0.11075493552033389,0.11036916161464026,0.10996992714916859,0.10955738925306509,0.10913170983096235,0.10869305545718848,0.108241597267239,0.10777751084663861,0.10730097611727732,0.10681217722132269,0.10631130240282442,0.10579854388710747,0.10527409775806276,0.10473816383345134,0.10419094553832765,0.10363264977669401,0.10306348680149746,0.102483670083093,0.10189341617627415,0.10129294458599143,0.1006824776318848,0.10006224031172854,0.09943246016392286,0.09879336712913228,0.09814519341120648,0.09748817333748433,0.09682254321860403,0.09614854120794174,0.09546640716078322,0.09477638249335167,0.09407871004180426,0.093373633921318,0.09266139938535664,0.09194225268526188,0.09121644093025373,0.0904842119479622,0.08974581414559264,0.08900149637183984,0.08825150777964401,0.08749609768990424,0.08673551545624308,0.08597001033092107,0.0851998313320099,0.08442522711190505,0.0836464458272857,0.08286373501060006,0.08207734144317913,0.08128751103005112,0.08049448867655666,0.07969851816683149,0.07889984204424547,0.07809870149387359,0.07729533622706898,0.07648998436821279,0.07568288234370832,0.07487426477328486,0.07406436436367349,0.07325341180471817,0.07244163566797027,0.07162926230783608,0.07081651576530917,0.07000361767435151,0.06919078717096142,0.06837824080496595,0.06756619245458918,0.06675485324381947,0.06594443146261406,0.06513513248997356,0.0643271587199043,0.0635207094903021,0.0627159810147729,0.061913166317406336,0.06111245517051866,0.06031403403538088,0.05951808600593217,0.058724790755491,0.057934324486465245,0.0571468598830589,0.056362566066975546,0.055581608556112966,0.05480414922623663,0.05403034627562741,0.053260354192684746,0.05249432372647005,0.051732401860173576,0.05097473178748506,0.05022145289183591,0.04947270072849873,0.04872860700950917,0.04798929959137793,0.047254902465567954,0.0465255357516944,0.0458013156934147,0.045082354656970136,0.04436876113233711,0.04366063973694623,0.04295809122192499,0.04226121248082061,0.04157009656075156,0.04088483267594283,0.04020550622359384,0.03953219880202494,0.03886498823105458,0.03820394857454857,0.03754915016508885,0.036900659630704644,0.0362585399236105,0.035622850350890134,0.03499364660707194,0.034370980808531085,0.033754901529663345,0.03314545384076672,0.03254267934757042,0.03194661623235054,0.03135729929656901,0.03077476000497394,0.0301990265311003,0.02963012380410669,0.029068073556885496,0.02851289437538497,0.02796460174908099,0.027423208122533068,0.026888722947966185,0.02636115273881328,0.025840501124159226,0.025326768904023925,0.02481995410542241,0.02432005203914336,0.023827055357187033,0.0233409541107994,0.022861735809048985,0.022389385477884726,0.021923885719619337,0.021465216772782413,0.02101335657228589,0.020568280809849127,0.020129962994629255,0.019698374514003743,0.01927348469445349,0.018855260862496483,0.018443668405621835,0.018038670833174716,0.017640229837146842,0.017248305352824435,0.01686285561924861,0.016483837239445468,0.016111205240382127,0.01574491313260679,0.015384912969533749,0.01503115540633352,0.014683589758390249,0.014342164059290098,0.01400682511830554,0.013677518577341879,0.013354188967311786,0.013036779763909156,0.01272523344275006,0.012419491533852947,0.01211949467543082,0.011825182666969036,0.011536494521564236,0.011253368517499866,0.01097574224903815,0.010703552676405085,0.010436736174950783,0.010175228583465723,0.009918965251635888,0.009667881086621028,0.009421910598741434,0.00918098794625942,0.008945046979242949,0.008714021282501422,0.008487844217581995,0.008266448963818665,0.00804976855842628,0.007837735935632075,0.007630283964839986,0.007427345487822768,0.007228853354938115,0.007034740460366953,0.006844939776371543,0.006659384386573511,0.006478007518251334,0.006300742573659384,0.006127523160369761,0.005958283120640295,0.005792956559812559,0.005631477873744041,0.005473781775280088,0.005319803319771599,0.005169477929645116,0.005022741418032902,0.004879530011470999,0.004739780371674068,0.004603429616396341,0.004470415339388393,0.004340675629460559,0.004214149088663692,0.004090774849598736,0.003970492591867174,0.0038532425576747417,0.003738965566600998,0.003627603029548211,0.003519096961882815,0.0034133899957836176,0.0033104253918106942,0.0032101470497096665,0.003112499518465899,0.003017428005623825,0.0029248783858865,0.0028347972090107328,0.0027471317070134945,0.002661829800705268,0.002578840105566189,0.0024981119369809996,0.0024195953148487617,0.0023432409675835956,0.002269000335522519,0.002196825573756623,0.0021266695544018,0.0020584858683252625,0.001992228826343961,0.0019278534599112137,0.0018653155213074638,0.001804571483351321,0.0017455785386468246,0.0016882945983827042,0.0016326782906994762,0.0015786889586399226,0.001526286657698451,0.001475432152984691,0.0014260869160164717,0.001378213121157196,0.001331773641712419,0.0012867320457003117,0.0012430525913103838,0.0012007002220647256,0.0011596405616958057,0.001119839908754568,0.0010812652309624333,0.001043884159320524,0.0010076649819891754,0.0009725766379506123,0.000938588710467366,0.0009056714203487654,0.000873795619037568,0.0008429327815285685,0.0008130549991307215,0.0007841349720840381,0.0007561460020422959,0.0007290619844322781,0.0007028574007000028,0.0006775073104541351,0.0006529873435164969,0.0006292736918892774,0.0006063431016483567,0.0005841728647717708,0.000562740810912156,0.000542025299121691,0.0005220052095378091,0.0005026599350376571,0.00048396937286902044,0.00046591391626516794,0.0004484744460508055,0.0004316323222460491,0.0004153693756750945,0.0003996678995859678,0.0003845106412875245,0.0003698807938095902,0.0003557619875918985,0.0003421382822072138,0.0003289941581238385,0.0003163145085124141,0.0003040846311017181,0.0002922902200879304,0.0002809173581016204,0.00026995250823647066,0.0002593825061435669,0.00024919455219483516,0.0002393762037190387,0.00022991536731352654,0.00022080029123472362,0.00021201955787017535,0.00020356207629476384,0.00019541707491353526,0.00018757409419339113,0.00018002297948573328,0.00017275387394198447,0.00016575721152373442,0.0001590237101091099,0.0001525443646968183,0.0001463104407091498,0.000140313467395101,0.0001345452313346269,0.00012899777004489582,0.0001236633656893056,0.0001185345388898766,0.00011360404264352966,0.00010886485634263177,0.0001043101799000925,0.00009993342797917294,0.00009572822432807809,0.00009168839621929982,0.00008780796899358524,0.00008408116070831204,0.00008050237688997368,0.00007706620539038493,0.00007376741134615409,0.00007060093224087778,0.00006756187306946056,0.00006464550160388065,0.00006184724375967184,0.00005916267906232184,0.000056587536212741776,0.00005411768875089781,0.000051749150816655045,0.0000494780730068367,0.00004730073832745131,0.000045213558240008936,0.000043213068800807744,0.00004129592689203403,0.00003945890654349136,0.000037698895343747375,0.000036012890939451216,0.00003439799762156196,0.00003285142299720005,0.00003137047474581535,0.000029952557458349715,0.0000285951695580575,0.000027295900301633028,0.000026052426859283736,0.000024862511472380745,0.000023723998687308703,0.0000226348126641329,0.000021592954558699234,0.000020596499976777977,0.00001964359649886338,0.00001873246127424385,0.000017861378682955013,0.000017028698064237118,0.000016232831510119897,0.00001547225172276472,0.000014745489934201946,0.000014051133887109243,0.000013387825875285964,0.000012754260842488486,0.000012149184538304388,0.000011571391729753004,0.000011019724467314007,0.000010493070404099864,0.000009990361166900122,0.000009510570777842322,0.000009052714125428778,0.000008615845483724591,0.000008199057078488256,0.000007801477699054218,0.000007422271354792523,0.000007060635974989362,0.000006715802151010152,0.000006387031919624355,0.000006073617586390522,0.0000057748805880184635,0.0000054901703926438794,0.000005218863436970679,0.000004960362099254656,0.0000047140937071218716,0.000004479509579233897,0.000004256084099832428,0.000004043313825213873,0.000003840716621204933,0.0000036478308307290847,0.0000034642144705731,0.0000032894444564823392,0.0000031231158557323254,0.0000029648411663431665,0.000002814249622122772,0.0000026709865227428267,0.000002534712588070767,0.0000024051033359991956,0.000002281848483032283,0.0000021646513669071077,0.000002053228390545681,0.000001947308486650809,0.0000018466326022767402,0.0000017509532027226895,0.00000166003379411411,0.000001573648464053496,0.0000014915814397390743,0.0000014136266629656987,0.0000013395873814384443,0.0000012692757558450696,0.000001202512482148859,0.0000011391264285785624,0.0000010789542868071283,0.000001021840236825271,9.676356250305516e-7,9.161986550664371e-7,8.67394090959701e-7,8.210929721179049e-7,7.771723397620001e-7,7.355149743818823e-7,6.960091438153878e-7,6.585483615636476e-7,6.230311549676035e-7,5.893608428824316e-7,5.574453224980486e-7,5.271968649650788e-7,4.985319194966724e-7,4.7137092562723884e-7,4.4563813331952355e-7,4.212614306216737e-7,3.981721785857942e-7,3.7630505316914116e-7,3.5559789384843766e-7,3.359915586870184e-7,3.174297856032471e-7,2.998590595974197e-7,2.832284857026829e-7,2.6748966743365274e-7,2.5259659051439807e-7,2.3850551167509495e-7,2.2517485231416364e-7,2.1256509682992992e-7,2.0063869543293075e-7,1.8935997125675898e-7,1.786950315920076e-7,1.6861168307426728e-7,1.590793506633581e-7,1.500690002570051e-7,1.415530647880051e-7,1.3350537365956752e-7,1.2590108537901318e-7,1.1871662325528247e-7,1.1192961403083167e-7,1.0551882932346743e-7,9.946412975843171e-8,9.374641167570478e-8,8.834755630195328e-8,8.325038128087716e-8,7.843859445988505e-8,7.389674983506716e-8,6.961020556030788e-8,6.556508393015808e-8,6.174823324969953e-8,5.8147191508131236e-8,5.475015177617984e-8,5.154592925069404e-8,4.8523929872900307e-8,4.567412044983394e-8,4.2987000211357835e-8,4.045357373798496e-8,3.806532519742179e-8,3.5814193830344444e-8,3.369255062841807e-8,3.1693176149979863e-8,2.980923942111774e-8,2.8034277872100325e-8,2.6362178261257954e-8,2.4787158540471696e-8,2.3303750618401878e-8,2.1906783979489378e-8,2.0591370118591995e-8,1.9352887752865285e-8,1.8186968774186227e-8,1.7089484907032345e-8,1.6056535038280986e-8,1.5084433186881498e-8,1.4169697082784191e-8,1.3309037325876448e-8,1.2499347096993451e-8,1.173769239432979e-8,1.1021302769785434e-8,1.0347562540937859e-8,9.714002455441368e-9,9.118291785715707e-9,8.558230832805174e-9,8.031743819263635e-9,7.536872151852532e-9,7.0717680357336524e-9,6.634688422691735e-9,6.223989276740219e-9,5.838120141245542e-9,5.475618992452981e-9,5.135107365013039e-9,4.815285735790714e-9,4.5149291528936755e-9,4.2328830974801445e-9,3.96805956650421e-9,3.7194333651258384e-9,3.4860385980580703e-9,3.2669653496434948e-9,3.0613565429482253e-9,2.8684049686359843e-9,2.687350474836586e-9,2.5174773096549673e-9,2.3581116083779048e-9,2.2086190178291054e-9,2.0684024506966275e-9,1.936899963014286e-9,1.8135827483188515e-9,1.697953242328663e-9,1.5895433322989159e-9,1.4879126655028901e-9,1.3926470515689853e-9,1.3033569536703805e-9,1.219676063818723e-9,1.1412599577549852e-9,1.0677848251614476e-9]}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Sampled Data and Fitted Exponnorm Distribution"},"xaxis":{"title":{"text":"X-axis"}},"yaxis":{"title":{"text":"Probability Density"}}}}},"metadata":{},"output_type":"display_data"}],"source":["import numpy as np\n","import plotly.graph_objs as go\n","from scipy.stats import exponnorm\n","from scipy.optimize import curve_fit\n","\n","# Define a function for the PDF of the fitted distribution\n","def exponnorm_pdf(x, loc, scale, K):\n","    return exponnorm.pdf(x, K, loc=loc, scale=scale)\n","\n","# Create a range of x values for the plot\n","x = np.linspace(-20, 20, 1000)\n","\n","# Create a scatter trace for the PDF curve of the fitted distribution\n","pdf_trace = go.Scatter(x=x, y=exponnorm_pdf(x, loc = loc_hat, scale = scale_hat, K = rate_hat), mode='lines', name='Fitted Exponnorm PDF', line=dict(color='red'))\n","\n","# Create the layout for the plot\n","layout = go.Layout(title='Sampled Data and Fitted Exponnorm Distribution',\n","                   xaxis=dict(title='X-axis'),\n","                   yaxis=dict(title='Probability Density'))\n","\n","# Create a figure and add the traces to it\n","fig = go.Figure(data=[pdf_trace], layout=layout)\n","\n","# Display the plot in Jupyter Notebook or as an HTML file\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOdfJoNF73jOSn/+XfcVm3w","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
