{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyMoEle8f713xJX7xEEhdht5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IqfRg7-Koroh","executionInfo":{"status":"ok","timestamp":1695730985622,"user_tz":-120,"elapsed":29818,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}},"outputId":"8db3d5a7-c19e-4486-d9a3-242400f3cc93"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["os.chdir(\"/content/gdrive/MyDrive/NFL_Challenge/NFL-GPT/NFL data\")\n","os.listdir()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FnNq39sVo0GQ","executionInfo":{"status":"ok","timestamp":1695730987211,"user_tz":-120,"elapsed":1593,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}},"outputId":"d43afe14-1237-4750-eb0a-d2c41e414573"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['.DS_Store',\n"," 'Contact Detection',\n"," 'Punt Prediction',\n"," 'Analytics',\n"," 'Impact Detection',\n"," 'data bowl 2021',\n"," 'data bowl 2023',\n"," 'data bowl 2022',\n"," 'data bowl 2020',\n"," 'asonty',\n"," 'Highlights_NGS_2019',\n"," 'Highlights_NGS_Prime',\n"," 'final_df.parquet',\n"," 'tokens.json',\n"," 'mapped_df.parquet',\n"," 'train_test_split.csv',\n"," 'class_weights.parquet',\n"," 'checkpoint',\n"," 'models (1)',\n"," 'models',\n"," 'test_tokens_NFL_GPT',\n"," 'train_tokens_NFL_GPT',\n"," 'training_history.csv']"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["class CustomSparseCategoricalAccuracy(tf.keras.metrics.Metric):\n","    def __init__(self, name='custom_sparse_categorical_accuracy', **kwargs):\n","        super(CustomSparseCategoricalAccuracy, self).__init__(name=name, **kwargs)\n","        self.total = self.add_weight(name='total', initializer='zeros')\n","        self.count = self.add_weight(name='count', initializer='zeros')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        mask = tf.not_equal(y_true, -100)  # Create a mask for valid labels\n","        valid_labels = tf.boolean_mask(y_true, mask)\n","        valid_labels = tf.cast(valid_labels, dtype=tf.int32)\n","\n","        preds = tf.argmax(y_pred, axis = -1)\n","        valid_preds = tf.boolean_mask(preds, mask)\n","        valid_preds = tf.cast(valid_preds, dtype=tf.int32)\n","\n","        correct = tf.equal(valid_labels, valid_preds)\n","\n","        accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n","\n","        self.total.assign_add(accuracy)\n","        self.count.assign_add(1.0)\n","\n","    def result(self):\n","        return self.total / self.count if self.count > 0 else 0.0\n","\n","    def reset_state(self):\n","        self.total.assign(0.0)\n","        self.count.assign(0.0)\n","\n","class CustomSparseCategoricalCrossentropy(tf.keras.losses.Loss):\n","    def __init__(self, from_logits=False, class_weights=None):\n","        super(CustomSparseCategoricalCrossentropy, self).__init__()\n","        self.from_logits = from_logits\n","        self.class_weights = class_weights\n","\n","    def call(self, y_true, y_pred):\n","        mask = tf.not_equal(y_true, -100)  # Create a mask for valid labels\n","        valid_labels = tf.boolean_mask(y_true, mask)\n","        valid_logits = tf.boolean_mask(y_pred, mask)\n","\n","        # Apply class weights if provided\n","        if self.class_weights is not None:\n","            # Create a tensor of weights using tf.gather\n","            weights = tf.gather(tf.constant(list(self.class_weights.values()), dtype=tf.float32), tf.cast(valid_labels, tf.int32))\n","            weighted_loss = tf.keras.losses.sparse_categorical_crossentropy(valid_labels, valid_logits, from_logits=self.from_logits)\n","            weighted_loss = weighted_loss * weights\n","            loss = tf.reduce_mean(weighted_loss)\n","        else:\n","            loss = tf.keras.losses.sparse_categorical_crossentropy(valid_labels, valid_logits, from_logits=self.from_logits)\n","\n","        return loss"],"metadata":{"id":"FqG2RMzDo8R4","executionInfo":{"status":"ok","timestamp":1695730987211,"user_tz":-120,"elapsed":5,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class PlayTypeEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(PlayTypeEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"PlayType\"])\n","    return embed\n","\n","class OffDefEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(OffDefEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"OffDef\"])\n","    return embed\n","\n","class TypeEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(TypeEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"token_type_ids\"])\n","    return embed\n","\n","class SidefEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(SidefEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"side_ids\"])\n","    return embed\n","\n","class PositionalEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(PositionalEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"pos_ids\"])\n","    return embed\n","\n","class InputEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(InputEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"input_ids\"])\n","    return embed\n","\n","class Embedding(tf.keras.Model):\n","  def __init__(self,\n","               input_vocab_size : int,\n","               positional_vocab_size : int,\n","               offdef_vocab_size : int,\n","               side_vocab_size : int,\n","               type_vocab_size : int,\n","               playtype_vocab_size : int,\n","               embedding_dim : int):\n","        super(Embedding, self).__init__()\n","\n","        self.InputEmbedding = InputEncoder(vocab_size=input_vocab_size,\n","                                           embedding_dim=embedding_dim)\n","        self.PositionalEmbedding = PositionalEncoder(vocab_size=positional_vocab_size,\n","                                                     embedding_dim=embedding_dim)\n","        self.OffDefEmbedding = OffDefEncoder(vocab_size=offdef_vocab_size,\n","                                             embedding_dim=embedding_dim)\n","        self.TypeEmbedding = OffDefEncoder(vocab_size=type_vocab_size,\n","                                             embedding_dim=embedding_dim)\n","        self.SideEmbedding = SidefEncoder(vocab_size=side_vocab_size,\n","                                             embedding_dim=embedding_dim)\n","        self.PlayTypeEmbedding = PlayTypeEncoder(vocab_size=playtype_vocab_size,\n","                                                 embedding_dim=embedding_dim)\n","        self.Add = tf.keras.layers.Add()\n","\n","        self.Dense = tf.keras.layers.Dense(embedding_dim)\n","\n","  def call(self, x):\n","    input_embed = self.InputEmbedding(x)\n","    positional_embed = self.PositionalEmbedding(x)\n","    side_embed = self.SideEmbedding(x)\n","    type_embed = self.TypeEmbedding(x)\n","    offdef_embed = self.OffDefEmbedding(x)\n","    playtype_embed = self.PlayTypeEmbedding(x)\n","\n","    embed = self.Add([input_embed, side_embed, type_embed, positional_embed, offdef_embed, playtype_embed])\n","    embed = self.Dense(embed)\n","\n","    return embed\n","\n","from typing import List, Optional, Union\n","\n","def shape_list(tensor: Union[tf.Tensor, np.ndarray]) -> List[int]:\n","    \"\"\"\n","    Deal with dynamic shape in tensorflow cleanly.\n","\n","    Args:\n","        tensor (`tf.Tensor` or `np.ndarray`): The tensor we want the shape of.\n","\n","    Returns:\n","        `List[int]`: The shape of the tensor as a list.\n","    \"\"\"\n","    if isinstance(tensor, np.ndarray):\n","        return list(tensor.shape)\n","\n","    dynamic = tf.shape(tensor)\n","\n","    if tensor.shape == tf.TensorShape(None):\n","        return dynamic\n","\n","    static = tensor.shape.as_list()\n","\n","    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n","\n","class AttentionBlock(tf.keras.Model):\n","  def __init__(self,\n","               num_heads : int,\n","               hidden_dim : int,\n","               output_dim : int):\n","        super(AttentionBlock, self).__init__()\n","\n","        self.num_attention_heads = num_heads\n","        self.attention_head_size = hidden_dim\n","        self.total_dim = num_heads * hidden_dim\n","        self.output_dim = output_dim\n","\n","        self.Query = tf.keras.layers.Dense(self.total_dim, name = \"Query\")\n","        self.Key = tf.keras.layers.Dense(self.total_dim, name = \"Key\")\n","        self.Value = tf.keras.layers.Dense(self.total_dim, name = \"Value\")\n","\n","\n","        self.Dense = tf.keras.layers.Dense(output_dim, name = \"Dense\", activation = \"relu\")\n","        self.Add = tf.keras.layers.Add(name = \"Add\")\n","        self.Drop = tf.keras.layers.Dropout(rate = 0.1)\n","        self.Norm = tf.keras.layers.BatchNormalization(name = \"Norm\")\n","\n","  def transpose_for_scores(self, tensor: tf.Tensor, batch_size: int) -> tf.Tensor:\n","        # Reshape from [batch_size, seq_length, all_head_size] to [batch_size, seq_length, num_attention_heads, attention_head_size]\n","        tensor = tf.reshape(tensor=tensor, shape=(batch_size, -1, self.num_attention_heads, self.attention_head_size))\n","\n","        # Transpose the tensor from [batch_size, seq_length, num_attention_heads, attention_head_size] to [batch_size, num_attention_heads, seq_length, attention_head_size]\n","        return tf.transpose(tensor, perm=[0, 2, 1, 3])\n","\n","  def create_causal_masks(self, temp_ids):\n","      # Use broadcasting to create the 2D comparison tensor\n","      causal_mask = temp_ids[:, :, tf.newaxis] >= temp_ids[:, tf.newaxis, :]\n","      causal_mask = (tf.cast(causal_mask, dtype=tf.float32) - 1) * 10000\n","      reshaped_tensor = tf.expand_dims(causal_mask, axis=1)\n","      duplicated_tensor = tf.tile(reshaped_tensor, multiples=[1, 3, 1, 1])\n","      return duplicated_tensor\n","\n","  def create_attention_mask(self, attn_mask):\n","    attn_mask = (tf.cast(attn_mask, dtype=tf.float32) -1) * 10000\n","    reshaped_tensor = tf.expand_dims(attn_mask, axis=1)\n","    reshaped_tensor = tf.expand_dims(reshaped_tensor, axis=1)\n","    duplicated_tensor = tf.tile(reshaped_tensor, multiples=[1, 3, 1, 1])\n","    return duplicated_tensor\n","\n","  def compute_scaled_attn_scores(self, query, key):\n","    attention_scores = tf.matmul(query, key, transpose_b=True)  # Transpose the second sequence\n","\n","    # If you want scaled dot-product attention, divide by the square root of the embedding dimension\n","    embedding_dim = query.shape[-1]\n","    scaled_attention_scores = attention_scores / tf.math.sqrt(tf.cast(embedding_dim, dtype=tf.float32))\n","\n","    return scaled_attention_scores\n","\n","  def compute_attention_weigths(self, query, key, temp_ids, masks):\n","\n","    attn_masks = self.create_attention_mask(masks)\n","    causal_masks = self.create_causal_masks(temp_ids)\n","    scaled_attn_scores = self.compute_scaled_attn_scores(query, key)\n","\n","    attn_scores = scaled_attn_scores - attn_masks - causal_masks\n","    return tf.nn.softmax(attn_scores, axis = -1)\n","\n","  def get_preds_and_attention(self,\n","           embeddings,\n","           temporal_ids,\n","           attention_masks):\n","\n","    query = self.Query(embeddings)\n","    key = self.Key(embeddings)\n","    value = self.Value(embeddings)\n","\n","    attention_weights = self.compute_attention_weigths(query, key, temporal_ids, attention_masks)\n","\n","    attention_scores = tf.matmul(attention_weights, value)\n","    attention_scores = self.Dense(attention_scores)\n","\n","    output = self.Add([attention_scores, embeddings])\n","    output = self.Drop(output)\n","    output = self.Norm(output)\n","    return output, attention_weights\n","\n","  def call(self,\n","           hidden_states : tf.Tensor,\n","           temporal_ids,\n","           attention_masks):\n","\n","    batch_size = shape_list(hidden_states)[0]\n","\n","    query = self.Query(hidden_states)\n","    queries = self.transpose_for_scores(query, batch_size)\n","\n","    key = self.Key(hidden_states)\n","    keys = self.transpose_for_scores(key, batch_size)\n","\n","    value = self.Value(hidden_states)\n","    values = self.transpose_for_scores(value, batch_size)\n","\n","    attention_weights = self.compute_attention_weigths(queries, keys, temporal_ids, attention_masks)\n","\n","    attention_scores = tf.matmul(attention_weights, values)\n","    attention_scores = tf.transpose(attention_scores, perm=[0, 2, 1, 3])\n","    attention_scores = tf.reshape(tensor=attention_scores, shape=(batch_size, -1, self.total_dim))\n","\n","    attention_scores = self.Dense(attention_scores)\n","\n","    output = self.Add([attention_scores, hidden_states])\n","    output = self.Drop(output)\n","    output = self.Norm(output)\n","    return output\n","\n","class NFLgpt(tf.keras.Model):\n","  def __init__(self,\n","               input_vocab_size : int,\n","               to_pred_size : int,\n","               positional_vocab_size : int,\n","               side_vocab_size : int,\n","               offdef_vocab_size : int,\n","               type_vocab_size : int,\n","               playtype_vocab_size : int,\n","               embedding_dim : int,\n","               hidden_dim : int):\n","        super(NFLgpt, self).__init__()\n","\n","        self.Embedding = Embedding(input_vocab_size = input_vocab_size,\n","                                   positional_vocab_size = positional_vocab_size,\n","                                   side_vocab_size = side_vocab_size,\n","                                   type_vocab_size = type_vocab_size,\n","                                   offdef_vocab_size = offdef_vocab_size,\n","                                   playtype_vocab_size = playtype_vocab_size,\n","                                   embedding_dim = embedding_dim)\n","\n","        self.Attention1 = AttentionBlock(num_heads = 3,\n","                                         hidden_dim = hidden_dim,\n","                                         output_dim = embedding_dim)\n","\n","        self.Attention2 = AttentionBlock(num_heads = 3,\n","                                         hidden_dim = hidden_dim,\n","                                         output_dim = embedding_dim)\n","\n","        self.Attention3 = AttentionBlock(num_heads = 3,\n","                                         hidden_dim = hidden_dim,\n","                                         output_dim = embedding_dim)\n","\n","\n","        self.DenseHead = tf.keras.layers.Dense(512, activation = \"gelu\")\n","\n","        self.NPPHead = tf.keras.layers.Dense(to_pred_size, activation = \"softmax\")\n","\n","  def call(self,\n","           x):\n","\n","    embed = self.Embedding(x)\n","    h1 = self.Attention1(embed, x[\"pos_ids\"], x[\"attention_mask\"])\n","    h2 = self.Attention2(h1, x[\"pos_ids\"], x[\"attention_mask\"])\n","    h3 = self.Attention3(h2, x[\"pos_ids\"], x[\"attention_mask\"])\n","\n","    logits = self.DenseHead(h3)\n","    pred = self.NPPHead(logits)\n","\n","    return pred"],"metadata":{"id":"mJ11F8G1s4B4","executionInfo":{"status":"ok","timestamp":1695730987211,"user_tz":-120,"elapsed":3,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["moves_id = 11164\n","starts_id = 1983\n","scrimmage_id = 99\n","positions_id = 28\n","temp_ids = 50\n","\n","model = NFLgpt(input_vocab_size = 13276,\n","               to_pred_size = moves_id+1,\n","               positional_vocab_size = temp_ids+2,\n","               offdef_vocab_size = 2,\n","               side_vocab_size = 2,\n","               type_vocab_size = 4,\n","               playtype_vocab_size = 9,\n","               embedding_dim = 256,\n","               hidden_dim = 256)\n","\n","model.load_weights(\"models/NFLGPT\")"],"metadata":{"id":"vnldbjeYs27O","executionInfo":{"status":"ok","timestamp":1695730992211,"user_tz":-120,"elapsed":5002,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"23298064-4058-422d-b5a4-64c6455eb72d"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x78fa80e9b2e0>"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["testing_data = tf.data.Dataset.load(\"test_tokens_NFL_GPT\")\n","test_length = [i for i,_ in enumerate(testing_data)][-1] + 1\n","\n","batch_size = 32\n","\n","testing_data = testing_data.shuffle(test_length).batch(batch_size)"],"metadata":{"id":"KZLTNxkYtdW_","executionInfo":{"status":"ok","timestamp":1695731018087,"user_tz":-120,"elapsed":25897,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class_weights = pd.read_parquet(\"class_weights.parquet\")\n","step_range = [(0, 10), (10, 100), (100, 1000), (1000, 10000), (10000, 50000), (50000, 100000), (100000, 300000), (300000, 500000), (500000, 1000000), (1000000, 10000000)]\n","\n","def insert_weights(df, w):\n","  df[\"weights\"] = [w for i in range(df.shape[0])]\n","  return df\n","\n","\n","from collections import Counter\n","weights = dict(Counter(class_weights[\"Zone_ID\"].to_numpy()))\n","weights_df = pd.DataFrame(np.array([[k, v] for k,v in weights.items()]), columns = [\"Class\", \"Count\"])\n","\n","weights_dict = {i : weights_df[(weights_df['Count'] > step_range[i][0]) & (weights_df['Count'] <= step_range[i][1])].reset_index(drop = True) for i in range(len(step_range))}\n","w_dict = {0 : 1,\n","          1 : 0.9,\n","          2 : 0.8,\n","          3 : 0.7,\n","          4 : 0.6,\n","          5 : 0.5,\n","          6 : 0.4,\n","          7 : 0.3,\n","          8 : 0.2,\n","          9 : 0.05,}\n","\n","weights_dict = {k:insert_weights(v, w_dict[k]) for k,v in weights_dict.items()}\n","\n","weights_df = pd.concat(list(weights_dict.values())).reset_index(drop = True)\n","\n","weights_inv = {v[0] : v[2] for v in weights_df.values}"],"metadata":{"id":"hu6cRYdVbew5","executionInfo":{"status":"ok","timestamp":1695731049520,"user_tz":-120,"elapsed":9657,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["loss = CustomSparseCategoricalCrossentropy(from_logits=False, class_weights=weights_inv)\n","acc = CustomSparseCategoricalAccuracy()"],"metadata":{"id":"Cbr9PzdD9dke","executionInfo":{"status":"ok","timestamp":1695731049521,"user_tz":-120,"elapsed":4,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"t1OJOiAjfK7F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["i = 0\n","\n","preds = []\n","trues = []\n","\n","for x, y in testing_data:\n","  if i < 5:\n","    print(type(x))\n","    print(type(y))\n","    print(y.shape)\n","    print(\" \")\n","\n","    batch_predictions = model.predict(x)  # Set training=False for inference\n","    preds.append(batch_predictions)\n","    trues.append(y)\n","    print(i)\n","    print(loss(y, batch_predictions))\n","    acc.update_state(y, batch_predictions)\n","    print(acc.result())\n","    print(\" \")\n","    print(\" \")\n","\n","    i+=1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Z8BEWaWcKj8","executionInfo":{"status":"ok","timestamp":1695731203420,"user_tz":-120,"elapsed":82195,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}},"outputId":"686eb539-8059-4781-ba0a-51571c41c45a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'dict'>\n","<class 'tensorflow.python.framework.ops.EagerTensor'>\n","(32, 256)\n"," \n","1/1 [==============================] - 30s 30s/step\n","0\n","tf.Tensor(1.2605363, shape=(), dtype=float32)\n","tf.Tensor(0.5343066, shape=(), dtype=float32)\n"," \n"," \n","<class 'dict'>\n","<class 'tensorflow.python.framework.ops.EagerTensor'>\n","(32, 256)\n"," \n","1/1 [==============================] - 0s 35ms/step\n","1\n","tf.Tensor(1.2072968, shape=(), dtype=float32)\n","tf.Tensor(0.54498875, shape=(), dtype=float32)\n"," \n"," \n","<class 'dict'>\n","<class 'tensorflow.python.framework.ops.EagerTensor'>\n","(32, 256)\n"," \n","1/1 [==============================] - 0s 44ms/step\n","2\n","tf.Tensor(1.4188111, shape=(), dtype=float32)\n","tf.Tensor(0.51909417, shape=(), dtype=float32)\n"," \n"," \n","<class 'dict'>\n","<class 'tensorflow.python.framework.ops.EagerTensor'>\n","(32, 256)\n"," \n","1/1 [==============================] - 0s 24ms/step\n","3\n","tf.Tensor(1.1242297, shape=(), dtype=float32)\n","tf.Tensor(0.53489095, shape=(), dtype=float32)\n"," \n"," \n","<class 'dict'>\n","<class 'tensorflow.python.framework.ops.EagerTensor'>\n","(32, 256)\n"," \n","1/1 [==============================] - 0s 26ms/step\n","4\n","tf.Tensor(1.1538084, shape=(), dtype=float32)\n","tf.Tensor(0.5479027, shape=(), dtype=float32)\n"," \n"," \n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VuLSJnF8dp_0"},"execution_count":null,"outputs":[]}]}