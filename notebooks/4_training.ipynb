{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29900,"status":"ok","timestamp":1695995584769,"user":{"displayName":"sam Chain","userId":"00779398991030525753"},"user_tz":-120},"id":"gohwn7N_b3TF","outputId":"6e4ca052-8912-4274-9bca-04f9aac17774"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1683,"status":"ok","timestamp":1695995586446,"user":{"displayName":"sam Chain","userId":"00779398991030525753"},"user_tz":-120},"id":"0dZk4pGEcWJn","outputId":"a6b56920-82ea-4089-b1c0-4953befa27c8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['.DS_Store',\n"," 'Contact Detection',\n"," 'Punt Prediction',\n"," 'Analytics',\n"," 'Impact Detection',\n"," 'data bowl 2021',\n"," 'data bowl 2023',\n"," 'data bowl 2022',\n"," 'data bowl 2020',\n"," 'asonty',\n"," 'Highlights_NGS_2019',\n"," 'Highlights_NGS_Prime',\n"," 'final_df.parquet',\n"," 'tokens.json',\n"," 'mapped_df.parquet',\n"," 'train_test_split.csv',\n"," 'class_weights.parquet',\n"," 'train_play_prediction_categ',\n"," 'test_play_prediction_categ',\n"," 'train_play_prediction_binary',\n"," 'test_play_prediction_binary',\n"," 'models',\n"," 'index',\n"," 'training_history_model_large.csv',\n"," 'training_history_large_model.csv',\n"," 'test_tokens_NFL_GPT',\n"," 'train_tokens_NFL_GPT',\n"," 'training_history_model_small.csv',\n"," 'training_history_model_medium.csv']"]},"metadata":{},"execution_count":2}],"source":["os.chdir(\"/content/gdrive/MyDrive/NFL_Challenge/NFL-GPT/NFL data\")\n","os.listdir()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ZC8PlPFzcaLL","executionInfo":{"status":"ok","timestamp":1695995593661,"user_tz":-120,"elapsed":7220,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"outputs":[],"source":["training_data = tf.data.Dataset.load(\"train_tokens_NFL_GPT\")\n","testing_data = tf.data.Dataset.load(\"test_tokens_NFL_GPT\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"qHqmx-rHkAqZ","executionInfo":{"status":"ok","timestamp":1695995680691,"user_tz":-120,"elapsed":87032,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"outputs":[],"source":["train_length = [i for i,_ in enumerate(training_data)][-1] + 1\n","test_length = [i for i,_ in enumerate(testing_data)][-1] + 1"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1695995680691,"user":{"displayName":"sam Chain","userId":"00779398991030525753"},"user_tz":-120},"id":"G74adOrokP8G","outputId":"ab38ab90-bd56-466f-b00e-86ed77ae847b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train length is :  205851\n","Test length is :  51463\n"]}],"source":["print(\"Train length is : \", str(train_length))\n","print(\"Test length is : \", str(test_length))"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"TisoHo24c-2_","executionInfo":{"status":"ok","timestamp":1695995680691,"user_tz":-120,"elapsed":16,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"outputs":[],"source":["batch_size = 32\n","\n","training_data = training_data.shuffle(train_length).batch(batch_size)\n","testing_data = testing_data.shuffle(test_length).batch(batch_size)"]},{"cell_type":"markdown","metadata":{"id":"wxvvriqbc2f-"},"source":["## Model classes"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"V70IUNz8cvSd","executionInfo":{"status":"ok","timestamp":1695995748225,"user_tz":-120,"elapsed":602,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"outputs":[],"source":["class PlayTypeEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(PlayTypeEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"PlayType\"])\n","    return embed\n","\n","class PositionEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(PositionEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"position_ids\"])\n","    return embed\n","\n","class ScrimmageEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(ScrimmageEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"scrim_ids\"])\n","    return embed\n","\n","class StartEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(StartEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"start_ids\"])\n","    return embed\n","\n","class OffDefEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(OffDefEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"OffDef\"])\n","    return embed\n","\n","class TypeEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(TypeEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"token_type_ids\"])\n","    return embed\n","\n","class PositionalEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(PositionalEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"pos_ids\"])\n","    return embed\n","\n","class InputEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(InputEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"input_ids\"])\n","    return embed\n","\n","class Embedding(tf.keras.Model):\n","  def __init__(self,\n","               input_vocab_size : int,\n","               positional_vocab_size : int,\n","               position_vocab_size : int,\n","               scrimmage_vocab_size : int,\n","               start_vocab_size: int,\n","               offdef_vocab_size : int,\n","               type_vocab_size : int,\n","               playtype_vocab_size : int,\n","               embedding_dim : int):\n","        super(Embedding, self).__init__()\n","\n","        self.InputEmbedding = InputEncoder(vocab_size=input_vocab_size,\n","                                           embedding_dim=embedding_dim)\n","        self.PositionalEmbedding = PositionalEncoder(vocab_size=positional_vocab_size,\n","                                                     embedding_dim=embedding_dim)\n","        self.PositionEmbedding = PositionEncoder(vocab_size=position_vocab_size,\n","                                                     embedding_dim=embedding_dim)\n","        self.ScrimEmbedding = ScrimmageEncoder(vocab_size=scrimmage_vocab_size,\n","                                                     embedding_dim=embedding_dim)\n","        self.StartEmbedding = StartEncoder(vocab_size=start_vocab_size,\n","                                                     embedding_dim=embedding_dim)\n","        self.OffDefEmbedding = OffDefEncoder(vocab_size=offdef_vocab_size,\n","                                             embedding_dim=embedding_dim)\n","        self.TypeEmbedding = TypeEncoder(vocab_size=type_vocab_size,\n","                                             embedding_dim=embedding_dim)\n","        self.PlayTypeEmbedding = PlayTypeEncoder(vocab_size=playtype_vocab_size,\n","                                                 embedding_dim=embedding_dim)\n","        self.Add = tf.keras.layers.Add()\n","\n","        self.Dense = tf.keras.layers.Dense(embedding_dim)\n","\n","  def call(self, x):\n","    input_embed = self.InputEmbedding(x)\n","    positional_embed = self.PositionalEmbedding(x)\n","    position_embed = self.PositionEmbedding(x)\n","    scrim_embed = self.ScrimEmbedding(x)\n","    start_embed = self.StartEmbedding(x)\n","    type_embed = self.TypeEmbedding(x)\n","    offdef_embed = self.OffDefEmbedding(x)\n","    playtype_embed = self.PlayTypeEmbedding(x)\n","\n","    embed = self.Add([input_embed,\n","                      positional_embed,\n","                      position_embed,\n","                      scrim_embed,\n","                      start_embed,\n","                      type_embed,\n","                      offdef_embed,\n","                      playtype_embed])\n","\n","    embed = self.Dense(embed)\n","\n","    return embed"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"rubzBCstBDkL","executionInfo":{"status":"ok","timestamp":1695995748831,"user_tz":-120,"elapsed":5,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"outputs":[],"source":["from typing import List, Optional, Union\n","\n","def shape_list(tensor: Union[tf.Tensor, np.ndarray]) -> List[int]:\n","    \"\"\"\n","    Deal with dynamic shape in tensorflow cleanly.\n","\n","    Args:\n","        tensor (`tf.Tensor` or `np.ndarray`): The tensor we want the shape of.\n","\n","    Returns:\n","        `List[int]`: The shape of the tensor as a list.\n","    \"\"\"\n","    if isinstance(tensor, np.ndarray):\n","        return list(tensor.shape)\n","\n","    dynamic = tf.shape(tensor)\n","\n","    if tensor.shape == tf.TensorShape(None):\n","        return dynamic\n","\n","    static = tensor.shape.as_list()\n","\n","    return [dynamic[i] if s is None else s for i, s in enumerate(static)]"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"r7ns4O3E3i19","executionInfo":{"status":"ok","timestamp":1695995748831,"user_tz":-120,"elapsed":4,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"outputs":[],"source":["class AttentionBlock(tf.keras.Model):\n","  def __init__(self,\n","               num_heads : int,\n","               hidden_dim : int,\n","               output_dim : int):\n","        super(AttentionBlock, self).__init__()\n","\n","        self.num_attention_heads = num_heads\n","        self.attention_head_size = hidden_dim\n","        self.total_dim = num_heads * hidden_dim\n","        self.output_dim = output_dim\n","\n","        self.Query = tf.keras.layers.Dense(self.total_dim, name = \"Query\")\n","        self.Key = tf.keras.layers.Dense(self.total_dim, name = \"Key\")\n","        self.Value = tf.keras.layers.Dense(self.total_dim, name = \"Value\")\n","\n","\n","        self.Dense = tf.keras.layers.Dense(output_dim, name = \"Dense\", activation = \"relu\")\n","        self.Add = tf.keras.layers.Add(name = \"Add\")\n","        self.Drop = tf.keras.layers.Dropout(rate = 0.1)\n","        self.Norm = tf.keras.layers.BatchNormalization(name = \"Norm\")\n","\n","  def transpose_for_scores(self, tensor: tf.Tensor, batch_size: int) -> tf.Tensor:\n","        # Reshape from [batch_size, seq_length, all_head_size] to [batch_size, seq_length, num_attention_heads, attention_head_size]\n","        tensor = tf.reshape(tensor=tensor, shape=(batch_size, -1, self.num_attention_heads, self.attention_head_size))\n","\n","        # Transpose the tensor from [batch_size, seq_length, num_attention_heads, attention_head_size] to [batch_size, num_attention_heads, seq_length, attention_head_size]\n","        return tf.transpose(tensor, perm=[0, 2, 1, 3])\n","\n","  def create_causal_masks(self, temp_ids):\n","      # Use broadcasting to create the 2D comparison tensor\n","      causal_mask = temp_ids[:, :, tf.newaxis] >= temp_ids[:, tf.newaxis, :]\n","      causal_mask = (tf.cast(causal_mask, dtype=tf.float32) - 1) * 10000\n","      reshaped_tensor = tf.expand_dims(causal_mask, axis=1)\n","      duplicated_tensor = tf.tile(reshaped_tensor, multiples=[1, 3, 1, 1])\n","      return duplicated_tensor\n","\n","  def create_attention_mask(self, attn_mask):\n","    attn_mask = (tf.cast(attn_mask, dtype=tf.float32) -1) * 10000\n","    reshaped_tensor = tf.expand_dims(attn_mask, axis=1)\n","    reshaped_tensor = tf.expand_dims(reshaped_tensor, axis=1)\n","    duplicated_tensor = tf.tile(reshaped_tensor, multiples=[1, 3, 1, 1])\n","    return duplicated_tensor\n","\n","  def compute_scaled_attn_scores(self, query, key):\n","    attention_scores = tf.matmul(query, key, transpose_b=True)  # Transpose the second sequence\n","\n","    # If you want scaled dot-product attention, divide by the square root of the embedding dimension\n","    embedding_dim = query.shape[-1]\n","    scaled_attention_scores = attention_scores / tf.math.sqrt(tf.cast(embedding_dim, dtype=tf.float32))\n","\n","    return scaled_attention_scores\n","\n","  def compute_attention_weigths(self, query, key, temp_ids, masks):\n","\n","    attn_masks = self.create_attention_mask(masks)\n","    causal_masks = self.create_causal_masks(temp_ids)\n","    scaled_attn_scores = self.compute_scaled_attn_scores(query, key)\n","\n","    attn_scores = scaled_attn_scores - attn_masks - causal_masks\n","    return tf.nn.softmax(attn_scores, axis = -1)\n","\n","  def get_preds_and_attention(self,\n","           embeddings,\n","           temporal_ids,\n","           attention_masks):\n","\n","    query = self.Query(embeddings)\n","    key = self.Key(embeddings)\n","    value = self.Value(embeddings)\n","\n","    attention_weights = self.compute_attention_weigths(query, key, temporal_ids, attention_masks)\n","\n","    attention_scores = tf.matmul(attention_weights, value)\n","    attention_scores = self.Dense(attention_scores)\n","\n","    output = self.Add([attention_scores, embeddings])\n","    output = self.Drop(output)\n","    output = self.Norm(output)\n","    return output, attention_weights\n","\n","  def call(self,\n","           hidden_states : tf.Tensor,\n","           temporal_ids,\n","           attention_masks):\n","\n","    batch_size = shape_list(hidden_states)[0]\n","\n","    query = self.Query(hidden_states)\n","    queries = self.transpose_for_scores(query, batch_size)\n","\n","    key = self.Key(hidden_states)\n","    keys = self.transpose_for_scores(key, batch_size)\n","\n","    value = self.Value(hidden_states)\n","    values = self.transpose_for_scores(value, batch_size)\n","\n","    attention_weights = self.compute_attention_weigths(queries, keys, temporal_ids, attention_masks)\n","\n","    attention_scores = tf.matmul(attention_weights, values)\n","    attention_scores = tf.transpose(attention_scores, perm=[0, 2, 1, 3])\n","    attention_scores = tf.reshape(tensor=attention_scores, shape=(batch_size, -1, self.total_dim))\n","\n","    attention_scores = self.Dense(attention_scores)\n","\n","    output = self.Add([attention_scores, hidden_states])\n","    output = self.Drop(output)\n","    output = self.Norm(output)\n","    return output\n"]},{"cell_type":"code","source":["class Encoder(tf.keras.Model):\n","  def __init__(self,\n","               input_vocab_size : int,\n","               positional_vocab_size : int,\n","               position_vocab_size : int,\n","               scrimmage_vocab_size : int,\n","               start_vocab_size: int,\n","               offdef_vocab_size : int,\n","               type_vocab_size : int,\n","               playtype_vocab_size : int,\n","               embedding_dim : int,\n","               hidden_dim : int):\n","        super(Encoder, self).__init__()\n","\n","        self.Embedding = Embedding(input_vocab_size = input_vocab_size,\n","                                   positional_vocab_size = positional_vocab_size,\n","                                   position_vocab_size = position_vocab_size,\n","                                   scrimmage_vocab_size = scrimmage_vocab_size,\n","                                   start_vocab_size = start_vocab_size,\n","                                   type_vocab_size = type_vocab_size,\n","                                   offdef_vocab_size = offdef_vocab_size,\n","                                   playtype_vocab_size = playtype_vocab_size,\n","                                   embedding_dim = embedding_dim)\n","\n","        self.Attention1 = AttentionBlock(num_heads = 3,\n","                                         hidden_dim = hidden_dim,\n","                                         output_dim = embedding_dim)\n","\n","        self.DenseHead = tf.keras.layers.Dense(embedding_dim, activation = \"relu\")\n","\n","  def call(self,\n","           x):\n","\n","    embed = self.Embedding(x)\n","    h1 = self.Attention1(embed, x[\"pos_ids\"], x[\"attention_mask\"])\n","\n","    encoded = self.DenseHead(h1)\n","\n","    return encoded\n","\n","class EncoderL(tf.keras.Model):\n","  def __init__(self,\n","               input_vocab_size : int,\n","               positional_vocab_size : int,\n","               position_vocab_size : int,\n","               scrimmage_vocab_size : int,\n","               start_vocab_size: int,\n","               offdef_vocab_size : int,\n","               type_vocab_size : int,\n","               playtype_vocab_size : int,\n","               embedding_dim : int,\n","               hidden_dim : int):\n","        super(EncoderL, self).__init__()\n","\n","        self.Embedding = Embedding(input_vocab_size = input_vocab_size,\n","                                   positional_vocab_size = positional_vocab_size,\n","                                   position_vocab_size = position_vocab_size,\n","                                   scrimmage_vocab_size = scrimmage_vocab_size,\n","                                   start_vocab_size = start_vocab_size,\n","                                   type_vocab_size = type_vocab_size,\n","                                   offdef_vocab_size = offdef_vocab_size,\n","                                   playtype_vocab_size = playtype_vocab_size,\n","                                   embedding_dim = embedding_dim)\n","\n","        self.Attention1 = AttentionBlock(num_heads = 3,\n","                                         hidden_dim = hidden_dim,\n","                                         output_dim = embedding_dim)\n","        self.Attention2 = AttentionBlock(num_heads = 3,\n","                                         hidden_dim = hidden_dim,\n","                                         output_dim = embedding_dim)\n","\n","        self.DenseHead = tf.keras.layers.Dense(embedding_dim, activation = \"relu\")\n","\n","  def call(self,\n","           x):\n","\n","    embed = self.Embedding(x)\n","    h1 = self.Attention1(embed, x[\"pos_ids\"], x[\"attention_mask\"])\n","    h2 = self.Attention2(h1, x[\"pos_ids\"], x[\"attention_mask\"])\n","\n","    encoded = self.DenseHead(h2)\n","\n","    return encoded\n","\n","class EncoderXL(tf.keras.Model):\n","  def __init__(self,\n","               input_vocab_size : int,\n","               positional_vocab_size : int,\n","               position_vocab_size : int,\n","               scrimmage_vocab_size : int,\n","               start_vocab_size: int,\n","               offdef_vocab_size : int,\n","               type_vocab_size : int,\n","               playtype_vocab_size : int,\n","               embedding_dim : int,\n","               hidden_dim : int):\n","        super(EncoderXL, self).__init__()\n","\n","        self.Embedding = Embedding(input_vocab_size = input_vocab_size,\n","                                   positional_vocab_size = positional_vocab_size,\n","                                   position_vocab_size = position_vocab_size,\n","                                   scrimmage_vocab_size = scrimmage_vocab_size,\n","                                   start_vocab_size = start_vocab_size,\n","                                   type_vocab_size = type_vocab_size,\n","                                   offdef_vocab_size = offdef_vocab_size,\n","                                   playtype_vocab_size = playtype_vocab_size,\n","                                   embedding_dim = embedding_dim)\n","\n","        self.Attention1 = AttentionBlock(num_heads = 3,\n","                                         hidden_dim = hidden_dim,\n","                                         output_dim = embedding_dim)\n","        self.Attention2 = AttentionBlock(num_heads = 3,\n","                                         hidden_dim = hidden_dim,\n","                                         output_dim = embedding_dim)\n","        self.Attention3 = AttentionBlock(num_heads = 3,\n","                                         hidden_dim = hidden_dim,\n","                                         output_dim = embedding_dim)\n","\n","        self.DenseHead = tf.keras.layers.Dense(embedding_dim, activation = \"relu\")\n","\n","  def call(self,\n","           x):\n","\n","    embed = self.Embedding(x)\n","    h1 = self.Attention1(embed, x[\"pos_ids\"], x[\"attention_mask\"])\n","    h2 = self.Attention2(h1, x[\"pos_ids\"], x[\"attention_mask\"])\n","    h3 = self.Attention3(h2, x[\"pos_ids\"], x[\"attention_mask\"])\n","\n","    encoded = self.DenseHead(h3)\n","\n","    return encoded"],"metadata":{"id":"np5ocyLs-Vpi","executionInfo":{"status":"ok","timestamp":1695999722677,"user_tz":-120,"elapsed":400,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["class QBGPT(tf.keras.Model):\n","  def __init__(self,\n","               input_vocab_size : int,\n","               positional_vocab_size : int,\n","               position_vocab_size : int,\n","               scrimmage_vocab_size : int,\n","               start_vocab_size: int,\n","               offdef_vocab_size : int,\n","               type_vocab_size : int,\n","               playtype_vocab_size : int,\n","               embedding_dim : int,\n","               hidden_dim : int,\n","               to_pred_size : int):\n","        super(QBGPT, self).__init__()\n","\n","        self.Encoder = Encoder(input_vocab_size = input_vocab_size,\n","                               positional_vocab_size = positional_vocab_size,\n","                               position_vocab_size = position_vocab_size,\n","                               scrimmage_vocab_size = scrimmage_vocab_size,\n","                               start_vocab_size = start_vocab_size,\n","                               type_vocab_size = type_vocab_size,\n","                               offdef_vocab_size = offdef_vocab_size,\n","                               playtype_vocab_size = playtype_vocab_size,\n","                               embedding_dim = embedding_dim,\n","                               hidden_dim = hidden_dim)\n","\n","        self.Logits = tf.keras.layers.Dense(to_pred_size)\n","\n","  def call(self, x):\n","\n","    encoded = self.Encoder(x)\n","    logits = self.Logits(encoded)\n","\n","    return logits\n","\n","class LargeQBGPT(tf.keras.Model):\n","  def __init__(self,\n","               input_vocab_size : int,\n","               positional_vocab_size : int,\n","               position_vocab_size : int,\n","               scrimmage_vocab_size : int,\n","               start_vocab_size: int,\n","               offdef_vocab_size : int,\n","               type_vocab_size : int,\n","               playtype_vocab_size : int,\n","               embedding_dim : int,\n","               hidden_dim : int,\n","               to_pred_size : int):\n","        super(LargeQBGPT, self).__init__()\n","\n","        self.Encoder = EncoderL(input_vocab_size = input_vocab_size,\n","                               positional_vocab_size = positional_vocab_size,\n","                               position_vocab_size = position_vocab_size,\n","                               scrimmage_vocab_size = scrimmage_vocab_size,\n","                               start_vocab_size = start_vocab_size,\n","                               type_vocab_size = type_vocab_size,\n","                               offdef_vocab_size = offdef_vocab_size,\n","                               playtype_vocab_size = playtype_vocab_size,\n","                               embedding_dim = embedding_dim,\n","                               hidden_dim = hidden_dim)\n","\n","        self.Logits = tf.keras.layers.Dense(to_pred_size)\n","\n","  def call(self, x):\n","\n","    encoded = self.Encoder(x)\n","    logits = self.Logits(encoded)\n","\n","    return logits\n","\n","class XLargeQBGPT(tf.keras.Model):\n","  def __init__(self,\n","               input_vocab_size : int,\n","               positional_vocab_size : int,\n","               position_vocab_size : int,\n","               scrimmage_vocab_size : int,\n","               start_vocab_size: int,\n","               offdef_vocab_size : int,\n","               type_vocab_size : int,\n","               playtype_vocab_size : int,\n","               embedding_dim : int,\n","               hidden_dim : int,\n","               to_pred_size : int):\n","        super(XLargeQBGPT, self).__init__()\n","\n","        self.Encoder = EncoderXL(input_vocab_size = input_vocab_size,\n","                               positional_vocab_size = positional_vocab_size,\n","                               position_vocab_size = position_vocab_size,\n","                               scrimmage_vocab_size = scrimmage_vocab_size,\n","                               start_vocab_size = start_vocab_size,\n","                               type_vocab_size = type_vocab_size,\n","                               offdef_vocab_size = offdef_vocab_size,\n","                               playtype_vocab_size = playtype_vocab_size,\n","                               embedding_dim = embedding_dim,\n","                               hidden_dim = hidden_dim)\n","\n","        self.Logits = tf.keras.layers.Dense(to_pred_size)\n","\n","  def call(self, x):\n","\n","    encoded = self.Encoder(x)\n","    logits = self.Logits(encoded)\n","\n","    return logits"],"metadata":{"id":"s6P_d8WKZI-y","executionInfo":{"status":"ok","timestamp":1695999731846,"user_tz":-120,"elapsed":476,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","execution_count":25,"metadata":{"id":"n3zGOP60Wtxj","executionInfo":{"status":"ok","timestamp":1695999735526,"user_tz":-120,"elapsed":404,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"outputs":[],"source":["class CustomSparseCategoricalCrossentropy(tf.keras.losses.Loss):\n","    def __init__(self, from_logits=False, class_weights=None):\n","        super(CustomSparseCategoricalCrossentropy, self).__init__()\n","        self.from_logits = from_logits\n","        self.class_weights = class_weights\n","\n","    def call(self, y_true, y_pred):\n","        mask = tf.not_equal(y_true, -100)  # Create a mask for valid labels\n","\n","        if self.from_logits == True:\n","          valid_preds = tf.nn.softmax(y_pred)\n","        else:\n","          valid_preds = y_pred\n","\n","        valid_labels = tf.boolean_mask(y_true, mask)\n","        valid_logits = tf.boolean_mask(valid_preds, mask)\n","\n","        # Apply class weights if provided\n","        if self.class_weights is not None:\n","            # Create a tensor of weights using tf.gather\n","            weights = tf.gather(tf.constant(list(self.class_weights.values()), dtype=tf.float32), tf.cast(valid_labels, tf.int32))\n","            weighted_loss = tf.keras.losses.sparse_categorical_crossentropy(valid_labels, valid_logits)\n","            weighted_loss = weighted_loss * weights\n","            loss = tf.reduce_mean(weighted_loss)\n","        else:\n","            loss = tf.keras.losses.sparse_categorical_crossentropy(valid_labels, valid_logits)\n","\n","        return loss\n","\n","class CustomSparseCategoricalAccuracy(tf.keras.metrics.Metric):\n","    def __init__(self, name='custom_sparse_categorical_accuracy', **kwargs):\n","        super(CustomSparseCategoricalAccuracy, self).__init__(name=name, **kwargs)\n","        self.total = self.add_weight(name='total', initializer='zeros')\n","        self.count = self.add_weight(name='count', initializer='zeros')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        mask = tf.not_equal(y_true, -100)  # Create a mask for valid labels\n","        valid_labels = tf.boolean_mask(y_true, mask)\n","\n","        preds = tf.nn.softmax(y_pred)\n","        preds = tf.argmax(preds, axis = -1)\n","        valid_preds = tf.boolean_mask(preds, mask)\n","\n","        correct = tf.equal(valid_labels, valid_preds)\n","\n","        accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n","\n","        self.total.assign_add(accuracy)\n","        self.count.assign_add(1.0)\n","\n","    def result(self):\n","        return self.total / self.count if self.count > 0 else 0.0\n","\n","    def reset_state(self):\n","        self.total.assign(0.0)\n","        self.count.assign(0.0)\n","\n","class CustomTopKAccuracy(tf.keras.metrics.Metric):\n","    def __init__(self, k=3, name='custom_top_k_accuracy', **kwargs):\n","        super(CustomTopKAccuracy, self).__init__(name=name, **kwargs)\n","        self.k = k\n","        self.total = self.add_weight(name='total', initializer='zeros')\n","        self.count = self.add_weight(name='count', initializer='zeros')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        mask = tf.not_equal(y_true, -100)  # Create a mask for valid labels\n","        valid_labels = tf.boolean_mask(y_true, mask)\n","\n","        # Get top-k predicted classes\n","        preds = tf.nn.softmax(y_pred)\n","        top_k_values, top_k_indices = tf.nn.top_k(preds, k=self.k)\n","        valid_preds = tf.boolean_mask(top_k_indices, mask)\n","\n","        # Broadcast valid_labels to match the shape of valid_preds\n","        valid_labels_broadcasted = tf.tile(tf.expand_dims(valid_labels, axis=-1), [1, self.k])\n","\n","        valid_labels_broadcasted = tf.cast(valid_labels_broadcasted, dtype=tf.int32)\n","        valid_preds = tf.cast(valid_preds, dtype=tf.int32)\n","\n","        correct = tf.reduce_sum(tf.cast(tf.equal(valid_labels_broadcasted, valid_preds), dtype=tf.float32))\n","\n","        accuracy = correct / tf.cast(tf.shape(valid_labels_broadcasted)[0], dtype=tf.float32)\n","\n","        self.total.assign_add(accuracy)\n","        self.count.assign_add(1.0)\n","\n","    def result(self):\n","        return self.total / self.count if self.count > 0 else 0.0\n","\n","    def reset_state(self):\n","        self.total.assign(0.0)\n","        self.count.assign(0.0)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"JLxwB6r9263v","executionInfo":{"status":"ok","timestamp":1695999737909,"user_tz":-120,"elapsed":452,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"outputs":[],"source":["class_weights = pd.read_parquet(\"class_weights.parquet\")"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"jLz24yR6bmPo","executionInfo":{"status":"ok","timestamp":1695999737909,"user_tz":-120,"elapsed":2,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"outputs":[],"source":["step_range = [(0, 10), (10, 100), (100, 1000), (1000, 10000), (10000, 50000), (50000, 100000), (100000, 300000), (300000, 500000), (500000, 1000000), (1000000, 10000000)]"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"ILWE5R-WcFOr","executionInfo":{"status":"ok","timestamp":1695999738654,"user_tz":-120,"elapsed":2,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"outputs":[],"source":["def insert_weights(df, w):\n","  df[\"weights\"] = [w for i in range(df.shape[0])]\n","  return df"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"qYgHoVIw-tHN","executionInfo":{"status":"ok","timestamp":1695999744283,"user_tz":-120,"elapsed":415,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"outputs":[],"source":["from collections import Counter\n","weights = dict(Counter(class_weights[\"Zone_ID\"].to_numpy()))\n","weights_df = pd.DataFrame(np.array([[k, v] for k,v in weights.items()]), columns = [\"Class\", \"Count\"])\n","\n","weights_dict = {i : weights_df[(weights_df['Count'] > step_range[i][0]) & (weights_df['Count'] <= step_range[i][1])].reset_index(drop = True) for i in range(len(step_range))}\n","w_dict = {0 : 1,\n","          1 : 0.9,\n","          2 : 0.8,\n","          3 : 0.7,\n","          4 : 0.6,\n","          5 : 0.5,\n","          6 : 0.4,\n","          7 : 0.3,\n","          8 : 0.2,\n","          9 : 0.05,}\n","\n","weights_dict = {k:insert_weights(v, w_dict[k]) for k,v in weights_dict.items()}\n","\n","weights_df = pd.concat(list(weights_dict.values())).reset_index(drop = True)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"L4i4OY-o2PbX","executionInfo":{"status":"ok","timestamp":1695999744283,"user_tz":-120,"elapsed":2,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"outputs":[],"source":["weights_inv = {v[0] : v[2] for v in weights_df.values}"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"yrfmrVSWwKeI","executionInfo":{"status":"ok","timestamp":1695999744283,"user_tz":-120,"elapsed":2,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"outputs":[],"source":["def scheduler(epoch, lr):\n","  if epoch < 1:\n","    return 3e-3\n","  elif (epoch >= 1) & (epoch < 2):\n","    return 2e-3\n","  elif (epoch >= 2) & (epoch < 3):\n","    return 1e-3\n","  elif (epoch >= 3) & (epoch < 5):\n","    return 5e-4\n","  elif (epoch >= 5) & (epoch < 7):\n","    return 1e-4\n","  else:\n","    return 5e-5\n","\n","\n","schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"vZnjt9Sli6S9","executionInfo":{"status":"ok","timestamp":1695999744283,"user_tz":-120,"elapsed":2,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"outputs":[],"source":["moves_to_pred = 11164\n","input_size = 11166\n","starts_size = 1985\n","scrimmage_size = 100\n","positions_id = 29\n","temp_ids = 52\n","\n","model_large = QBGPT(input_vocab_size = input_size,\n","                    positional_vocab_size = temp_ids,\n","                    position_vocab_size=positions_id,\n","                    start_vocab_size=starts_size,\n","                    scrimmage_vocab_size=scrimmage_size,\n","                    offdef_vocab_size = 2,\n","                    type_vocab_size = 2,\n","                    playtype_vocab_size = 9,\n","                    embedding_dim = 512,\n","                    hidden_dim = 512,\n","                    to_pred_size = moves_to_pred)\n","\n","model_medium = QBGPT(input_vocab_size = input_size,\n","                    positional_vocab_size = temp_ids,\n","                    position_vocab_size=positions_id,\n","                    start_vocab_size=starts_size,\n","                    scrimmage_vocab_size=scrimmage_size,\n","                    offdef_vocab_size = 2,\n","                    type_vocab_size = 2,\n","                    playtype_vocab_size = 9,\n","                    embedding_dim = 256,\n","                    hidden_dim = 256,\n","                    to_pred_size = moves_to_pred)\n","\n","model_small = QBGPT(input_vocab_size = input_size,\n","                    positional_vocab_size = temp_ids,\n","                    position_vocab_size=positions_id,\n","                    start_vocab_size=starts_size,\n","                    scrimmage_vocab_size=scrimmage_size,\n","                    offdef_vocab_size = 2,\n","                    type_vocab_size = 2,\n","                    playtype_vocab_size = 9,\n","                    embedding_dim = 128,\n","                    hidden_dim = 128,\n","                    to_pred_size = moves_to_pred)\n","\n","large_model = LargeQBGPT(input_vocab_size = input_size,\n","                    positional_vocab_size = temp_ids,\n","                    position_vocab_size=positions_id,\n","                    start_vocab_size=starts_size,\n","                    scrimmage_vocab_size=scrimmage_size,\n","                    offdef_vocab_size = 2,\n","                    type_vocab_size = 2,\n","                    playtype_vocab_size = 9,\n","                    embedding_dim = 256,\n","                    hidden_dim = 256,\n","                    to_pred_size = moves_to_pred)\n","\n","xlarge_model = XLargeQBGPT(input_vocab_size = input_size,\n","                    positional_vocab_size = temp_ids,\n","                    position_vocab_size=positions_id,\n","                    start_vocab_size=starts_size,\n","                    scrimmage_vocab_size=scrimmage_size,\n","                    offdef_vocab_size = 2,\n","                    type_vocab_size = 2,\n","                    playtype_vocab_size = 9,\n","                    embedding_dim = 256,\n","                    hidden_dim = 256,\n","                    to_pred_size = moves_to_pred)"]},{"cell_type":"code","source":["custom_loss = CustomSparseCategoricalCrossentropy(from_logits=True, class_weights=weights_inv)\n","\n","model_small.compile(optimizer=tf.keras.optimizers.Adam(),\n","                    loss=custom_loss,\n","                    metrics=[CustomSparseCategoricalAccuracy(),\n","                             CustomTopKAccuracy(k=3, name='custom_top_3_accuracy'),\n","                             CustomTopKAccuracy(k=5, name='custom_top_5_accuracy')])\n","\n","history_small = model_small.fit(training_data, validation_data = testing_data, epochs=9, callbacks = [schedule])\n","\n","pd.DataFrame(history_small.history).to_csv(\"training_history_model_small.csv\", index = False, sep = \";\")\n","\n","model_small.save_weights(\"models/model_small/QBGPT\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZEJDthj2iOT","executionInfo":{"status":"ok","timestamp":1695945531729,"user_tz":-120,"elapsed":2776475,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}},"outputId":"1595bae9-e4e4-475f-f474-76204f5f316e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/9\n","6433/6433 [==============================] - 481s 71ms/step - loss: 1.4406 - custom_sparse_categorical_accuracy: 0.5059 - custom_top_3_accuracy: 0.7849 - custom_top_5_accuracy: 0.8970 - val_loss: 1.3025 - val_custom_sparse_categorical_accuracy: 0.5268 - val_custom_top_3_accuracy: 0.8107 - val_custom_top_5_accuracy: 0.9191 - lr: 0.0030\n","Epoch 2/9\n","6433/6433 [==============================] - 297s 44ms/step - loss: 1.2915 - custom_sparse_categorical_accuracy: 0.5301 - custom_top_3_accuracy: 0.8132 - custom_top_5_accuracy: 0.9211 - val_loss: 1.2636 - val_custom_sparse_categorical_accuracy: 0.5379 - val_custom_top_3_accuracy: 0.8199 - val_custom_top_5_accuracy: 0.9255 - lr: 0.0020\n","Epoch 3/9\n","6433/6433 [==============================] - 289s 43ms/step - loss: 1.2588 - custom_sparse_categorical_accuracy: 0.5392 - custom_top_3_accuracy: 0.8216 - custom_top_5_accuracy: 0.9264 - val_loss: 1.2403 - val_custom_sparse_categorical_accuracy: 0.5442 - val_custom_top_3_accuracy: 0.8261 - val_custom_top_5_accuracy: 0.9294 - lr: 0.0010\n","Epoch 4/9\n","6433/6433 [==============================] - 286s 43ms/step - loss: 1.2424 - custom_sparse_categorical_accuracy: 0.5440 - custom_top_3_accuracy: 0.8260 - custom_top_5_accuracy: 0.9291 - val_loss: 1.2288 - val_custom_sparse_categorical_accuracy: 0.5477 - val_custom_top_3_accuracy: 0.8294 - val_custom_top_5_accuracy: 0.9311 - lr: 5.0000e-04\n","Epoch 5/9\n","6433/6433 [==============================] - 285s 42ms/step - loss: 1.2387 - custom_sparse_categorical_accuracy: 0.5448 - custom_top_3_accuracy: 0.8269 - custom_top_5_accuracy: 0.9299 - val_loss: 1.2262 - val_custom_sparse_categorical_accuracy: 0.5487 - val_custom_top_3_accuracy: 0.8303 - val_custom_top_5_accuracy: 0.9316 - lr: 5.0000e-04\n","Epoch 6/9\n","6433/6433 [==============================] - 286s 43ms/step - loss: 1.2290 - custom_sparse_categorical_accuracy: 0.5481 - custom_top_3_accuracy: 0.8298 - custom_top_5_accuracy: 0.9315 - val_loss: 1.2199 - val_custom_sparse_categorical_accuracy: 0.5511 - val_custom_top_3_accuracy: 0.8321 - val_custom_top_5_accuracy: 0.9327 - lr: 1.0000e-04\n","Epoch 7/9\n","6433/6433 [==============================] - 284s 42ms/step - loss: 1.2272 - custom_sparse_categorical_accuracy: 0.5486 - custom_top_3_accuracy: 0.8303 - custom_top_5_accuracy: 0.9318 - val_loss: 1.2188 - val_custom_sparse_categorical_accuracy: 0.5513 - val_custom_top_3_accuracy: 0.8323 - val_custom_top_5_accuracy: 0.9329 - lr: 1.0000e-04\n","Epoch 8/9\n","6433/6433 [==============================] - 282s 42ms/step - loss: 1.2255 - custom_sparse_categorical_accuracy: 0.5491 - custom_top_3_accuracy: 0.8307 - custom_top_5_accuracy: 0.9322 - val_loss: 1.2179 - val_custom_sparse_categorical_accuracy: 0.5516 - val_custom_top_3_accuracy: 0.8326 - val_custom_top_5_accuracy: 0.9330 - lr: 5.0000e-05\n","Epoch 9/9\n","6433/6433 [==============================] - 282s 42ms/step - loss: 1.2248 - custom_sparse_categorical_accuracy: 0.5492 - custom_top_3_accuracy: 0.8309 - custom_top_5_accuracy: 0.9322 - val_loss: 1.2176 - val_custom_sparse_categorical_accuracy: 0.5517 - val_custom_top_3_accuracy: 0.8328 - val_custom_top_5_accuracy: 0.9331 - lr: 5.0000e-05\n"]}]},{"cell_type":"code","source":["model_small.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LSx5kt044i17","executionInfo":{"status":"ok","timestamp":1695945562215,"user_tz":-120,"elapsed":394,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}},"outputId":"ab72053c-a3cc-4ed7-f952-3a5b8767533f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"qbgpt_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," encoder_2 (Encoder)         multiple                  1939584   \n","                                                                 \n"," dense_8 (Dense)             multiple                  1440156   \n","                                                                 \n","=================================================================\n","Total params: 3379740 (12.89 MB)\n","Trainable params: 3379484 (12.89 MB)\n","Non-trainable params: 256 (1.00 KB)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8YARcSnUXUz1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695991560736,"user_tz":-120,"elapsed":2925593,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}},"outputId":"db96f145-75dc-4607-e7aa-73cd8abab7a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/9\n","6433/6433 [==============================] - 472s 70ms/step - loss: 1.4208 - custom_sparse_categorical_accuracy: 0.5094 - custom_top_3_accuracy: 0.7895 - custom_top_5_accuracy: 0.9008 - val_loss: 1.2922 - val_custom_sparse_categorical_accuracy: 0.5306 - val_custom_top_3_accuracy: 0.8140 - val_custom_top_5_accuracy: 0.9213 - lr: 0.0030\n","Epoch 2/9\n","6433/6433 [==============================] - 316s 47ms/step - loss: 1.2714 - custom_sparse_categorical_accuracy: 0.5351 - custom_top_3_accuracy: 0.8196 - custom_top_5_accuracy: 0.9256 - val_loss: 1.2455 - val_custom_sparse_categorical_accuracy: 0.5413 - val_custom_top_3_accuracy: 0.8259 - val_custom_top_5_accuracy: 0.9298 - lr: 0.0020\n","Epoch 3/9\n","6433/6433 [==============================] - 310s 46ms/step - loss: 1.2320 - custom_sparse_categorical_accuracy: 0.5456 - custom_top_3_accuracy: 0.8300 - custom_top_5_accuracy: 0.9324 - val_loss: 1.2201 - val_custom_sparse_categorical_accuracy: 0.5490 - val_custom_top_3_accuracy: 0.8327 - val_custom_top_5_accuracy: 0.9340 - lr: 0.0010\n","Epoch 4/9\n","6433/6433 [==============================] - 307s 46ms/step - loss: 1.2130 - custom_sparse_categorical_accuracy: 0.5512 - custom_top_3_accuracy: 0.8352 - custom_top_5_accuracy: 0.9357 - val_loss: 1.2081 - val_custom_sparse_categorical_accuracy: 0.5531 - val_custom_top_3_accuracy: 0.8361 - val_custom_top_5_accuracy: 0.9360 - lr: 5.0000e-04\n","Epoch 5/9\n","6433/6433 [==============================] - 305s 45ms/step - loss: 1.2081 - custom_sparse_categorical_accuracy: 0.5523 - custom_top_3_accuracy: 0.8363 - custom_top_5_accuracy: 0.9366 - val_loss: 1.2044 - val_custom_sparse_categorical_accuracy: 0.5540 - val_custom_top_3_accuracy: 0.8370 - val_custom_top_5_accuracy: 0.9368 - lr: 5.0000e-04\n","Epoch 6/9\n","6433/6433 [==============================] - 305s 45ms/step - loss: 1.1944 - custom_sparse_categorical_accuracy: 0.5571 - custom_top_3_accuracy: 0.8405 - custom_top_5_accuracy: 0.9391 - val_loss: 1.1930 - val_custom_sparse_categorical_accuracy: 0.5581 - val_custom_top_3_accuracy: 0.8407 - val_custom_top_5_accuracy: 0.9389 - lr: 1.0000e-04\n","Epoch 7/9\n","6433/6433 [==============================] - 304s 45ms/step - loss: 1.1895 - custom_sparse_categorical_accuracy: 0.5586 - custom_top_3_accuracy: 0.8421 - custom_top_5_accuracy: 0.9401 - val_loss: 1.1891 - val_custom_sparse_categorical_accuracy: 0.5591 - val_custom_top_3_accuracy: 0.8420 - val_custom_top_5_accuracy: 0.9398 - lr: 1.0000e-04\n","Epoch 8/9\n","6433/6433 [==============================] - 302s 45ms/step - loss: 1.1856 - custom_sparse_categorical_accuracy: 0.5596 - custom_top_3_accuracy: 0.8433 - custom_top_5_accuracy: 0.9409 - val_loss: 1.1877 - val_custom_sparse_categorical_accuracy: 0.5594 - val_custom_top_3_accuracy: 0.8423 - val_custom_top_5_accuracy: 0.9400 - lr: 5.0000e-05\n","Epoch 9/9\n","6433/6433 [==============================] - 301s 45ms/step - loss: 1.1844 - custom_sparse_categorical_accuracy: 0.5599 - custom_top_3_accuracy: 0.8436 - custom_top_5_accuracy: 0.9411 - val_loss: 1.1866 - val_custom_sparse_categorical_accuracy: 0.5597 - val_custom_top_3_accuracy: 0.8427 - val_custom_top_5_accuracy: 0.9403 - lr: 5.0000e-05\n"]}],"source":["custom_loss = CustomSparseCategoricalCrossentropy(from_logits=True, class_weights=weights_inv)\n","\n","model_medium.compile(optimizer=tf.keras.optimizers.Adam(),\n","                     loss=custom_loss,\n","                     metrics=[CustomSparseCategoricalAccuracy(),\n","                              CustomTopKAccuracy(k=3, name='custom_top_3_accuracy'),\n","                              CustomTopKAccuracy(k=5, name='custom_top_5_accuracy')])\n","\n","history_medium = model_medium.fit(training_data, validation_data = testing_data, epochs=9, callbacks = [schedule])\n","\n","pd.DataFrame(history_medium.history).to_csv(\"training_history_model_medium.csv\", index = False, sep = \";\")\n","\n","model_medium.save_weights(\"models/model_medium/QBGPT\")"]},{"cell_type":"code","source":["model_medium.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jw11bDzz4fye","executionInfo":{"status":"ok","timestamp":1695991577863,"user_tz":-120,"elapsed":505,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}},"outputId":"72f980f4-26b7-4447-eddb-7020a52e6849"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"qbgpt_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," encoder_4 (Encoder)         multiple                  4337920   \n","                                                                 \n"," dense_20 (Dense)            multiple                  2869148   \n","                                                                 \n","=================================================================\n","Total params: 7207068 (27.49 MB)\n","Trainable params: 7206556 (27.49 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","execution_count":20,"metadata":{"id":"nS370fCo3pd-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695998963591,"user_tz":-120,"elapsed":1593625,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}},"outputId":"2fda58bc-b1c8-4932-eec5-3ed30e86659a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/9\n","6433/6433 [==============================] - 555s 82ms/step - loss: 1.4149 - custom_sparse_categorical_accuracy: 0.5107 - custom_top_3_accuracy: 0.7911 - custom_top_5_accuracy: 0.9023 - val_loss: 1.2823 - val_custom_sparse_categorical_accuracy: 0.5320 - val_custom_top_3_accuracy: 0.8168 - val_custom_top_5_accuracy: 0.9239 - lr: 0.0030\n","Epoch 2/9\n","6433/6433 [==============================] - 344s 51ms/step - loss: 1.2584 - custom_sparse_categorical_accuracy: 0.5381 - custom_top_3_accuracy: 0.8231 - custom_top_5_accuracy: 0.9282 - val_loss: 1.2386 - val_custom_sparse_categorical_accuracy: 0.5431 - val_custom_top_3_accuracy: 0.8283 - val_custom_top_5_accuracy: 0.9313 - lr: 0.0020\n","Epoch 3/9\n","6433/6433 [==============================] - 329s 49ms/step - loss: 1.2175 - custom_sparse_categorical_accuracy: 0.5491 - custom_top_3_accuracy: 0.8339 - custom_top_5_accuracy: 0.9353 - val_loss: 1.2119 - val_custom_sparse_categorical_accuracy: 0.5505 - val_custom_top_3_accuracy: 0.8351 - val_custom_top_5_accuracy: 0.9358 - lr: 0.0010\n","Epoch 4/9\n","6433/6433 [==============================] - 332s 50ms/step - loss: 1.1967 - custom_sparse_categorical_accuracy: 0.5553 - custom_top_3_accuracy: 0.8397 - custom_top_5_accuracy: 0.9389 - val_loss: 1.1994 - val_custom_sparse_categorical_accuracy: 0.5547 - val_custom_top_3_accuracy: 0.8386 - val_custom_top_5_accuracy: 0.9378 - lr: 5.0000e-04\n","Epoch 5/9\n","6433/6433 [==============================] - 328s 49ms/step - loss: 1.1914 - custom_sparse_categorical_accuracy: 0.5567 - custom_top_3_accuracy: 0.8411 - custom_top_5_accuracy: 0.9398 - val_loss: 1.1981 - val_custom_sparse_categorical_accuracy: 0.5552 - val_custom_top_3_accuracy: 0.8392 - val_custom_top_5_accuracy: 0.9381 - lr: 5.0000e-04\n","Epoch 6/9\n","6433/6433 [==============================] - 330s 49ms/step - loss: 1.1783 - custom_sparse_categorical_accuracy: 0.5612 - custom_top_3_accuracy: 0.8448 - custom_top_5_accuracy: 0.9420 - val_loss: 1.1917 - val_custom_sparse_categorical_accuracy: 0.5577 - val_custom_top_3_accuracy: 0.8408 - val_custom_top_5_accuracy: 0.9390 - lr: 1.0000e-04\n","Epoch 7/9\n","6433/6433 [==============================] - 328s 49ms/step - loss: 1.1761 - custom_sparse_categorical_accuracy: 0.5618 - custom_top_3_accuracy: 0.8455 - custom_top_5_accuracy: 0.9423 - val_loss: 1.1913 - val_custom_sparse_categorical_accuracy: 0.5574 - val_custom_top_3_accuracy: 0.8408 - val_custom_top_5_accuracy: 0.9391 - lr: 1.0000e-04\n","Epoch 8/9\n","6433/6433 [==============================] - 323s 48ms/step - loss: 1.1736 - custom_sparse_categorical_accuracy: 0.5626 - custom_top_3_accuracy: 0.8462 - custom_top_5_accuracy: 0.9428 - val_loss: 1.1904 - val_custom_sparse_categorical_accuracy: 0.5579 - val_custom_top_3_accuracy: 0.8411 - val_custom_top_5_accuracy: 0.9393 - lr: 5.0000e-05\n","Epoch 9/9\n","6433/6433 [==============================] - 324s 48ms/step - loss: 1.1730 - custom_sparse_categorical_accuracy: 0.5627 - custom_top_3_accuracy: 0.8463 - custom_top_5_accuracy: 0.9429 - val_loss: 1.1904 - val_custom_sparse_categorical_accuracy: 0.5580 - val_custom_top_3_accuracy: 0.8412 - val_custom_top_5_accuracy: 0.9393 - lr: 5.0000e-05\n"]}],"source":["custom_loss = CustomSparseCategoricalCrossentropy(from_logits=True, class_weights=weights_inv)\n","\n","model_large.compile(optimizer=tf.keras.optimizers.Adam(),\n","                    loss=custom_loss,\n","                    metrics=[CustomSparseCategoricalAccuracy(),\n","                             CustomTopKAccuracy(k=3, name='custom_top_3_accuracy'),\n","                             CustomTopKAccuracy(k=5, name='custom_top_5_accuracy')])\n","\n","history_large = model_large.fit(training_data, validation_data = testing_data, epochs=9, callbacks = [schedule])\n","\n","pd.DataFrame(history_large.history).to_csv(\"training_history_model_large.csv\", index = False, sep = \";\")\n","\n","model_large.save_weights(\"models/model_large/QBGPT\")"]},{"cell_type":"code","source":["model_large.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nbV72hqP4cvw","executionInfo":{"status":"ok","timestamp":1695998963596,"user_tz":-120,"elapsed":5,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}},"outputId":"31c6c766-cc0e-4a61-bcab-08c93ee892a1"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"qbgpt\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," encoder (Encoder)           multiple                  10510848  \n","                                                                 \n"," dense_2 (Dense)             multiple                  5727132   \n","                                                                 \n","=================================================================\n","Total params: 16237980 (61.94 MB)\n","Trainable params: 16236956 (61.94 MB)\n","Non-trainable params: 1024 (4.00 KB)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["custom_loss = CustomSparseCategoricalCrossentropy(from_logits=True, class_weights=weights_inv)\n","\n","large_model.compile(optimizer=tf.keras.optimizers.Adam(),\n","                    loss=custom_loss,\n","                    metrics=[CustomSparseCategoricalAccuracy(),\n","                             CustomTopKAccuracy(k=3, name='custom_top_3_accuracy'),\n","                             CustomTopKAccuracy(k=5, name='custom_top_5_accuracy')])\n","\n","history_large = large_model.fit(training_data, validation_data = testing_data, epochs=9, callbacks = [schedule])\n","\n","pd.DataFrame(history_large.history).to_csv(\"training_history_large_model.csv\", index = False, sep = \";\")\n","\n","large_model.save_weights(\"models/large_model/QBGPT\")"],"metadata":{"id":"9ePjJx6v2iGG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696002872169,"user_tz":-120,"elapsed":3123026,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}},"outputId":"092bc50c-20a1-4e61-b8d1-83bee1097405"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/9\n","6433/6433 [==============================] - 533s 80ms/step - loss: 1.4355 - custom_sparse_categorical_accuracy: 0.5069 - custom_top_3_accuracy: 0.7862 - custom_top_5_accuracy: 0.8982 - val_loss: 1.2904 - val_custom_sparse_categorical_accuracy: 0.5297 - val_custom_top_3_accuracy: 0.8129 - val_custom_top_5_accuracy: 0.9211 - lr: 0.0030\n","Epoch 2/9\n","6433/6433 [==============================] - 336s 50ms/step - loss: 1.2792 - custom_sparse_categorical_accuracy: 0.5332 - custom_top_3_accuracy: 0.8172 - custom_top_5_accuracy: 0.9238 - val_loss: 1.2522 - val_custom_sparse_categorical_accuracy: 0.5398 - val_custom_top_3_accuracy: 0.8235 - val_custom_top_5_accuracy: 0.9283 - lr: 0.0020\n","Epoch 3/9\n","6433/6433 [==============================] - 329s 49ms/step - loss: 1.2426 - custom_sparse_categorical_accuracy: 0.5432 - custom_top_3_accuracy: 0.8268 - custom_top_5_accuracy: 0.9302 - val_loss: 1.2241 - val_custom_sparse_categorical_accuracy: 0.5482 - val_custom_top_3_accuracy: 0.8314 - val_custom_top_5_accuracy: 0.9332 - lr: 0.0010\n","Epoch 4/9\n","6433/6433 [==============================] - 322s 48ms/step - loss: 1.2219 - custom_sparse_categorical_accuracy: 0.5492 - custom_top_3_accuracy: 0.8327 - custom_top_5_accuracy: 0.9341 - val_loss: 1.2090 - val_custom_sparse_categorical_accuracy: 0.5531 - val_custom_top_3_accuracy: 0.8364 - val_custom_top_5_accuracy: 0.9360 - lr: 5.0000e-04\n","Epoch 5/9\n","6433/6433 [==============================] - 322s 48ms/step - loss: 1.2143 - custom_sparse_categorical_accuracy: 0.5512 - custom_top_3_accuracy: 0.8351 - custom_top_5_accuracy: 0.9357 - val_loss: 1.2032 - val_custom_sparse_categorical_accuracy: 0.5544 - val_custom_top_3_accuracy: 0.8382 - val_custom_top_5_accuracy: 0.9375 - lr: 5.0000e-04\n","Epoch 6/9\n","6433/6433 [==============================] - 320s 48ms/step - loss: 1.2012 - custom_sparse_categorical_accuracy: 0.5553 - custom_top_3_accuracy: 0.8388 - custom_top_5_accuracy: 0.9380 - val_loss: 1.1955 - val_custom_sparse_categorical_accuracy: 0.5573 - val_custom_top_3_accuracy: 0.8407 - val_custom_top_5_accuracy: 0.9388 - lr: 1.0000e-04\n","Epoch 7/9\n","6433/6433 [==============================] - 320s 48ms/step - loss: 1.1976 - custom_sparse_categorical_accuracy: 0.5562 - custom_top_3_accuracy: 0.8398 - custom_top_5_accuracy: 0.9388 - val_loss: 1.1940 - val_custom_sparse_categorical_accuracy: 0.5575 - val_custom_top_3_accuracy: 0.8411 - val_custom_top_5_accuracy: 0.9393 - lr: 1.0000e-04\n","Epoch 8/9\n","6433/6433 [==============================] - 318s 48ms/step - loss: 1.1949 - custom_sparse_categorical_accuracy: 0.5568 - custom_top_3_accuracy: 0.8406 - custom_top_5_accuracy: 0.9393 - val_loss: 1.1921 - val_custom_sparse_categorical_accuracy: 0.5583 - val_custom_top_3_accuracy: 0.8415 - val_custom_top_5_accuracy: 0.9395 - lr: 5.0000e-05\n","Epoch 9/9\n","6433/6433 [==============================] - 318s 47ms/step - loss: 1.1940 - custom_sparse_categorical_accuracy: 0.5571 - custom_top_3_accuracy: 0.8408 - custom_top_5_accuracy: 0.9394 - val_loss: 1.1922 - val_custom_sparse_categorical_accuracy: 0.5579 - val_custom_top_3_accuracy: 0.8417 - val_custom_top_5_accuracy: 0.9396 - lr: 5.0000e-05\n"]}]},{"cell_type":"code","source":["large_model.summary()"],"metadata":{"id":"BA-dph_m4jvq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696003521946,"user_tz":-120,"elapsed":444,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}},"outputId":"030ae786-477a-4d12-f5ad-939332930189"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"large_qbgpt_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," encoder_l_1 (EncoderL)      multiple                  5127936   \n","                                                                 \n"," dense_26 (Dense)            multiple                  2869148   \n","                                                                 \n","=================================================================\n","Total params: 7997084 (30.51 MB)\n","Trainable params: 7996060 (30.50 MB)\n","Non-trainable params: 1024 (4.00 KB)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"SW-PxTSR52PA"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyOn549AXhLPlk7xAIgVjOzz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}