{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOdfJoNF73jOSn/+XfcVm3w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BcuLdi7seAKr","executionInfo":{"status":"ok","timestamp":1695750089313,"user_tz":-120,"elapsed":23522,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}},"outputId":"a57ae5af-106b-43d0-9d5a-816d53a8a4ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["os.chdir(\"/content/gdrive/MyDrive/NFL_Challenge/NFL-GPT/NFL data\")\n","os.listdir()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B2pOQ6qWeILn","executionInfo":{"status":"ok","timestamp":1695750225368,"user_tz":-120,"elapsed":215,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}},"outputId":"7e91fa10-a10c-4681-e348-1d5fed9420b1"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['train_play_prediction_categ',\n"," 'test_play_prediction_categ',\n"," 'train_play_prediction_binary',\n"," 'test_play_prediction_binary',\n"," '.DS_Store',\n"," 'Contact Detection',\n"," 'Punt Prediction',\n"," 'Analytics',\n"," 'Impact Detection',\n"," 'data bowl 2021',\n"," 'data bowl 2023',\n"," 'data bowl 2022',\n"," 'data bowl 2020',\n"," 'asonty',\n"," 'Highlights_NGS_2019',\n"," 'Highlights_NGS_Prime',\n"," 'final_df.parquet',\n"," 'tokens.json',\n"," 'mapped_df.parquet',\n"," 'train_test_split.csv',\n"," 'class_weights.parquet',\n"," 'checkpoint',\n"," 'models',\n"," 'test_tokens_NFL_GPT',\n"," 'train_tokens_NFL_GPT',\n"," 'training_history.csv']"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["training_data = tf.data.Dataset.load(\"train_play_prediction_binary\")\n","testing_data = tf.data.Dataset.load(\"test_play_prediction_binary\")"],"metadata":{"id":"n21dB8fmlAi_","executionInfo":{"status":"ok","timestamp":1695750238260,"user_tz":-120,"elapsed":6338,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["training_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8D31HaTelyvO","executionInfo":{"status":"ok","timestamp":1695750241105,"user_tz":-120,"elapsed":6,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}},"outputId":"1d3697f5-a143-43a1-9ee9-413564503b82"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_LoadDataset element_spec=({'def': {'token_type_ids': TensorSpec(shape=(11,), dtype=tf.int64, name=None), 'OffDef_ID': TensorSpec(shape=(11,), dtype=tf.int64, name=None), 'playId': TensorSpec(shape=(), dtype=tf.int64, name=None), 'yard_ID': TensorSpec(shape=(), dtype=tf.int64, name=None), 'position_ID': TensorSpec(shape=(11,), dtype=tf.int64, name=None), 'down_ID': TensorSpec(shape=(11,), dtype=tf.int64, name=None), 'gameId': TensorSpec(shape=(), dtype=tf.int64, name=None), 'team_ID': TensorSpec(shape=(11,), dtype=tf.int64, name=None), 'Success': TensorSpec(shape=(), dtype=tf.float64, name=None)}, 'off': {'playId': TensorSpec(shape=(), dtype=tf.int64, name=None), 'down_ID': TensorSpec(shape=(11,), dtype=tf.int64, name=None), 'position_ID': TensorSpec(shape=(11,), dtype=tf.int64, name=None), 'Success': TensorSpec(shape=(), dtype=tf.float64, name=None), 'team_ID': TensorSpec(shape=(11,), dtype=tf.int64, name=None), 'OffDef_ID': TensorSpec(shape=(11,), dtype=tf.int64, name=None), 'gameId': TensorSpec(shape=(), dtype=tf.int64, name=None), 'yard_ID': TensorSpec(shape=(), dtype=tf.int64, name=None), 'token_type_ids': TensorSpec(shape=(11,), dtype=tf.int64, name=None)}}, TensorSpec(shape=(), dtype=tf.float64, name=None))>"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["class PlayTypeEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(PlayTypeEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"PlayType\"])\n","    return embed\n","\n","class OffDefEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(OffDefEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"OffDef_ID\"])\n","    return embed\n","\n","class InputEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(InputEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"input_ids\"])\n","    return embed\n","\n","class NFLTeamEncoder(tf.keras.Model):\n","  def __init__(self, nb_team : int, embedding_dim : int):\n","        super(NFLTeamEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = nb_team,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"NFLTeam\"])\n","    return embed\n","\n","class DownEncoder(tf.keras.Model):\n","  def __init__(self, embedding_dim : int, nb_down = 5,):\n","        super(DownEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = nb_down,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"down_ID\"])\n","    return embed\n","\n","class TypeEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size : int, embedding_dim : int):\n","        super(TypeEncoder, self).__init__()\n","\n","        self.Embedding = tf.keras.layers.Embedding(input_dim = vocab_size,\n","                                                   output_dim = embedding_dim)\n","\n","  def call(self, x):\n","    embed = self.Embedding(x[\"token_type_ids\"])\n","    return embed"],"metadata":{"id":"C2poomB3efHQ","executionInfo":{"status":"ok","timestamp":1695732460502,"user_tz":-120,"elapsed":383,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class Embedding(tf.keras.Model):\n","  def __init__(self,\n","               input_vocab_size : int,\n","               offdef_vocab_size : int,\n","               type_size: int,\n","               playtype_vocab_size : int,\n","               nfl_team_size : int,\n","               down_size : int,\n","               embedding_dim : int):\n","        super(Embedding, self).__init__()\n","\n","        self.InputEmbedding = InputEncoder(vocab_size=input_vocab_size,\n","                                           embedding_dim=embedding_dim)\n","        self.OffDefEmbedding = OffDefEncoder(vocab_size=offdef_vocab_size,\n","                                             embedding_dim=embedding_dim)\n","        self.TypeEmbedding = TypeEncoder(vocab_size=type_size,\n","                                             embedding_dim=embedding_dim)\n","        self.PlayTypeEmbedding = PlayTypeEncoder(vocab_size=playtype_vocab_size,\n","                                                 embedding_dim=embedding_dim)\n","        self.NFLTeamEmbedding = NFLTeamEncoder(vocab_size=nfl_team_size,\n","                                                 embedding_dim=embedding_dim)\n","        self.DownEmbedding = DownEncoder(vocab_size=down_size,\n","                                                 embedding_dim=embedding_dim)\n","        self.Add = tf.keras.layers.Add()\n","\n","        self.Dense = tf.keras.layers.Dense(embedding_dim, activation = \"relu\")\n","\n","  def call(self, x):\n","    input_embed = self.InputEmbedding(x)\n","    offdef_embed = self.OffDefEmbedding(x)\n","    playtype_embed = self.PlayTypeEmbedding(x)\n","    NFLTeamEncoder_embed = self.NFLTeamEmbedding(x)\n","    down_embed = self.DownEmbedding(x)\n","\n","    embed = self.Add([input_embed, offdef_embed, playtype_embed, NFLTeamEncoder_embed, down_embed])\n","    embed = self.Dense(embed)\n","\n","    return embed"],"metadata":{"id":"7vqjm9vfexiz","executionInfo":{"status":"ok","timestamp":1695733359684,"user_tz":-120,"elapsed":195,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from typing import List, Optional, Union\n","\n","def shape_list(tensor: Union[tf.Tensor, np.ndarray]) -> List[int]:\n","    \"\"\"\n","    Deal with dynamic shape in tensorflow cleanly.\n","\n","    Args:\n","        tensor (`tf.Tensor` or `np.ndarray`): The tensor we want the shape of.\n","\n","    Returns:\n","        `List[int]`: The shape of the tensor as a list.\n","    \"\"\"\n","    if isinstance(tensor, np.ndarray):\n","        return list(tensor.shape)\n","\n","    dynamic = tf.shape(tensor)\n","\n","    if tensor.shape == tf.TensorShape(None):\n","        return dynamic\n","\n","    static = tensor.shape.as_list()\n","\n","    return [dynamic[i] if s is None else s for i, s in enumerate(static)]"],"metadata":{"id":"tnd3orq2kQjD","executionInfo":{"status":"ok","timestamp":1695733361683,"user_tz":-120,"elapsed":274,"user":{"displayName":"sam Chain","userId":"00779398991030525753"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class AttentionBlock(tf.keras.Model):\n","  def __init__(self,\n","               num_heads : int,\n","               hidden_dim : int,\n","               output_dim : int):\n","        super(AttentionBlock, self).__init__()\n","\n","        self.num_attention_heads = num_heads\n","        self.attention_head_size = hidden_dim\n","        self.total_dim = num_heads * hidden_dim\n","        self.output_dim = output_dim\n","\n","        self.Query = tf.keras.layers.Dense(self.total_dim, name = \"Query\")\n","        self.Key = tf.keras.layers.Dense(self.total_dim, name = \"Key\")\n","        self.Value = tf.keras.layers.Dense(self.total_dim, name = \"Value\")\n","\n","\n","        self.Dense = tf.keras.layers.Dense(output_dim, name = \"Dense\", activation = \"relu\")\n","        self.Add = tf.keras.layers.Add(name = \"Add\")\n","        self.Drop = tf.keras.layers.Dropout(rate = 0.1)\n","        self.Norm = tf.keras.layers.BatchNormalization(name = \"Norm\")\n","\n","  def transpose_for_scores(self, tensor: tf.Tensor, batch_size: int) -> tf.Tensor:\n","        # Reshape from [batch_size, seq_length, all_head_size] to [batch_size, seq_length, num_attention_heads, attention_head_size]\n","        tensor = tf.reshape(tensor=tensor, shape=(batch_size, -1, self.num_attention_heads, self.attention_head_size))\n","\n","        # Transpose the tensor from [batch_size, seq_length, num_attention_heads, attention_head_size] to [batch_size, num_attention_heads, seq_length, attention_head_size]\n","        return tf.transpose(tensor, perm=[0, 2, 1, 3])\n","\n","  def compute_scaled_attn_scores(self, query, key):\n","    attention_scores = tf.matmul(query, key, transpose_b=True)  # Transpose the second sequence\n","\n","    # If you want scaled dot-product attention, divide by the square root of the embedding dimension\n","    embedding_dim = query.shape[-1]\n","    scaled_attention_scores = attention_scores / tf.math.sqrt(tf.cast(embedding_dim, dtype=tf.float32))\n","\n","    return scaled_attention_scores\n","\n","  def compute_attention_weigths(self, query, key, temp_ids, masks):\n","\n","    scaled_attn_scores = self.compute_scaled_attn_scores(query, key)\n","\n","    return tf.nn.softmax(scaled_attn_scores, axis = -1)\n","\n","  def get_preds_and_attention(self,\n","           embeddings):\n","\n","    query = self.Query(embeddings)\n","    key = self.Key(embeddings)\n","    value = self.Value(embeddings)\n","\n","    attention_weights = self.compute_attention_weigths(query, key)\n","\n","    attention_scores = tf.matmul(attention_weights, value)\n","    attention_scores = self.Dense(attention_scores)\n","\n","    output = self.Add([attention_scores, embeddings])\n","    output = self.Drop(output)\n","    output = self.Norm(output)\n","    return output, attention_weights\n","\n","  def call(self,\n","           hidden_states : tf.Tensor,\n","           temporal_ids,\n","           attention_masks):\n","\n","    batch_size = shape_list(hidden_states)[0]\n","\n","    query = self.Query(hidden_states)\n","    queries = self.transpose_for_scores(query, batch_size)\n","\n","    key = self.Key(hidden_states)\n","    keys = self.transpose_for_scores(key, batch_size)\n","\n","    value = self.Value(hidden_states)\n","    values = self.transpose_for_scores(value, batch_size)\n","\n","    attention_weights = self.compute_attention_weigths(queries, keys, temporal_ids, attention_masks)\n","\n","    attention_scores = tf.matmul(attention_weights, values)\n","    attention_scores = tf.transpose(attention_scores, perm=[0, 2, 1, 3])\n","    attention_scores = tf.reshape(tensor=attention_scores, shape=(batch_size, -1, self.total_dim))\n","\n","    attention_scores = self.Dense(attention_scores)\n","\n","    output = self.Add([attention_scores, hidden_states])\n","    output = self.Drop(output)\n","    output = self.Norm(output)\n","    return output\n","\n","class PlayPredictor(tf.keras.Model):\n","  def __init__(self,\n","               input_vocab_size : int,\n","               offdef_vocab_size : int,\n","               nfl_team_size : int,\n","               down_size : int,\n","               playtype_vocab_size : int,\n","               embedding_dim : int,\n","               hidden_dim : int):\n","        super(PlayPredictor, self).__init__()\n","\n","        self.Embedding = Embedding(input_vocab_size =input_vocab_size,\n","                                   offdef_vocab_size =offdef_vocab_size,\n","                                   playtype_vocab_size =playtype_vocab_size,\n","                                   nfl_team_size =nfl_team_size,\n","                                   down_size = down_size,\n","                                   embedding_dim =embedding_dim)\n","\n","        self.Attention1 = AttentionBlock(num_heads = 3,\n","                                         hidden_dim = hidden_dim,\n","                                         output_dim = embedding_dim)\n","\n","\n","        self.DenseHead = tf.keras.layers.Dense(128, activation = \"gelu\")\n","        self.Sub = tf.keras.layers.Subtract()\n","\n","        self.PPHead = tf.keras.layers.Dense(1, activation = \"sigmoid\")\n","\n","  def call(self,\n","           x):\n","\n","    embed_off = self.Embedding(x[\"off\"])\n","    h1_off = self.Attention1(embed_off)\n","    logits_off = self.DenseHead(h1_off)\n","\n","    embed_def = self.Embedding(x[\"def\"])\n","    h1_def = self.Attention1(embed_def)\n","    logits_def = self.DenseHead(h1_def)\n","\n","    logits = self.Sub([logits_off, logits_def])\n","    avg_logits = tf.reduce_mean(logits, axis=1)\n","\n","    pred = self.PPHead(logits)\n","\n","    return pred"],"metadata":{"id":"32qzLEL0jkZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EHNS1yT0p5sM"},"execution_count":null,"outputs":[]}]}